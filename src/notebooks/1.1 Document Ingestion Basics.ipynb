{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1.1 Document Ingestion Basics - Ungraph\n",
        "\n",
        "Este notebook cubre la fase **EXTRACT** del patr√≥n ETI: c√≥mo cargar documentos desde diferentes formatos al sistema.\n",
        "\n",
        "## Objetivos\n",
        "\n",
        "1. **Configurar Ungraph** - Conexi√≥n a Neo4j y configuraci√≥n b√°sica\n",
        "2. **Cargar documentos** - Markdown, TXT, Word, PDF\n",
        "3. **Verificar carga** - Validar que los documentos se cargaron correctamente\n",
        "4. **Obtener recomendaciones** - Estrategias de chunking recomendadas\n",
        "\n",
        "## Formatos Soportados\n",
        "\n",
        "- ‚úÖ **Markdown (.md)** - Documentos estructurados\n",
        "- ‚úÖ **Texto plano (.txt)** - Con detecci√≥n autom√°tica de encoding\n",
        "- ‚úÖ **Word (.docx)** - Documentos de Microsoft Word\n",
        "- ‚úÖ **PDF (.pdf)** - Usando IBM Docling para mejor extracci√≥n\n",
        "\n",
        "**Referencias:**\n",
        "- [Gu√≠a de Ingesta](../../docs/guides/ingestion.md)\n",
        "- [API P√∫blica](../../docs/api/public-api.md)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_src_to_path(path_folder: str):\n",
        "    ''' \n",
        "    Helper function for adding the \"path_folder\" directory to the path.\n",
        "    '''\n",
        "    import sys\n",
        "    from pathlib import Path\n",
        "\n",
        "    base_path = Path().resolve()\n",
        "    for parent in [base_path] + list(base_path.parents):\n",
        "        candidate = parent / path_folder\n",
        "        if candidate.exists():\n",
        "            parent_dir = candidate.parent\n",
        "            if str(parent_dir) not in sys.path:\n",
        "                sys.path.insert(0, str(parent_dir))\n",
        "            if str(candidate) not in sys.path:\n",
        "                sys.path.append(str(candidate))\n",
        "            return\n",
        "\n",
        "# Agregar carpetas necesarias al path\n",
        "add_src_to_path(path_folder=\"src\")\n",
        "add_src_to_path(path_folder=\"src/utils\")\n",
        "add_src_to_path(path_folder=\"src/data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Ungraph importado desde src/ (modo desarrollo)\n",
            "üì¶ Ungraph version: 0.1.0\n"
          ]
        }
      ],
      "source": [
        "# Importar librer√≠as necesarias\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Importar handlers\n",
        "from src.utils.handlers import find_in_project\n",
        "\n",
        "# Importar ungraph\n",
        "try:\n",
        "    import ungraph\n",
        "    print(\"‚úÖ Ungraph importado como paquete instalado\")\n",
        "except ImportError:\n",
        "    import src\n",
        "    ungraph = src\n",
        "    print(\"‚úÖ Ungraph importado desde src/ (modo desarrollo)\")\n",
        "\n",
        "print(f\"üì¶ Ungraph version: {ungraph.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 1: Configuraci√≥n Inicial\n",
        "\n",
        "Primero configuramos la conexi√≥n a Neo4j. Puedes usar variables de entorno o configuraci√≥n program√°tica.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Configuraci√≥n completada\n",
            "üí° Si obtienes AuthError, verifica que Neo4j est√© corriendo y las credenciales sean correctas\n"
          ]
        }
      ],
      "source": [
        "# Configurar Ungraph\n",
        "# Opci√≥n 1: Usar variables de entorno (recomendado)\n",
        "# export NEO4J_URI=\"bolt://localhost:7687\"\n",
        "# export NEO4J_PASSWORD=\"tu_contrase√±a\"\n",
        "\n",
        "# Opci√≥n 2: Configurar program√°ticamente\n",
        "ungraph.configure(\n",
        "    neo4j_uri=\"bolt://localhost:7687\",\n",
        "    neo4j_user=\"neo4j\",\n",
        "    neo4j_password=\"Ungraph22\",  # ‚ö†Ô∏è CAMBIAR: Usa tu contrase√±a real\n",
        "    neo4j_database=\"neo4j\",\n",
        "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Configuraci√≥n completada\")\n",
        "print(\"üí° Si obtienes AuthError, verifica que Neo4j est√© corriendo y las credenciales sean correctas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 2: Encontrar Archivos de Datos\n",
        "\n",
        "Localicemos los archivos de ejemplo disponibles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÅ Carpeta de datos: D:\\projects\\Ungraph\\src\\data\n",
            "\n",
            "üìÑ Archivos disponibles:\n",
            "  - 110225.md (6.6 KB)\n",
            "  - AnnyLetter.txt (17.8 KB)\n",
            "  - peach-et-al-2014-qsar-modeling-of-imbalanced-high-throughput-screening-data-in-pubchem.pdf (447.4 KB)\n",
            "  - Usar s√≠mboles de silencio de corchea.docx (25.2 KB)\n"
          ]
        }
      ],
      "source": [
        "# Encontrar la carpeta data\n",
        "data_path = find_in_project(\n",
        "    target=\"data\",\n",
        "    search_type=\"folder\",\n",
        "    project_root=None\n",
        ")\n",
        "\n",
        "print(f\"üìÅ Carpeta de datos: {data_path}\")\n",
        "\n",
        "# Listar archivos disponibles\n",
        "files = list(data_path.glob(\"*\"))\n",
        "print(f\"\\nüìÑ Archivos disponibles:\")\n",
        "for file in files:\n",
        "    if file.is_file():\n",
        "        size_kb = file.stat().st_size / 1024\n",
        "        print(f\"  - {file.name} ({size_kb:.1f} KB)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 3: Ingerir Documentos por Formato\n",
        "\n",
        "Ahora ingerimos documentos de diferentes formatos. Cada formato tiene sus caracter√≠sticas espec√≠ficas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Ingerir Markdown (.md)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Ingiriendo: 110225.md\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-25 16:04:38,939 - INFO - CUDA no disponible, usando CPU para embeddings.\n",
            "2025-12-25 16:04:38,945 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
            "2025-12-25 16:04:41,967 - INFO - Embedding service initialized with model: sentence-transformers/all-MiniLM-L6-v2\n",
            "2025-12-25 16:04:41,968 - INFO - Loading spaCy model: en_core_web_sm\n",
            "2025-12-25 16:04:43,404 - INFO - spaCy model loaded successfully\n",
            "2025-12-25 16:04:43,405 - INFO - Starting document ingestion: D:\\projects\\Ungraph\\src\\data\\110225.md\n",
            "2025-12-25 16:04:43,408 - INFO - Using pattern: FILE_PAGE_CHUNK\n",
            "2025-12-25 16:04:43,409 - INFO - Step 1: Loading document\n",
            "2025-12-25 16:04:43,410 - INFO - Cargando archivo Markdown: D:\\projects\\Ungraph\\src\\data\\110225.md\n",
            "2025-12-25 16:04:47,492 - INFO - Starting text cleaning.\n",
            "2025-12-25 16:04:47,494 - INFO - Text cleaning completed.\n",
            "2025-12-25 16:04:47,495 - INFO - Archivo cargado exitosamente. Documentos generados: 1\n",
            "2025-12-25 16:04:47,496 - INFO - Step 2: Chunking document\n",
            "2025-12-25 16:04:47,496 - INFO - Chunking document: 110225.md\n",
            "2025-12-25 16:04:47,500 - INFO - Document divided into 9 chunks\n",
            "2025-12-25 16:04:47,500 - INFO - Step 3: Generating embeddings\n",
            "2025-12-25 16:04:47,501 - INFO - Generating embeddings for 9 chunks\n",
            "2025-12-25 16:04:47,807 - INFO - Embeddings generation completed\n",
            "2025-12-25 16:04:47,807 - INFO - Step 4: Running inference phase (ETI)\n",
            "2025-12-25 16:04:47,859 - INFO - Inferred 23 facts from chunk 110225.md_bd7bc8ad-1c65-4bfb-8dfc-47b05ff94ca9\n",
            "2025-12-25 16:04:47,888 - INFO - Inferred 26 facts from chunk 110225.md_16ed4563-a987-47eb-a361-d00cffac4952\n",
            "2025-12-25 16:04:47,919 - INFO - Inferred 25 facts from chunk 110225.md_b387be51-d7bc-45db-9f9e-a5a9a6754552\n",
            "2025-12-25 16:04:47,950 - INFO - Inferred 18 facts from chunk 110225.md_e01f775e-489f-4b18-bd13-e0dc51010f5a\n",
            "2025-12-25 16:04:47,983 - INFO - Inferred 17 facts from chunk 110225.md_79ab711f-9e0d-41b6-8cb9-caa1813cd12b\n",
            "2025-12-25 16:04:48,011 - INFO - Inferred 24 facts from chunk 110225.md_c638b832-9829-453f-9cf9-9eef060eab9e\n",
            "2025-12-25 16:04:48,048 - INFO - Inferred 22 facts from chunk 110225.md_4f90ea6a-281f-4f55-a802-1f5cefd0be01\n",
            "2025-12-25 16:04:48,084 - INFO - Inferred 20 facts from chunk 110225.md_ce73c568-7e26-4113-8c8b-367dddb650cd\n",
            "2025-12-25 16:04:48,097 - INFO - Inferred 6 facts from chunk 110225.md_c7fc652b-09f1-40d5-a452-20994b74efdd\n",
            "2025-12-25 16:04:48,098 - INFO - Inference phase completed. Generated 181 facts\n",
            "2025-12-25 16:04:48,099 - INFO - Step 5: Setting up indexes\n",
            "2025-12-25 16:04:48,100 - INFO - Setting up all indexes\n",
            "2025-12-25 16:04:48,102 - INFO - Connecting to Neo4j at bolt://localhost:7687 with user neo4j\n",
            "2025-12-25 16:04:50,233 - INFO - Successfully connected to Neo4j\n",
            "2025-12-25 16:04:50,312 - INFO - Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE RANGE INDEX chunk_consecutive_idx IF NOT EXISTS FOR (e:Chunk) ON (e.chunk_id_consecutive)` has no effect.} {description: `RANGE INDEX chunk_consecutive_idx FOR (e:Chunk) ON (e.chunk_id_consecutive)` already exists.} {position: None} for query: '\\n        CREATE INDEX chunk_consecutive_idx IF NOT EXISTS\\n        FOR (c:Chunk)\\n        ON (c.chunk_id_consecutive)\\n        '\n",
            "2025-12-25 16:04:50,318 - INFO - Regular index 'chunk_consecutive_idx' created successfully\n",
            "2025-12-25 16:04:50,588 - INFO - Vector index 'chunk_embeddings' already exists\n",
            "2025-12-25 16:04:50,598 - INFO - Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE FULLTEXT INDEX chunk_content IF NOT EXISTS FOR (e:Chunk) ON EACH [e.page_content] OPTIONS {indexConfig: {`fulltext.analyzer`: \"spanish\", `fulltext.eventually_consistent`: false}}` has no effect.} {description: `FULLTEXT INDEX chunk_content FOR (e:Chunk) ON EACH [e.page_content]` already exists.} {position: None} for query: \"\\n        CREATE FULLTEXT INDEX chunk_content IF NOT EXISTS\\n        FOR (c:Chunk)\\n        ON EACH [c.page_content]\\n        OPTIONS {\\n            indexConfig: {\\n                `fulltext.analyzer`: 'spanish',\\n                `fulltext.eventually_consistent`: false\\n            }\\n        }\\n        \"\n",
            "2025-12-25 16:04:50,601 - INFO - Full-text index 'chunk_content' created successfully\n",
            "2025-12-25 16:04:50,601 - INFO - All indexes setup completed\n",
            "2025-12-25 16:04:50,602 - INFO - Step 6: Persisting chunks with pattern FILE_PAGE_CHUNK\n",
            "2025-12-25 16:04:50,605 - INFO - Using existing save_batch() for FILE_PAGE_CHUNK pattern\n",
            "2025-12-25 16:04:50,605 - INFO - Connecting to Neo4j at bolt://localhost:7687 with user neo4j\n",
            "2025-12-25 16:04:52,647 - INFO - Successfully connected to Neo4j\n",
            "2025-12-25 16:04:56,343 - INFO - Step 7: Persisting 181 facts\n",
            "2025-12-25 16:04:58,183 - INFO - Successfully saved 181 facts to Neo4j\n",
            "2025-12-25 16:04:58,184 - INFO - Successfully persisted 181 facts\n",
            "2025-12-25 16:04:58,186 - INFO - Step 8: Creating chunk relationships\n",
            "2025-12-25 16:04:58,477 - INFO - Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Statement.CartesianProduct} {category: PERFORMANCE} {title: This query builds a cartesian product between disconnected patterns.} {description: If a part of a query contains multiple disconnected patterns, this will build a cartesian product between all those parts. This may produce a large amount of data and slow down query processing. While occasionally intended, it may often be possible to reformulate the query that avoids the use of this cross product, perhaps by adding a relationship between the different parts or by using OPTIONAL MATCH (identifier is: (c2))} {position: line: 2, column: 5, offset: 5} for query: '\\n    MATCH (c1:Chunk),(c2:Chunk)\\n    WHERE c1.chunk_id_consecutive + 1 = c2.chunk_id_consecutive\\n    MERGE (c1)-[:NEXT_CHUNK]->(c2)\\n    '\n",
            "2025-12-25 16:04:58,488 - INFO - Chunk relationships created successfully\n",
            "2025-12-25 16:04:58,490 - INFO - Document ingestion completed. Created 9 chunks and 181 facts\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Documento ingerido exitosamente!\n",
            "   Total de chunks: 9\n",
            "   Chunks con embeddings: 9\n",
            "\n",
            "üìÑ Primeros 2 chunks:\n",
            "\n",
            "   Chunk 1:\n",
            "   - ID: 110225.md_bd7bc8ad-1c65-4bfb-8dfc-47b05ff94ca9...\n",
            "   - Contenido: Incluso es lo primero que se abrio, quiza lo que me falta en este momento es la palabra, y necesite ...\n",
            "   - Consecutivo: 1\n",
            "   - Embeddings: 384 dimensiones\n",
            "\n",
            "   Chunk 2:\n",
            "   - ID: 110225.md_16ed4563-a987-47eb-a361-d00cffac4952...\n",
            "   - Contenido: quiero narrar como estoy desarrollando la vision de mi mismo, la que quiero para mi, el humano que q...\n",
            "   - Consecutivo: 2\n",
            "   - Embeddings: 384 dimensiones\n"
          ]
        }
      ],
      "source": [
        "# Ingerir archivo Markdown\n",
        "markdown_file = data_path / \"110225.md\"\n",
        "\n",
        "if markdown_file.exists():\n",
        "    print(f\"üì• Ingiriendo: {markdown_file.name}\")\n",
        "    chunks = ungraph.ingest_document(\n",
        "        markdown_file,\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        clean_text=True\n",
        "    )\n",
        "    print(f\"‚úÖ Documento ingerido exitosamente!\")\n",
        "    print(f\"   Total de chunks: {len(chunks)}\")\n",
        "    print(f\"   Chunks con embeddings: {sum(1 for c in chunks if c.embeddings)}\")\n",
        "    \n",
        "    # Mostrar informaci√≥n de algunos chunks\n",
        "    print(f\"\\nüìÑ Primeros 2 chunks:\")\n",
        "    for i, chunk in enumerate(chunks[:2], 1):\n",
        "        print(f\"\\n   Chunk {i}:\")\n",
        "        print(f\"   - ID: {chunk.id[:50]}...\")\n",
        "        print(f\"   - Contenido: {chunk.page_content[:100]}...\")\n",
        "        print(f\"   - Consecutivo: {chunk.chunk_id_consecutive}\")\n",
        "        if chunk.embeddings:\n",
        "            print(f\"   - Embeddings: {len(chunk.embeddings)} dimensiones\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Archivo no encontrado: {markdown_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Ingerir Texto Plano (.txt)\n",
        "\n",
        "Los archivos de texto tienen detecci√≥n autom√°tica de encoding (UTF-8, Windows-1252, Latin-1, etc.).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-25 16:04:58,507 - INFO - CUDA no disponible, usando CPU para embeddings.\n",
            "2025-12-25 16:04:58,511 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Ingiriendo: AnnyLetter.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-25 16:05:00,898 - INFO - Embedding service initialized with model: sentence-transformers/all-MiniLM-L6-v2\n",
            "2025-12-25 16:05:00,900 - INFO - Loading spaCy model: en_core_web_sm\n",
            "2025-12-25 16:05:01,432 - INFO - spaCy model loaded successfully\n",
            "2025-12-25 16:05:01,433 - INFO - Starting document ingestion: D:\\projects\\Ungraph\\src\\data\\AnnyLetter.txt\n",
            "2025-12-25 16:05:01,434 - INFO - Using pattern: FILE_PAGE_CHUNK\n",
            "2025-12-25 16:05:01,435 - INFO - Step 1: Loading document\n",
            "2025-12-25 16:05:01,436 - INFO - Cargando archivo de texto: D:\\projects\\Ungraph\\src\\data\\AnnyLetter.txt\n",
            "2025-12-25 16:05:01,463 - INFO - Codificaci√≥n detectada por chardet: iso-8859-1 (confianza: 73.00%)\n",
            "2025-12-25 16:05:01,464 - INFO - Codificaci√≥n detectada: iso-8859-1\n",
            "2025-12-25 16:05:01,466 - INFO - Archivo cargado exitosamente con codificaci√≥n: iso-8859-1\n",
            "2025-12-25 16:05:01,468 - INFO - Starting text cleaning.\n",
            "2025-12-25 16:05:01,471 - INFO - Text cleaning completed.\n",
            "2025-12-25 16:05:01,472 - INFO - Archivo cargado exitosamente. Documentos generados: 1\n",
            "2025-12-25 16:05:01,473 - INFO - Step 2: Chunking document\n",
            "2025-12-25 16:05:01,475 - INFO - Chunking document: AnnyLetter.txt\n",
            "2025-12-25 16:05:01,480 - INFO - Document divided into 45 chunks\n",
            "2025-12-25 16:05:01,483 - INFO - Step 3: Generating embeddings\n",
            "2025-12-25 16:05:01,484 - INFO - Generating embeddings for 45 chunks\n",
            "2025-12-25 16:05:03,214 - INFO - Embeddings generation completed\n",
            "2025-12-25 16:05:03,215 - INFO - Step 4: Running inference phase (ETI)\n",
            "2025-12-25 16:05:03,233 - INFO - Inferred 15 facts from chunk AnnyLetter.txt_993aa22e-5321-484f-935b-ba0bf9352de4\n",
            "2025-12-25 16:05:03,259 - INFO - Inferred 11 facts from chunk AnnyLetter.txt_344c3248-692b-4fd1-bf51-9847f1c996d5\n",
            "2025-12-25 16:05:03,286 - INFO - Inferred 12 facts from chunk AnnyLetter.txt_ebb895f5-ff32-4d71-97c8-543ea05a0ba0\n",
            "2025-12-25 16:05:03,317 - INFO - Inferred 9 facts from chunk AnnyLetter.txt_414eb496-5306-4d62-9be4-2c3afe5bb6ae\n",
            "2025-12-25 16:05:03,341 - INFO - Inferred 12 facts from chunk AnnyLetter.txt_329e6647-73a3-45b7-b847-8337263e24e2\n",
            "2025-12-25 16:05:03,370 - INFO - Inferred 11 facts from chunk AnnyLetter.txt_17489bad-2599-4c37-bae1-f20e3078d971\n",
            "2025-12-25 16:05:03,401 - INFO - Inferred 10 facts from chunk AnnyLetter.txt_d7e33266-3770-4fd1-9d87-33a731180a8a\n",
            "2025-12-25 16:05:03,423 - INFO - Inferred 7 facts from chunk AnnyLetter.txt_082d9194-ab7b-4c57-8a0d-a36e091f8ff8\n",
            "2025-12-25 16:05:03,442 - INFO - Inferred 13 facts from chunk AnnyLetter.txt_f4d4980c-1bb7-4c5f-9f01-6306c6475507\n",
            "2025-12-25 16:05:03,458 - INFO - Inferred 8 facts from chunk AnnyLetter.txt_0971f8cb-3f3f-4ad2-93c2-990b0145b974\n",
            "2025-12-25 16:05:03,476 - INFO - Inferred 12 facts from chunk AnnyLetter.txt_5271189d-9889-4f53-9f3f-7237c1133427\n",
            "2025-12-25 16:05:03,492 - INFO - Inferred 9 facts from chunk AnnyLetter.txt_97caa740-78c2-4ac9-b130-0f5a3c16210e\n",
            "2025-12-25 16:05:03,507 - INFO - Inferred 10 facts from chunk AnnyLetter.txt_6085c21f-e93e-40b1-bad0-a221a456170c\n",
            "2025-12-25 16:05:03,522 - INFO - Inferred 12 facts from chunk AnnyLetter.txt_acfdefd6-0a1f-469f-bb47-2879e5cdbb60\n",
            "2025-12-25 16:05:03,541 - INFO - Inferred 13 facts from chunk AnnyLetter.txt_421f5141-dc94-4a8d-b910-adec1a5ca323\n",
            "2025-12-25 16:05:03,558 - INFO - Inferred 7 facts from chunk AnnyLetter.txt_158af2de-9e9f-4ce8-a5b5-296c9db59632\n",
            "2025-12-25 16:05:03,577 - INFO - Inferred 7 facts from chunk AnnyLetter.txt_41ad66c3-b27f-484f-859b-964e7d7ef36a\n",
            "2025-12-25 16:05:03,594 - INFO - Inferred 10 facts from chunk AnnyLetter.txt_c86d6dd7-b1f8-41bc-adeb-393b44a14cde\n",
            "2025-12-25 16:05:03,611 - INFO - Inferred 13 facts from chunk AnnyLetter.txt_6b1b2500-807b-4b0b-a4fc-091cc1a7f465\n",
            "2025-12-25 16:05:03,627 - INFO - Inferred 11 facts from chunk AnnyLetter.txt_1bc6f720-8a47-4776-9278-2dc6145b02e5\n",
            "2025-12-25 16:05:03,646 - INFO - Inferred 11 facts from chunk AnnyLetter.txt_937a744d-2463-4b49-a106-d5dc23b63e15\n",
            "2025-12-25 16:05:03,663 - INFO - Inferred 12 facts from chunk AnnyLetter.txt_e6d51742-4aee-4e0b-8438-71063918597c\n",
            "2025-12-25 16:05:03,678 - INFO - Inferred 11 facts from chunk AnnyLetter.txt_13a9eead-1c74-4cfe-94ba-76b80e52ffc3\n",
            "2025-12-25 16:05:03,698 - INFO - Inferred 14 facts from chunk AnnyLetter.txt_f974b79e-72a0-49fa-a001-75cc084bf277\n",
            "2025-12-25 16:05:03,719 - INFO - Inferred 5 facts from chunk AnnyLetter.txt_f65c6f49-fbe3-4e0d-931c-b3c17a0fae12\n",
            "2025-12-25 16:05:03,736 - INFO - Inferred 10 facts from chunk AnnyLetter.txt_54ff0134-2723-47ec-9c39-96c0835fd010\n",
            "2025-12-25 16:05:03,752 - INFO - Inferred 9 facts from chunk AnnyLetter.txt_ffb11ca7-0653-46de-b67c-27b143ad9e67\n",
            "2025-12-25 16:05:03,769 - INFO - Inferred 7 facts from chunk AnnyLetter.txt_e3fba1c8-1d20-4c05-82d1-e336a5bcaac5\n",
            "2025-12-25 16:05:03,784 - INFO - Inferred 8 facts from chunk AnnyLetter.txt_6507d2c3-a65a-47f7-9762-4a85cb162c3f\n",
            "2025-12-25 16:05:03,802 - INFO - Inferred 9 facts from chunk AnnyLetter.txt_b724edc3-215b-440c-9157-6d48a0e8b561\n",
            "2025-12-25 16:05:03,821 - INFO - Inferred 10 facts from chunk AnnyLetter.txt_db72f815-1319-4b7f-845f-1ad853b63d6b\n",
            "2025-12-25 16:05:03,845 - INFO - Inferred 12 facts from chunk AnnyLetter.txt_a8d96fc4-c828-4a1e-9728-eb22ae30c853\n",
            "2025-12-25 16:05:03,876 - INFO - Inferred 9 facts from chunk AnnyLetter.txt_86f48201-3529-4ec4-a1e5-3b7cc11ac42e\n",
            "2025-12-25 16:05:03,916 - INFO - Inferred 9 facts from chunk AnnyLetter.txt_1f371be8-5b75-4d40-a003-23a7bb226a53\n",
            "2025-12-25 16:05:03,958 - INFO - Inferred 10 facts from chunk AnnyLetter.txt_f6ecea00-167f-4035-bbcc-d2da7a9d1f38\n",
            "2025-12-25 16:05:03,989 - INFO - Inferred 8 facts from chunk AnnyLetter.txt_19847714-201d-4f83-9776-27f8be353b6f\n",
            "2025-12-25 16:05:04,014 - INFO - Inferred 13 facts from chunk AnnyLetter.txt_650386e1-a02b-44d1-b20e-e31c6d9c01ae\n",
            "2025-12-25 16:05:04,040 - INFO - Inferred 4 facts from chunk AnnyLetter.txt_a9b3dd49-e5d9-41c0-b63a-ee3282613120\n",
            "2025-12-25 16:05:04,061 - INFO - Inferred 6 facts from chunk AnnyLetter.txt_dd035cf8-3e18-4074-977c-ce3e71ca7f52\n",
            "2025-12-25 16:05:04,085 - INFO - Inferred 6 facts from chunk AnnyLetter.txt_17dad364-de3b-48cc-85ad-0f863f5e5ee8\n",
            "2025-12-25 16:05:04,112 - INFO - Inferred 11 facts from chunk AnnyLetter.txt_69123f65-a74c-4a31-84d2-feff96e65ea3\n",
            "2025-12-25 16:05:04,134 - INFO - Inferred 12 facts from chunk AnnyLetter.txt_d07bc794-d018-40c7-b126-1fe19d81def8\n",
            "2025-12-25 16:05:04,156 - INFO - Inferred 8 facts from chunk AnnyLetter.txt_3cbb5bcb-e3bf-435c-9dbb-efb58038ad3c\n",
            "2025-12-25 16:05:04,179 - INFO - Inferred 5 facts from chunk AnnyLetter.txt_8ce932e5-c33b-459e-8832-2caa78dc0fec\n",
            "2025-12-25 16:05:04,196 - INFO - Inferred 5 facts from chunk AnnyLetter.txt_41976810-687e-4bff-8d33-bf036dd2e755\n",
            "2025-12-25 16:05:04,202 - INFO - Inference phase completed. Generated 436 facts\n",
            "2025-12-25 16:05:04,211 - INFO - Step 5: Setting up indexes\n",
            "2025-12-25 16:05:04,216 - INFO - Setting up all indexes\n",
            "2025-12-25 16:05:04,221 - INFO - Connecting to Neo4j at bolt://localhost:7687 with user neo4j\n",
            "2025-12-25 16:05:06,271 - INFO - Successfully connected to Neo4j\n",
            "2025-12-25 16:05:06,276 - INFO - Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE RANGE INDEX chunk_consecutive_idx IF NOT EXISTS FOR (e:Chunk) ON (e.chunk_id_consecutive)` has no effect.} {description: `RANGE INDEX chunk_consecutive_idx FOR (e:Chunk) ON (e.chunk_id_consecutive)` already exists.} {position: None} for query: '\\n        CREATE INDEX chunk_consecutive_idx IF NOT EXISTS\\n        FOR (c:Chunk)\\n        ON (c.chunk_id_consecutive)\\n        '\n",
            "2025-12-25 16:05:06,279 - INFO - Regular index 'chunk_consecutive_idx' created successfully\n",
            "2025-12-25 16:05:06,326 - INFO - Vector index 'chunk_embeddings' already exists\n",
            "2025-12-25 16:05:06,334 - INFO - Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE FULLTEXT INDEX chunk_content IF NOT EXISTS FOR (e:Chunk) ON EACH [e.page_content] OPTIONS {indexConfig: {`fulltext.analyzer`: \"spanish\", `fulltext.eventually_consistent`: false}}` has no effect.} {description: `FULLTEXT INDEX chunk_content FOR (e:Chunk) ON EACH [e.page_content]` already exists.} {position: None} for query: \"\\n        CREATE FULLTEXT INDEX chunk_content IF NOT EXISTS\\n        FOR (c:Chunk)\\n        ON EACH [c.page_content]\\n        OPTIONS {\\n            indexConfig: {\\n                `fulltext.analyzer`: 'spanish',\\n                `fulltext.eventually_consistent`: false\\n            }\\n        }\\n        \"\n",
            "2025-12-25 16:05:06,336 - INFO - Full-text index 'chunk_content' created successfully\n",
            "2025-12-25 16:05:06,337 - INFO - All indexes setup completed\n",
            "2025-12-25 16:05:06,338 - INFO - Step 6: Persisting chunks with pattern FILE_PAGE_CHUNK\n",
            "2025-12-25 16:05:06,338 - INFO - Using existing save_batch() for FILE_PAGE_CHUNK pattern\n",
            "2025-12-25 16:05:06,339 - INFO - Connecting to Neo4j at bolt://localhost:7687 with user neo4j\n",
            "2025-12-25 16:05:08,378 - INFO - Successfully connected to Neo4j\n",
            "2025-12-25 16:05:27,447 - INFO - Step 7: Persisting 436 facts\n",
            "2025-12-25 16:05:29,150 - INFO - Successfully saved 436 facts to Neo4j\n",
            "2025-12-25 16:05:29,152 - INFO - Successfully persisted 436 facts\n",
            "2025-12-25 16:05:29,152 - INFO - Step 8: Creating chunk relationships\n",
            "2025-12-25 16:05:29,282 - INFO - Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Statement.CartesianProduct} {category: PERFORMANCE} {title: This query builds a cartesian product between disconnected patterns.} {description: If a part of a query contains multiple disconnected patterns, this will build a cartesian product between all those parts. This may produce a large amount of data and slow down query processing. While occasionally intended, it may often be possible to reformulate the query that avoids the use of this cross product, perhaps by adding a relationship between the different parts or by using OPTIONAL MATCH (identifier is: (c2))} {position: line: 2, column: 5, offset: 5} for query: '\\n    MATCH (c1:Chunk),(c2:Chunk)\\n    WHERE c1.chunk_id_consecutive + 1 = c2.chunk_id_consecutive\\n    MERGE (c1)-[:NEXT_CHUNK]->(c2)\\n    '\n",
            "2025-12-25 16:05:29,329 - INFO - Chunk relationships created successfully\n",
            "2025-12-25 16:05:29,330 - INFO - Document ingestion completed. Created 45 chunks and 436 facts\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Documento ingerido exitosamente!\n",
            "   Total de chunks: 45\n"
          ]
        }
      ],
      "source": [
        "# Ingerir archivo de texto\n",
        "txt_file = data_path / \"AnnyLetter.txt\"\n",
        "\n",
        "if txt_file.exists():\n",
        "    print(f\"üì• Ingiriendo: {txt_file.name}\")\n",
        "    chunks_txt = ungraph.ingest_document(\n",
        "        txt_file,\n",
        "        chunk_size=500,\n",
        "        chunk_overlap=100,\n",
        "        clean_text=True\n",
        "    )\n",
        "    print(f\"‚úÖ Documento ingerido exitosamente!\")\n",
        "    print(f\"   Total de chunks: {len(chunks_txt)}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Archivo no encontrado: {txt_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Ingerir Word (.docx)\n",
        "\n",
        "Los documentos Word se procesan manteniendo la estructura y formato.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-25 16:05:29,348 - INFO - CUDA no disponible, usando CPU para embeddings.\n",
            "2025-12-25 16:05:29,351 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Ingiriendo: Usar s√≠mboles de silencio de corchea.docx\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-25 16:05:31,760 - INFO - Embedding service initialized with model: sentence-transformers/all-MiniLM-L6-v2\n",
            "2025-12-25 16:05:31,761 - INFO - Loading spaCy model: en_core_web_sm\n",
            "2025-12-25 16:05:32,209 - INFO - spaCy model loaded successfully\n",
            "2025-12-25 16:05:32,210 - INFO - Starting document ingestion: D:\\projects\\Ungraph\\src\\data\\Usar s√≠mboles de silencio de corchea.docx\n",
            "2025-12-25 16:05:32,210 - INFO - Using pattern: FILE_PAGE_CHUNK\n",
            "2025-12-25 16:05:32,211 - INFO - Step 1: Loading document\n",
            "2025-12-25 16:05:32,212 - INFO - Cargando archivo Word: D:\\projects\\Ungraph\\src\\data\\Usar s√≠mboles de silencio de corchea.docx\n",
            "2025-12-25 16:05:32,630 - INFO - Starting text cleaning.\n",
            "2025-12-25 16:05:32,633 - INFO - Text cleaning completed.\n",
            "2025-12-25 16:05:32,634 - INFO - Archivo cargado exitosamente. Documentos generados: 1\n",
            "2025-12-25 16:05:32,634 - INFO - Step 2: Chunking document\n",
            "2025-12-25 16:05:32,635 - INFO - Chunking document: Usar s√≠mboles de silencio de corchea.docx\n",
            "2025-12-25 16:05:32,641 - INFO - Document divided into 27 chunks\n",
            "2025-12-25 16:05:32,641 - INFO - Step 3: Generating embeddings\n",
            "2025-12-25 16:05:32,642 - INFO - Generating embeddings for 27 chunks\n",
            "2025-12-25 16:05:33,681 - INFO - Embeddings generation completed\n",
            "2025-12-25 16:05:33,682 - INFO - Step 4: Running inference phase (ETI)\n",
            "2025-12-25 16:05:33,717 - INFO - Inferred 26 facts from chunk Usar s√≠mboles de silencio de corchea.docx_0e56f8d1-e391-4f0d-9efe-f4f334765194\n",
            "2025-12-25 16:05:33,746 - INFO - Inferred 18 facts from chunk Usar s√≠mboles de silencio de corchea.docx_fc8835c4-0930-4646-9497-8623aa420c64\n",
            "2025-12-25 16:05:33,782 - INFO - Inferred 23 facts from chunk Usar s√≠mboles de silencio de corchea.docx_b7913a35-2eee-4e3c-b0c6-38b1b2b146cb\n",
            "2025-12-25 16:05:33,811 - INFO - Inferred 22 facts from chunk Usar s√≠mboles de silencio de corchea.docx_11d5e319-b67e-4f1f-b13d-19e65aa067db\n",
            "2025-12-25 16:05:33,842 - INFO - Inferred 23 facts from chunk Usar s√≠mboles de silencio de corchea.docx_23df68ce-62bd-4bb2-95a6-44c421ecf4ee\n",
            "2025-12-25 16:05:33,874 - INFO - Inferred 15 facts from chunk Usar s√≠mboles de silencio de corchea.docx_7aae898e-61a4-4e2a-9092-c9474555b227\n",
            "2025-12-25 16:05:33,902 - INFO - Inferred 17 facts from chunk Usar s√≠mboles de silencio de corchea.docx_c39cad3b-b0aa-41cb-af9f-b0878dba98e9\n",
            "2025-12-25 16:05:33,930 - INFO - Inferred 18 facts from chunk Usar s√≠mboles de silencio de corchea.docx_7645f9ff-c314-44ee-93c8-6ab125727018\n",
            "2025-12-25 16:05:33,952 - INFO - Inferred 17 facts from chunk Usar s√≠mboles de silencio de corchea.docx_ea169474-06d5-41e3-89f0-e3b3c5ddd2c2\n",
            "2025-12-25 16:05:33,978 - INFO - Inferred 14 facts from chunk Usar s√≠mboles de silencio de corchea.docx_c429600b-8e6d-49d7-afca-8a26084464ee\n",
            "2025-12-25 16:05:34,012 - INFO - Inferred 17 facts from chunk Usar s√≠mboles de silencio de corchea.docx_6275b936-6c40-4cb6-a2e1-52d76acd0520\n",
            "2025-12-25 16:05:34,045 - INFO - Inferred 13 facts from chunk Usar s√≠mboles de silencio de corchea.docx_40c6167d-a661-4455-94c0-b832366b29c9\n",
            "2025-12-25 16:05:34,073 - INFO - Inferred 20 facts from chunk Usar s√≠mboles de silencio de corchea.docx_d0049a49-8272-4009-80e0-c7ce04174d09\n",
            "2025-12-25 16:05:34,104 - INFO - Inferred 21 facts from chunk Usar s√≠mboles de silencio de corchea.docx_4e20f6ff-07df-485f-bfb4-b33fe3e004bb\n",
            "2025-12-25 16:05:34,128 - INFO - Inferred 26 facts from chunk Usar s√≠mboles de silencio de corchea.docx_ac773334-b5cb-4dd5-81a2-253fea3ae466\n",
            "2025-12-25 16:05:34,155 - INFO - Inferred 16 facts from chunk Usar s√≠mboles de silencio de corchea.docx_5e34cfe3-21d5-4273-9d81-d259d618f8ac\n",
            "2025-12-25 16:05:34,188 - INFO - Inferred 15 facts from chunk Usar s√≠mboles de silencio de corchea.docx_184c695f-4467-41fd-a25b-3d0dd6d7f7c5\n",
            "2025-12-25 16:05:34,219 - INFO - Inferred 19 facts from chunk Usar s√≠mboles de silencio de corchea.docx_a5c01620-41a6-4748-bd78-fc58a07b87e3\n",
            "2025-12-25 16:05:34,247 - INFO - Inferred 24 facts from chunk Usar s√≠mboles de silencio de corchea.docx_bc0edf62-4e1b-4161-9abb-608f9a69e4c3\n",
            "2025-12-25 16:05:34,275 - INFO - Inferred 25 facts from chunk Usar s√≠mboles de silencio de corchea.docx_4234c4c1-0618-4d82-9c01-3b16b458b36e\n",
            "2025-12-25 16:05:34,305 - INFO - Inferred 17 facts from chunk Usar s√≠mboles de silencio de corchea.docx_838259ed-7845-45fd-b26a-c460e69869d1\n",
            "2025-12-25 16:05:34,343 - INFO - Inferred 17 facts from chunk Usar s√≠mboles de silencio de corchea.docx_0af0d0cb-2e1e-4721-b700-87919aa53812\n",
            "2025-12-25 16:05:34,383 - INFO - Inferred 20 facts from chunk Usar s√≠mboles de silencio de corchea.docx_c8c013cb-1a85-46a4-a6a6-adc860e64dd1\n",
            "2025-12-25 16:05:34,429 - INFO - Inferred 19 facts from chunk Usar s√≠mboles de silencio de corchea.docx_a3207558-0ac3-459f-8cc5-685daffe0555\n",
            "2025-12-25 16:05:34,463 - INFO - Inferred 23 facts from chunk Usar s√≠mboles de silencio de corchea.docx_487d43a7-97f9-4f50-9a1a-0a18caf8f36c\n",
            "2025-12-25 16:05:34,513 - INFO - Inferred 21 facts from chunk Usar s√≠mboles de silencio de corchea.docx_d8022756-8a84-4781-8138-fac26418bcd0\n",
            "2025-12-25 16:05:34,549 - INFO - Inferred 9 facts from chunk Usar s√≠mboles de silencio de corchea.docx_a2789c42-4906-4496-aa71-b5a4cf4c4161\n",
            "2025-12-25 16:05:34,549 - INFO - Inference phase completed. Generated 515 facts\n",
            "2025-12-25 16:05:34,549 - INFO - Step 5: Setting up indexes\n",
            "2025-12-25 16:05:34,550 - INFO - Setting up all indexes\n",
            "2025-12-25 16:05:34,551 - INFO - Connecting to Neo4j at bolt://localhost:7687 with user neo4j\n",
            "2025-12-25 16:05:36,611 - INFO - Successfully connected to Neo4j\n",
            "2025-12-25 16:05:36,619 - INFO - Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE RANGE INDEX chunk_consecutive_idx IF NOT EXISTS FOR (e:Chunk) ON (e.chunk_id_consecutive)` has no effect.} {description: `RANGE INDEX chunk_consecutive_idx FOR (e:Chunk) ON (e.chunk_id_consecutive)` already exists.} {position: None} for query: '\\n        CREATE INDEX chunk_consecutive_idx IF NOT EXISTS\\n        FOR (c:Chunk)\\n        ON (c.chunk_id_consecutive)\\n        '\n",
            "2025-12-25 16:05:36,621 - INFO - Regular index 'chunk_consecutive_idx' created successfully\n",
            "2025-12-25 16:05:36,628 - INFO - Vector index 'chunk_embeddings' already exists\n",
            "2025-12-25 16:05:36,634 - INFO - Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE FULLTEXT INDEX chunk_content IF NOT EXISTS FOR (e:Chunk) ON EACH [e.page_content] OPTIONS {indexConfig: {`fulltext.analyzer`: \"spanish\", `fulltext.eventually_consistent`: false}}` has no effect.} {description: `FULLTEXT INDEX chunk_content FOR (e:Chunk) ON EACH [e.page_content]` already exists.} {position: None} for query: \"\\n        CREATE FULLTEXT INDEX chunk_content IF NOT EXISTS\\n        FOR (c:Chunk)\\n        ON EACH [c.page_content]\\n        OPTIONS {\\n            indexConfig: {\\n                `fulltext.analyzer`: 'spanish',\\n                `fulltext.eventually_consistent`: false\\n            }\\n        }\\n        \"\n",
            "2025-12-25 16:05:36,636 - INFO - Full-text index 'chunk_content' created successfully\n",
            "2025-12-25 16:05:36,637 - INFO - All indexes setup completed\n",
            "2025-12-25 16:05:36,637 - INFO - Step 6: Persisting chunks with pattern FILE_PAGE_CHUNK\n",
            "2025-12-25 16:05:36,639 - INFO - Using existing save_batch() for FILE_PAGE_CHUNK pattern\n",
            "2025-12-25 16:05:36,640 - INFO - Connecting to Neo4j at bolt://localhost:7687 with user neo4j\n",
            "2025-12-25 16:05:38,708 - INFO - Successfully connected to Neo4j\n",
            "2025-12-25 16:05:47,691 - INFO - Step 7: Persisting 515 facts\n",
            "2025-12-25 16:05:49,892 - INFO - Successfully saved 515 facts to Neo4j\n",
            "2025-12-25 16:05:49,893 - INFO - Successfully persisted 515 facts\n",
            "2025-12-25 16:05:49,894 - INFO - Step 8: Creating chunk relationships\n",
            "2025-12-25 16:05:50,631 - INFO - Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Statement.CartesianProduct} {category: PERFORMANCE} {title: This query builds a cartesian product between disconnected patterns.} {description: If a part of a query contains multiple disconnected patterns, this will build a cartesian product between all those parts. This may produce a large amount of data and slow down query processing. While occasionally intended, it may often be possible to reformulate the query that avoids the use of this cross product, perhaps by adding a relationship between the different parts or by using OPTIONAL MATCH (identifier is: (c2))} {position: line: 2, column: 5, offset: 5} for query: '\\n    MATCH (c1:Chunk),(c2:Chunk)\\n    WHERE c1.chunk_id_consecutive + 1 = c2.chunk_id_consecutive\\n    MERGE (c1)-[:NEXT_CHUNK]->(c2)\\n    '\n",
            "2025-12-25 16:05:50,839 - INFO - Chunk relationships created successfully\n",
            "2025-12-25 16:05:50,840 - INFO - Document ingestion completed. Created 27 chunks and 515 facts\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Documento ingerido exitosamente!\n",
            "   Total de chunks: 27\n",
            "\n",
            "üìã Metadatos del primer chunk:\n",
            "   - filename: Usar s√≠mboles de silencio de corchea.docx\n",
            "   - file_type: docx\n",
            "   - file_path: D:\\projects\\Ungraph\\src\\data\\Usar s√≠mboles de silencio de corchea.docx\n",
            "   - source: D:\\projects\\Ungraph\\src\\data\\Usar s√≠mboles de silencio de corchea.docx\n"
          ]
        }
      ],
      "source": [
        "# Ingerir archivo Word\n",
        "docx_file = data_path / \"Usar s√≠mboles de silencio de corchea.docx\"\n",
        "\n",
        "if docx_file.exists():\n",
        "    print(f\"üì• Ingiriendo: {docx_file.name}\")\n",
        "    chunks_docx = ungraph.ingest_document(\n",
        "        docx_file,\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        clean_text=True\n",
        "    )\n",
        "    print(f\"‚úÖ Documento ingerido exitosamente!\")\n",
        "    print(f\"   Total de chunks: {len(chunks_docx)}\")\n",
        "    \n",
        "    # Mostrar metadatos espec√≠ficos de Word\n",
        "    if chunks_docx:\n",
        "        print(f\"\\nüìã Metadatos del primer chunk:\")\n",
        "        for key, value in list(chunks_docx[0].metadata.items())[:5]:\n",
        "            print(f\"   - {key}: {value}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Archivo no encontrado: {docx_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Ingerir PDF (.pdf)\n",
        "\n",
        "Los PDFs se procesan usando IBM Docling, que proporciona mejor extracci√≥n de texto, estructura, tablas e im√°genes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-25 16:05:50,862 - INFO - CUDA no disponible, usando CPU para embeddings.\n",
            "2025-12-25 16:05:50,867 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Ingiriendo PDF: peach-et-al-2014-qsar-modeling-of-imbalanced-high-throughput-screening-data-in-pubchem.pdf\n",
            "   Tama√±o: 447.4 KB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-25 16:05:53,616 - INFO - Embedding service initialized with model: sentence-transformers/all-MiniLM-L6-v2\n",
            "2025-12-25 16:05:53,618 - INFO - Loading spaCy model: en_core_web_sm\n",
            "2025-12-25 16:05:54,550 - INFO - spaCy model loaded successfully\n",
            "2025-12-25 16:05:54,553 - INFO - Starting document ingestion: D:\\projects\\Ungraph\\src\\data\\peach-et-al-2014-qsar-modeling-of-imbalanced-high-throughput-screening-data-in-pubchem.pdf\n",
            "2025-12-25 16:05:54,554 - INFO - Using pattern: FILE_PAGE_CHUNK\n",
            "2025-12-25 16:05:54,555 - INFO - Step 1: Loading document\n",
            "2025-12-25 16:05:54,556 - INFO - Cargando archivo PDF con Docling: D:\\projects\\Ungraph\\src\\data\\peach-et-al-2014-qsar-modeling-of-imbalanced-high-throughput-screening-data-in-pubchem.pdf\n",
            "2025-12-25 16:05:55,364 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
            "2025-12-25 16:05:55,394 - INFO - Going to convert document batch...\n",
            "2025-12-25 16:05:55,395 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
            "2025-12-25 16:05:55,426 - INFO - Loading plugin 'docling_defaults'\n",
            "2025-12-25 16:05:55,431 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
            "2025-12-25 16:05:55,432 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
            "2025-12-25 16:05:55,455 - INFO - Loading plugin 'docling_defaults'\n",
            "2025-12-25 16:05:55,468 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
            "2025-12-25 16:05:55,470 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
            "2025-12-25 16:05:55,472 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
            "2025-12-25 16:05:55,474 - INFO - easyocr cannot be used because it is not installed.\n",
            "2025-12-25 16:05:55,783 - INFO - Accelerator device: 'cpu'\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:55,800 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:55,809 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:55,826 [RapidOCR] download_file.py:60: File exists and is valid: D:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:55,828 [RapidOCR] main.py:50: Using D:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:56,040 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:56,042 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:56,046 [RapidOCR] download_file.py:60: File exists and is valid: D:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:56,047 [RapidOCR] main.py:50: Using D:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:56,128 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:56,129 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:56,160 [RapidOCR] download_file.py:60: File exists and is valid: D:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:56,160 [RapidOCR] main.py:50: Using D:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "2025-12-25 16:05:56,379 - INFO - Auto OCR model selected rapidocr with torch.\n",
            "2025-12-25 16:05:56,405 - INFO - Loading plugin 'docling_defaults'\n",
            "2025-12-25 16:05:56,411 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
            "2025-12-25 16:05:56,412 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
            "2025-12-25 16:05:56,425 - INFO - Accelerator device: 'cpu'\n",
            "2025-12-25 16:05:56,881 - INFO - Loading plugin 'docling_defaults'\n",
            "2025-12-25 16:05:56,886 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
            "2025-12-25 16:05:56,888 - INFO - Registered table structure engines: ['docling_tableformer']\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "2025-12-25 16:06:03,518 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "2025-12-25 16:06:04,707 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "2025-12-25 16:06:17,098 - INFO - Accelerator device: 'cpu'\n",
            "2025-12-25 16:06:17,809 - INFO - Processing document peach-et-al-2014-qsar-modeling-of-imbalanced-high-throughput-screening-data-in-pubchem.pdf\n"
          ]
        }
      ],
      "source": [
        "# Buscar archivo PDF en la carpeta data\n",
        "pdf_files = list(data_path.glob(\"*.pdf\"))\n",
        "\n",
        "if pdf_files:\n",
        "    pdf_file = pdf_files[0]\n",
        "    print(f\"üì• Ingiriendo PDF: {pdf_file.name}\")\n",
        "    print(f\"   Tama√±o: {pdf_file.stat().st_size / 1024:.1f} KB\")\n",
        "    \n",
        "    try:\n",
        "        chunks_pdf = ungraph.ingest_document(\n",
        "            pdf_file,\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200,\n",
        "            clean_text=True\n",
        "        )\n",
        "        print(f\"‚úÖ PDF ingerido exitosamente!\")\n",
        "        print(f\"   Total de chunks: {len(chunks_pdf)}\")\n",
        "        \n",
        "        # Mostrar metadatos espec√≠ficos de PDF (pueden incluir page_number, document_structure, etc.)\n",
        "        if chunks_pdf:\n",
        "            print(f\"\\nüìã Metadatos del primer chunk:\")\n",
        "            for key, value in list(chunks_pdf[0].metadata.items())[:5]:\n",
        "                print(f\"   - {key}: {value}\")\n",
        "    except ImportError as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        print(\"   Instala langchain-docling con: pip install langchain-docling\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error procesando PDF: {e}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No se encontraron archivos PDF en la carpeta data\")\n",
        "    print(\"   Puedes a√±adir un PDF para probar esta funcionalidad\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 4: Obtener Recomendaciones de Chunking\n",
        "\n",
        "Antes de ingerir, podemos obtener recomendaciones sobre la mejor estrategia de chunking para cada documento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtener recomendaci√≥n para un archivo\n",
        "markdown_file = data_path / \"110225.md\"\n",
        "\n",
        "if markdown_file.exists():\n",
        "    print(f\"üìä Analizando: {markdown_file.name}\")\n",
        "    recommendation = ungraph.suggest_chunking_strategy(markdown_file)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Recomendaci√≥n obtenida:\")\n",
        "    print(f\"   Estrategia: {recommendation.strategy}\")\n",
        "    print(f\"   Chunk size: {recommendation.chunk_size}\")\n",
        "    print(f\"   Chunk overlap: {recommendation.chunk_overlap}\")\n",
        "    print(f\"   Quality score: {recommendation.quality_score:.2f}\")\n",
        "    print(f\"\\nüìù Explicaci√≥n:\")\n",
        "    print(recommendation.explanation)\n",
        "    \n",
        "    # Usar la recomendaci√≥n para ingerir\n",
        "    print(f\"\\nüí° Puedes usar esta recomendaci√≥n al ingerir:\")\n",
        "    print(f\"   chunks = ungraph.ingest_document(\")\n",
        "    print(f\"       '{markdown_file.name}',\")\n",
        "    print(f\"       chunk_size={recommendation.chunk_size},\")\n",
        "    print(f\"       chunk_overlap={recommendation.chunk_overlap}\")\n",
        "    print(f\"   )\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Archivo no encontrado: {markdown_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 5: Verificar Datos en el Grafo\n",
        "\n",
        "Verifiquemos que los documentos se guardaron correctamente en Neo4j.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar datos en el grafo\n",
        "from src.utils.graph_operations import graph_session\n",
        "\n",
        "driver = graph_session()\n",
        "try:\n",
        "    with driver.session() as session:\n",
        "        # Contar nodos por tipo\n",
        "        result = session.run(\"MATCH (f:File) RETURN count(f) as count\")\n",
        "        file_count = result.single()[\"count\"]\n",
        "        \n",
        "        result = session.run(\"MATCH (p:Page) RETURN count(p) as count\")\n",
        "        page_count = result.single()[\"count\"]\n",
        "        \n",
        "        result = session.run(\"MATCH (c:Chunk) RETURN count(c) as count\")\n",
        "        chunk_count = result.single()[\"count\"]\n",
        "        \n",
        "        print(\"üìä Datos en el grafo:\")\n",
        "        print(f\"   Files: {file_count}\")\n",
        "        print(f\"   Pages: {page_count}\")\n",
        "        print(f\"   Chunks: {chunk_count}\")\n",
        "        \n",
        "        # Listar archivos ingeridos\n",
        "        if file_count > 0:\n",
        "            result = session.run(\"MATCH (f:File) RETURN f.filename as filename ORDER BY f.filename\")\n",
        "            print(f\"\\nüìÑ Archivos ingeridos:\")\n",
        "            for record in result:\n",
        "                print(f\"   - {record['filename']}\")\n",
        "finally:\n",
        "    driver.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resumen y Mejores Pr√°cticas\n",
        "\n",
        "### Formatos Soportados\n",
        "\n",
        "| Formato | Extensi√≥n | Caracter√≠sticas |\n",
        "|---------|-----------|----------------|\n",
        "| Markdown | .md | Estructura preservada, headers detectados |\n",
        "| Texto | .txt | Detecci√≥n autom√°tica de encoding |\n",
        "| Word | .docx | Estructura y formato preservados |\n",
        "| PDF | .pdf | Extracci√≥n avanzada con Docling (tablas, im√°genes, estructura) |\n",
        "\n",
        "### Mejores Pr√°cticas\n",
        "\n",
        "1. **Usar recomendaciones**: `suggest_chunking_strategy()` ayuda a elegir par√°metros √≥ptimos\n",
        "2. **Ajustar chunk_size**: \n",
        "   - Textos largos: 1000-2000 caracteres\n",
        "   - Textos cortos: 500-1000 caracteres\n",
        "3. **Overlap razonable**: 10-20% del chunk_size (ej: 200 para chunk_size=1000)\n",
        "4. **Limpiar texto**: `clean_text=True` mejora la calidad de embeddings\n",
        "5. **Verificar carga**: Siempre verifica que los documentos se cargaron correctamente\n",
        "\n",
        "### Siguiente Paso\n",
        "\n",
        "Una vez que has ingerido documentos, contin√∫a con:\n",
        "- **2.1 Graph Pattern Construction** - Construir estructuras de grafo personalizadas\n",
        "- **2.2 Smart Chunking Strategies** - Optimizar estrategias de chunking\n",
        "- **3.1 Entity Extraction & Facts** - Extraer entidades y facts (fase Inference)\n",
        "\n",
        "## Referencias\n",
        "\n",
        "- [Gu√≠a de Ingesta](../../docs/guides/ingestion.md)\n",
        "- [API P√∫blica](../../docs/api/public-api.md)\n",
        "- [Document Loader Service](../../src/infrastructure/services/langchain_document_loader_service.py)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
