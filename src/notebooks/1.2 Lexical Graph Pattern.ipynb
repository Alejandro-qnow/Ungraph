{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1.2: Lexical Graph Pattern\n",
    "\n",
    "Este notebook prueba el patr√≥n **LEXICAL_GRAPH** con datos reales.\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "1. **Limpiar el grafo** antes de empezar\n",
    "2. **Crear patr√≥n LEXICAL_GRAPH** con entidades y chunks\n",
    "3. **Ingerir 3 documentos** usando el patr√≥n l√©xico\n",
    "4. **Explorar el grafo** creado\n",
    "5. **Probar b√∫squedas** con los patrones GraphRAG implementados\n",
    "\n",
    "## Patr√≥n LEXICAL_GRAPH\n",
    "\n",
    "- **Entity**: Entidades extra√≠das (PERSON, ORGANIZATION, CONCEPT, etc.)\n",
    "- **Chunk**: Fragmentos de texto\n",
    "- **MENTIONS**: Chunk menciona Entity\n",
    "- **RELATED_TO**: Entity relacionada con otra Entity\n",
    "\n",
    "**Estructura:**\n",
    "```\n",
    "Chunk -[:MENTIONS]-> Entity -[:RELATED_TO]-> Entity\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def add_src_to_path(path_folder: str):\n",
    "    ''' \n",
    "    Helper function for adding the \"path_folder\" directory to the path.\n",
    "    in order to work on notebooks and scripts\n",
    "    '''\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "\n",
    "    base_path = Path().resolve()\n",
    "    for parent in [base_path] + list(base_path.parents):\n",
    "        candidate = parent / path_folder\n",
    "        if candidate.exists():\n",
    "            parent_dir = candidate.parent\n",
    "            if str(parent_dir) not in sys.path:\n",
    "                sys.path.insert(0, str(parent_dir))\n",
    "                print(f\"Path Folder parent added: {parent_dir}\")\n",
    "            if str(candidate) not in sys.path:\n",
    "                sys.path.append(str(candidate))\n",
    "                print(f\"Path Folder {path_folder} added: {candidate}\")\n",
    "            return\n",
    "    print(f\"Not found '{path_folder}' folder on the hierarchy of directories\")\n",
    "\n",
    "# Agregar carpetas necesarias al path\n",
    "add_src_to_path(path_folder=\"src\")\n",
    "add_src_to_path(path_folder=\"src/utils\")\n",
    "add_src_to_path(path_folder=\"src/data\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Importar librer√≠as necesarias\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Importar handlers\n",
    "from src.utils.handlers import find_in_project\n",
    "\n",
    "# Importar ungraph\n",
    "try:\n",
    "    import ungraph\n",
    "    print(\"‚úÖ Ungraph importado como paquete instalado\")\n",
    "except ImportError:\n",
    "    import src\n",
    "    ungraph = src\n",
    "    print(\"‚úÖ Ungraph importado desde src/ (modo desarrollo)\")\n",
    "\n",
    "# Importar servicios para limpieza\n",
    "from infrastructure.services.neo4j_index_service import Neo4jIndexService\n",
    "\n",
    "# Importar patrones\n",
    "from domain.value_objects.graph_pattern import GraphPattern, NodeDefinition, RelationshipDefinition\n",
    "\n",
    "print(f\"üì¶ Ungraph version: {ungraph.__version__}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Configuraci√≥n y Limpieza\n",
    "\n",
    "Configuramos Neo4j y limpiamos el grafo antes de empezar."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Configurar Neo4j\n",
    "ungraph.configure(\n",
    "    neo4j_uri=\"bolt://localhost:7687\",\n",
    "    neo4j_user=\"neo4j\",\n",
    "    neo4j_password=\"Ungraph22\",  # ‚ö†Ô∏è CAMBIAR: Usa tu contrase√±a real\n",
    "    neo4j_database=\"neo4j\",\n",
    "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Configuraci√≥n completada\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Limpiar el grafo antes de empezar\n",
    "print(\"üßπ Limpiando grafo...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "index_service = Neo4jIndexService()\n",
    "\n",
    "# Limpiar todos los nodos y relaciones\n",
    "try:\n",
    "    index_service.clean_graph()\n",
    "    print(\"‚úÖ Grafo limpiado (todos los nodos y relaciones eliminados)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error al limpiar grafo: {e}\")\n",
    "\n",
    "# Eliminar todos los √≠ndices\n",
    "try:\n",
    "    index_service.drop_all_indexes()\n",
    "    print(\"‚úÖ √çndices eliminados\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error al eliminar √≠ndices: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Limpieza completada. Listo para ingesta.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Preparar Documentos\n",
    "\n",
    "Localizamos los 3 documentos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Encontrar carpeta de datos\n",
    "data_path = find_in_project(\n",
    "    target=\"data\",\n",
    "    search_type=\"folder\",\n",
    "    project_root=None\n",
    ")\n",
    "\n",
    "if data_path:\n",
    "    print(f\"‚úÖ Carpeta de datos encontrada: {data_path}\")\n",
    "    \n",
    "    # Seleccionar los 3 documentos de prueba\n",
    "    test_files = [\n",
    "        data_path / \"110225.md\",\n",
    "        data_path / \"AnnyLetter.txt\",\n",
    "        data_path / \"Usar s√≠mboles de silencio de corchea.docx\"\n",
    "    ]\n",
    "    \n",
    "    # Verificar que existen\n",
    "    available_files = [f for f in test_files if f.exists()]\n",
    "    print(f\"\\nüìÑ Archivos disponibles ({len(available_files)}/{len(test_files)}):\")\n",
    "    for f in available_files:\n",
    "        print(f\"   ‚úÖ {f.name}\")\n",
    "    \n",
    "    for f in test_files:\n",
    "        if not f.exists():\n",
    "            print(f\"   ‚ö†Ô∏è  No encontrado: {f.name}\")\n",
    "else:\n",
    "    print(\"‚ùå Carpeta de datos no encontrada\")\n",
    "    available_files = []"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Crear Patr√≥n LEXICAL_GRAPH\n",
    "\n",
    "Definimos el patr√≥n l√©xico con entidades y chunks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Crear patr√≥n LEXICAL_GRAPH\n",
    "print(\"üìù CREANDO PATR√ìN LEXICAL_GRAPH\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Nodo Entity\n",
    "entity_node = NodeDefinition(\n",
    "    label=\"Entity\",\n",
    "    required_properties={\n",
    "        \"name\": str,\n",
    "        \"type\": str  # \"PERSON\", \"ORGANIZATION\", \"CONCEPT\", etc.\n",
    "    },\n",
    "    optional_properties={\n",
    "        \"description\": str,\n",
    "        \"frequency\": int\n",
    "    },\n",
    "    indexes=[\"name\", \"type\"]\n",
    ")\n",
    "\n",
    "# Nodo Chunk\n",
    "chunk_node = NodeDefinition(\n",
    "    label=\"Chunk\",\n",
    "    required_properties={\n",
    "        \"chunk_id\": str,\n",
    "        \"content\": str,\n",
    "        \"embeddings\": list,\n",
    "        \"embeddings_dimensions\": int\n",
    "    },\n",
    "    optional_properties={\n",
    "        \"chunk_id_consecutive\": int,\n",
    "        \"source_file\": str\n",
    "    },\n",
    "    indexes=[\"chunk_id\"]\n",
    ")\n",
    "\n",
    "# Relaci√≥n: Chunk menciona Entity\n",
    "mentions_rel = RelationshipDefinition(\n",
    "    from_node=\"Chunk\",\n",
    "    to_node=\"Entity\",\n",
    "    relationship_type=\"MENTIONS\",\n",
    "    properties={\"count\": int},  # N√∫mero de veces que se menciona\n",
    "    direction=\"OUTGOING\"\n",
    ")\n",
    "\n",
    "# Relaci√≥n: Entity relacionada con otra Entity\n",
    "related_rel = RelationshipDefinition(\n",
    "    from_node=\"Entity\",\n",
    "    to_node=\"Entity\",\n",
    "    relationship_type=\"RELATED_TO\",\n",
    "    properties={\"strength\": float},  # Fuerza de la relaci√≥n\n",
    "    direction=\"OUTGOING\"\n",
    ")\n",
    "\n",
    "LEXICAL_GRAPH_PATTERN = GraphPattern(\n",
    "    name=\"LEXICAL_GRAPH\",\n",
    "    description=\"Grafo l√©xico con entidades extra√≠das y sus relaciones. √ötil para an√°lisis sem√°ntico.\",\n",
    "    node_definitions=[entity_node, chunk_node],\n",
    "    relationship_definitions=[mentions_rel, related_rel],\n",
    "    search_patterns=[\"basic\", \"hybrid\", \"pattern_matching\"]\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Patr√≥n creado: {LEXICAL_GRAPH_PATTERN.name}\")\n",
    "print(f\"   Nodos: {[n.label for n in LEXICAL_GRAPH_PATTERN.node_definitions]}\")\n",
    "print(f\"   Relaciones: {[r.relationship_type for r in LEXICAL_GRAPH_PATTERN.relationship_definitions]}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: Ingesta con Patr√≥n LEXICAL_GRAPH\n",
    "\n",
    "Ingerimos los documentos usando el patr√≥n l√©xico."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Ingesta con patr√≥n LEXICAL_GRAPH\n",
    "print(\"üì• INGESTA CON PATR√ìN LEXICAL_GRAPH\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_chunks_lexical = []\n",
    "\n",
    "for file_path in available_files:\n",
    "    print(f\"\\nüìÑ Procesando: {file_path.name}\")\n",
    "    try:\n",
    "        chunks = ungraph.ingest_document(\n",
    "            file_path,\n",
    "            pattern=LEXICAL_GRAPH_PATTERN,\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            clean_text=True\n",
    "        )\n",
    "        all_chunks_lexical.extend(chunks)\n",
    "        print(f\"   ‚úÖ {len(chunks)} chunks creados\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total chunks con patr√≥n LEXICAL_GRAPH: {len(all_chunks_lexical)}\")\n",
    "print(\"\\nüí° Nota: Este patr√≥n requiere extracci√≥n de entidades.\")\n",
    "print(\"   En una implementaci√≥n completa, se usar√≠a NER para extraer entidades.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: Explorar Grafo\n",
    "\n",
    "Exploramos la estructura del grafo creado."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Explorar estructura del grafo\n",
    "from src.utils.graph_operations import graph_session\n",
    "\n",
    "driver = graph_session()\n",
    "with driver.session() as session:\n",
    "    # Contar nodos por tipo\n",
    "    result = session.run(\"\"\"\n",
    "        MATCH (n)\n",
    "        RETURN labels(n)[0] as label, count(n) as count\n",
    "        ORDER BY count DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"üìä ESTRUCTURA DEL GRAFO:\")\n",
    "    print(\"=\" * 80)\n",
    "    for record in result:\n",
    "        print(f\"   {record['label']}: {record['count']} nodos\")\n",
    "    \n",
    "    # Contar relaciones\n",
    "    result = session.run(\"\"\"\n",
    "        MATCH ()-[r]->()\n",
    "        RETURN type(r) as rel_type, count(r) as count\n",
    "        ORDER BY count DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\nüîó RELACIONES:\")\n",
    "    for record in result:\n",
    "        print(f\"   {record['rel_type']}: {record['count']} relaciones\")\n",
    "\n",
    "driver.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4.1: Visualizar Grafo\n",
    "\n",
    "Visualizamos el grafo usando yFiles for Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Importar funciones de visualizaci√≥n\n",
    "from src.notebooks.graph_visualization import (\n",
    "    visualize_file_page_chunk_pattern,\n",
    "    visualize_simple_chunk_pattern,\n",
    "    visualize_lexical_graph_pattern,\n",
    "    visualize_hierarchical_pattern,\n",
    "    visualize_sequential_chunks_pattern,\n",
    "    visualize_pattern_structure,\n",
    "    visualize_custom_query\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Funciones de visualizaci√≥n importadas\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 5: Probar B√∫squedas\n",
    "\n",
    "Probamos los patrones de b√∫squeda GraphRAG implementados."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Probar b√∫squedas\n",
    "test_query = \"test\"\n",
    "print(f\"üîç PROBANDO B√öSQUEDAS CON QUERY: '{test_query}'\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Basic Retriever\n",
    "print(\"\\n1. Basic Retriever:\")\n",
    "try:\n",
    "    results = ungraph.search_with_pattern(\n",
    "        test_query,\n",
    "        pattern_type=\"basic\",\n",
    "        limit=3\n",
    "    )\n",
    "    print(f\"   ‚úÖ {len(results)} resultados\")\n",
    "    if results:\n",
    "        print(f\"   Score promedio: {sum(r.score for r in results) / len(results):.3f}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "# 2. Metadata Filtering\n",
    "print(\"\\n2. Metadata Filtering:\")\n",
    "try:\n",
    "    # Obtener un filename del grafo\n",
    "    driver = graph_session()\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"MATCH (f:File) RETURN f.filename as filename LIMIT 1\")\n",
    "        record = result.single()\n",
    "        if not record:\n",
    "            # Intentar con Chunk si no hay File\n",
    "            result = session.run(\"MATCH (c:Chunk) RETURN c.source_file as filename LIMIT 1\")\n",
    "            record = result.single()\n",
    "        if record:\n",
    "            filename = record[\"filename\"]\n",
    "            results = ungraph.search_with_pattern(\n",
    "                test_query,\n",
    "                pattern_type=\"metadata_filtering\",\n",
    "                metadata_filters={\"filename\": filename},\n",
    "                limit=3\n",
    "            )\n",
    "            print(f\"   ‚úÖ {len(results)} resultados (filtrado por '{filename}')\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  No hay archivos en el grafo\")\n",
    "    driver.close()\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "# 3. Parent-Child Retriever (si aplica)\n",
    "print(\"\\n3. Parent-Child Retriever:\")\n",
    "try:\n",
    "    results = ungraph.search_with_pattern(\n",
    "        test_query,\n",
    "        pattern_type=\"parent_child\",\n",
    "        parent_label=\"Page\",\n",
    "        child_label=\"Chunk\",\n",
    "        relationship_type=\"HAS_CHUNK\",\n",
    "        limit=3\n",
    "    )\n",
    "    print(f\"   ‚úÖ {len(results)} resultados\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  No aplica para este patr√≥n: {e}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualizar patr√≥n LEXICAL_GRAPH\n",
    "print(\"üé® VISUALIZANDO PATR√ìN LEXICAL_GRAPH\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "driver = graph_session()\n",
    "try:\n",
    "    visualize_lexical_graph_pattern(driver, limit_entities=10, limit_chunks=15)\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error al visualizar: {e}\")\n",
    "    print(\"üí° Aseg√∫rate de tener yfiles_jupyter_graphs_for_neo4j instalado\")\n",
    "finally:\n",
    "    driver.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 6: Resumen\n",
    "\n",
    "Resumen del patr√≥n LEXICAL_GRAPH."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"üìä RESUMEN DEL PATR√ìN LEXICAL_GRAPH\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Chunks creados: {len(all_chunks_lexical)}\")\n",
    "print(f\"Estructura: Chunk -[:MENTIONS]-> Entity -[:RELATED_TO]-> Entity\")\n",
    "print(f\"Uso recomendado: An√°lisis sem√°ntico, extracci√≥n de entidades\")\n",
    "print(\"\\n‚úÖ Notebook completado exitosamente\")"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}