{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1.2 Document Formats & Metadata - Ungraph\n",
        "\n",
        "Este notebook compara los diferentes formatos de documentos soportados y c√≥mo se extraen sus metadatos.\n",
        "\n",
        "## Objetivos\n",
        "\n",
        "1. **Comparar formatos** - Markdown, TXT, Word, PDF lado a lado\n",
        "2. **Extracci√≥n de metadatos** - Qu√© metadatos se extraen de cada formato\n",
        "3. **Manejo de encoding** - Detecci√≥n autom√°tica en archivos de texto\n",
        "4. **Mejores pr√°cticas** - Cu√°ndo usar cada formato\n",
        "\n",
        "**Referencias:**\n",
        "- [Gu√≠a de Ingesta](../../docs/guides/ingestion.md)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ Ungraph version: 0.1.0\n"
          ]
        }
      ],
      "source": [
        "def add_src_to_path(path_folder: str):\n",
        "    import sys\n",
        "    from pathlib import Path\n",
        "    base_path = Path().resolve()\n",
        "    for parent in [base_path] + list(base_path.parents):\n",
        "        candidate = parent / path_folder\n",
        "        if candidate.exists():\n",
        "            parent_dir = candidate.parent\n",
        "            if str(parent_dir) not in sys.path:\n",
        "                sys.path.insert(0, str(parent_dir))\n",
        "            if str(candidate) not in sys.path:\n",
        "                sys.path.append(str(candidate))\n",
        "            return\n",
        "\n",
        "add_src_to_path(path_folder=\"src\")\n",
        "add_src_to_path(path_folder=\"src/utils\")\n",
        "add_src_to_path(path_folder=\"src/data\")\n",
        "\n",
        "try:\n",
        "    import ungraph\n",
        "except ImportError:\n",
        "    import src\n",
        "    ungraph = src\n",
        "\n",
        "from infrastructure.services.langchain_document_loader_service import LangChainDocumentLoaderService\n",
        "from src.utils.handlers import find_in_project\n",
        "\n",
        "print(f\"üì¶ Ungraph version: {ungraph.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 1: Comparar Carga de Formatos\n",
        "\n",
        "Carguemos el mismo contenido conceptual en diferentes formatos y comparemos los resultados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-12-25 16:05:16,420 - INFO - Encontrado: D:\\projects\\Ungraph\\src\\data\n",
            "2025-12-25 16:05:16,421 - INFO - Cargando archivo Markdown: D:\\projects\\Ungraph\\src\\data\\110225.md\n",
            "2025-12-25 16:05:19,713 - INFO - Archivo cargado exitosamente. Documentos generados: 1\n",
            "2025-12-25 16:05:19,716 - INFO - Cargando archivo de texto: D:\\projects\\Ungraph\\src\\data\\AnnyLetter.txt\n",
            "2025-12-25 16:05:19,742 - INFO - Codificaci√≥n detectada por chardet: iso-8859-1 (confianza: 73.00%)\n",
            "2025-12-25 16:05:19,743 - INFO - Codificaci√≥n detectada: iso-8859-1\n",
            "2025-12-25 16:05:19,744 - INFO - Archivo cargado exitosamente con codificaci√≥n: iso-8859-1\n",
            "2025-12-25 16:05:19,745 - INFO - Archivo cargado exitosamente. Documentos generados: 1\n",
            "2025-12-25 16:05:19,746 - INFO - Cargando archivo Word: D:\\projects\\Ungraph\\src\\data\\Usar s√≠mboles de silencio de corchea.docx\n",
            "2025-12-25 16:05:20,507 - INFO - Archivo cargado exitosamente. Documentos generados: 1\n",
            "2025-12-25 16:05:20,508 - INFO - Cargando archivo PDF con Docling: D:\\projects\\Ungraph\\src\\data\\peach-et-al-2014-qsar-modeling-of-imbalanced-high-throughput-screening-data-in-pubchem.pdf\n",
            "2025-12-25 16:05:21,768 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
            "2025-12-25 16:05:26,403 - INFO - Going to convert document batch...\n",
            "2025-12-25 16:05:26,405 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
            "2025-12-25 16:05:26,489 - INFO - Loading plugin 'docling_defaults'\n",
            "2025-12-25 16:05:26,575 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
            "2025-12-25 16:05:26,576 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
            "2025-12-25 16:05:26,600 - INFO - Loading plugin 'docling_defaults'\n",
            "2025-12-25 16:05:26,843 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
            "2025-12-25 16:05:26,845 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
            "2025-12-25 16:05:26,847 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
            "2025-12-25 16:05:26,849 - INFO - easyocr cannot be used because it is not installed.\n",
            "2025-12-25 16:05:32,117 - INFO - Accelerator device: 'cpu'\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:32,148 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:32,363 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:32,385 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.4.0/torch/PP-OCRv4/det/ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:35,964 [RapidOCR] download_file.py:82: Download size: 13.83MB\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:36,476 [RapidOCR] download_file.py:95: Successfully saved to: D:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:36,478 [RapidOCR] main.py:50: Using D:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:37,122 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:37,124 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:37,125 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.4.0/torch/PP-OCRv4/cls/ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:39,009 [RapidOCR] download_file.py:82: Download size: 0.56MB\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:39,043 [RapidOCR] download_file.py:95: Successfully saved to: D:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:39,046 [RapidOCR] main.py:50: Using D:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:39,204 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:39,205 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:39,207 [RapidOCR] download_file.py:68: Initiating download: https://www.modelscope.cn/models/RapidAI/RapidOCR/resolve/v3.4.0/torch/PP-OCRv4/rec/ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:41,108 [RapidOCR] download_file.py:82: Download size: 25.67MB\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:42,112 [RapidOCR] download_file.py:95: Successfully saved to: D:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "\u001b[32m[INFO] 2025-12-25 16:05:42,116 [RapidOCR] main.py:50: Using D:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
            "2025-12-25 16:05:42,457 - INFO - Auto OCR model selected rapidocr with torch.\n",
            "2025-12-25 16:05:42,518 - INFO - Loading plugin 'docling_defaults'\n",
            "2025-12-25 16:05:42,613 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
            "2025-12-25 16:05:42,616 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
            "2025-12-25 16:05:42,709 - INFO - Accelerator device: 'cpu'\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "2025-12-25 16:05:43,465 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "d:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jefai\\.cache\\huggingface\\hub\\models--docling-project--docling-layout-heron. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "2025-12-25 16:05:50,458 - INFO - Loading plugin 'docling_defaults'\n",
            "2025-12-25 16:05:50,515 - WARNING - The plugin langchain_docling will not be loaded because Docling is being executed with allow_external_plugins=false.\n",
            "2025-12-25 16:05:50,515 - INFO - Registered table structure engines: ['docling_tableformer']\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "2025-12-25 16:05:51,405 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "d:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jefai\\.cache\\huggingface\\hub\\models--docling-project--docling-models. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "2025-12-25 16:05:51,487 - WARNING - Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "2025-12-25 16:06:05,112 - INFO - Accelerator device: 'cpu'\n",
            "2025-12-25 16:06:05,606 - INFO - Processing document peach-et-al-2014-qsar-modeling-of-imbalanced-high-throughput-screening-data-in-pubchem.pdf\n",
            "2025-12-25 16:06:49,228 - INFO - Finished converting document peach-et-al-2014-qsar-modeling-of-imbalanced-high-throughput-screening-data-in-pubchem.pdf in 87.45 sec.\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (640 > 512). Running this sequence through the model will result in indexing errors\n",
            "2025-12-25 16:06:50,324 - INFO - PDF cargado exitosamente con Docling. Documentos generados: 80\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Formatos cargados: ['Markdown', 'Texto', 'Word', 'PDF']\n"
          ]
        }
      ],
      "source": [
        "# Encontrar archivos de ejemplo\n",
        "data_path = find_in_project(\"data\", \"folder\", None)\n",
        "\n",
        "# Crear servicio de carga\n",
        "loader_service = LangChainDocumentLoaderService()\n",
        "\n",
        "# Cargar diferentes formatos\n",
        "formats_data = {}\n",
        "\n",
        "# Markdown\n",
        "md_file = data_path / \"110225.md\"\n",
        "if md_file.exists():\n",
        "    docs_md = loader_service.load(md_file, clean=False)\n",
        "    formats_data[\"Markdown\"] = {\"file\": md_file, \"docs\": docs_md}\n",
        "\n",
        "# Texto\n",
        "txt_file = data_path / \"AnnyLetter.txt\"\n",
        "if txt_file.exists():\n",
        "    docs_txt = loader_service.load(txt_file, clean=False)\n",
        "    formats_data[\"Texto\"] = {\"file\": txt_file, \"docs\": docs_txt}\n",
        "\n",
        "# Word\n",
        "docx_file = data_path / \"Usar s√≠mboles de silencio de corchea.docx\"\n",
        "if docx_file.exists():\n",
        "    docs_docx = loader_service.load(docx_file, clean=False)\n",
        "    formats_data[\"Word\"] = {\"file\": docx_file, \"docs\": docs_docx}\n",
        "\n",
        "# PDF\n",
        "pdf_files = list(data_path.glob(\"*.pdf\"))\n",
        "if pdf_files:\n",
        "    pdf_file = pdf_files[0]\n",
        "    try:\n",
        "        docs_pdf = loader_service.load(pdf_file, clean=False)\n",
        "        formats_data[\"PDF\"] = {\"file\": pdf_file, \"docs\": docs_pdf}\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Error cargando PDF: {e}\")\n",
        "\n",
        "print(f\"‚úÖ Formatos cargados: {list(formats_data.keys())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 2: Comparar Metadatos Extra√≠dos\n",
        "\n",
        "Comparemos qu√© metadatos se extraen de cada formato.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Comparaci√≥n de Metadatos por Formato\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Markdown (110225.md):\n",
            "--------------------------------------------------------------------------------\n",
            "  Contenido: 6676 caracteres\n",
            "  Metadatos extra√≠dos:\n",
            "    - file_path: D:\\projects\\Ungraph\\src\\data\\110225.md\n",
            "    - source: D:\\projects\\Ungraph\\src\\data\\110225.md\n",
            "\n",
            "Texto (AnnyLetter.txt):\n",
            "--------------------------------------------------------------------------------\n",
            "  Contenido: 17975 caracteres\n",
            "  Metadatos extra√≠dos:\n",
            "    - file_path: D:\\projects\\Ungraph\\src\\data\\AnnyLetter.txt\n",
            "    - encoding: iso-8859-1\n",
            "    - source: D:\\projects\\Ungraph\\src\\data\\AnnyLetter.txt\n",
            "\n",
            "Word (Usar s√≠mboles de silencio de corchea.docx):\n",
            "--------------------------------------------------------------------------------\n",
            "  Contenido: 22030 caracteres\n",
            "  Metadatos extra√≠dos:\n",
            "    - file_path: D:\\projects\\Ungraph\\src\\data\\Usar s√≠mboles de silencio de corchea.docx\n",
            "    - source: D:\\projects\\Ungraph\\src\\data\\Usar s√≠mboles de silencio de corchea.docx\n",
            "\n",
            "PDF (peach-et-al-2014-qsar-modeling-of-imbalanced-high-throughput-screening-data-in-pubchem.pdf):\n",
            "--------------------------------------------------------------------------------\n",
            "  Contenido: 25 caracteres\n",
            "  Metadatos extra√≠dos:\n",
            "    - file_path: D:\\projects\\Ungraph\\src\\data\\peach-et-al-2014-qsar-modeling-of-imbalanced-high-throughput-screening-\n",
            "    - file_type: pdf\n",
            "    - source: D:\\projects\\Ungraph\\src\\data\\peach-et-al-2014-qsar-modeling-of-imbalanced-high-throughput-screening-\n",
            "    - dl_meta: dict\n"
          ]
        }
      ],
      "source": [
        "# Comparar metadatos por formato\n",
        "print(\"üìä Comparaci√≥n de Metadatos por Formato\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for format_name, data in formats_data.items():\n",
        "    print(f\"\\n{format_name} ({data['file'].name}):\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    if data['docs']:\n",
        "        doc = data['docs'][0]\n",
        "        print(f\"  Contenido: {len(doc.content)} caracteres\")\n",
        "        print(f\"  Metadatos extra√≠dos:\")\n",
        "        \n",
        "        for key, value in doc.metadata.items():\n",
        "            if isinstance(value, (str, int, float)):\n",
        "                display_value = str(value)[:100] if len(str(value)) > 100 else str(value)\n",
        "                print(f\"    - {key}: {display_value}\")\n",
        "            else:\n",
        "                print(f\"    - {key}: {type(value).__name__}\")\n",
        "    else:\n",
        "        print(\"  ‚ö†Ô∏è  No se pudieron cargar documentos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 3: Metadatos Espec√≠ficos por Formato\n",
        "\n",
        "Cada formato tiene metadatos √∫nicos que pueden ser √∫tiles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìã Metadatos Espec√≠ficos por Formato\n",
            "\n",
            "Markdown:\n",
            "  - file_type: N/A\n",
            "  - filename: N/A\n",
            "  - source: D:\\projects\\Ungraph\\src\\data\\110225.md\n",
            "\n",
            "PDF (metadatos avanzados de Docling):\n",
            "\n",
            "Word:\n",
            "  - file_type: N/A\n",
            "  - source: D:\\projects\\Ungraph\\src\\data\\Usar s√≠mboles de silencio de corchea.docx\n"
          ]
        }
      ],
      "source": [
        "# Analizar metadatos espec√≠ficos\n",
        "print(\"üìã Metadatos Espec√≠ficos por Formato\\n\")\n",
        "\n",
        "# Markdown - puede tener informaci√≥n de estructura\n",
        "if \"Markdown\" in formats_data and formats_data[\"Markdown\"][\"docs\"]:\n",
        "    md_doc = formats_data[\"Markdown\"][\"docs\"][0]\n",
        "    print(\"Markdown:\")\n",
        "    print(f\"  - file_type: {md_doc.metadata.get('file_type', 'N/A')}\")\n",
        "    print(f\"  - filename: {md_doc.metadata.get('filename', 'N/A')}\")\n",
        "    if 'source' in md_doc.metadata:\n",
        "        print(f\"  - source: {md_doc.metadata['source']}\")\n",
        "\n",
        "# PDF - puede tener page_number, document_structure, tables, images\n",
        "if \"PDF\" in formats_data and formats_data[\"PDF\"][\"docs\"]:\n",
        "    pdf_doc = formats_data[\"PDF\"][\"docs\"][0]\n",
        "    print(\"\\nPDF (metadatos avanzados de Docling):\")\n",
        "    for key in ['page_number', 'document_structure', 'tables', 'images']:\n",
        "        if key in pdf_doc.metadata:\n",
        "            print(f\"  - {key}: {pdf_doc.metadata[key]}\")\n",
        "\n",
        "# Word - puede tener informaci√≥n de formato\n",
        "if \"Word\" in formats_data and formats_data[\"Word\"][\"docs\"]:\n",
        "    word_doc = formats_data[\"Word\"][\"docs\"][0]\n",
        "    print(\"\\nWord:\")\n",
        "    print(f\"  - file_type: {word_doc.metadata.get('file_type', 'N/A')}\")\n",
        "    if 'source' in word_doc.metadata:\n",
        "        print(f\"  - source: {word_doc.metadata['source']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 4: Manejo de Encoding en Archivos de Texto\n",
        "\n",
        "Los archivos de texto pueden tener diferentes encodings. Ungraph los detecta autom√°ticamente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÑ Archivo: AnnyLetter.txt\n",
            "   Encoding detectado: ISO-8859-1\n",
            "   Confianza: 73.00%\n",
            "   Tama√±o: 18219 bytes\n",
            "\n",
            "   Pruebas de lectura:\n",
            "     ‚ùå utf-8: Error\n",
            "     ‚úÖ windows-1252: 18219 caracteres\n",
            "     ‚úÖ latin-1: 18219 caracteres\n"
          ]
        }
      ],
      "source": [
        "# Verificar encoding de archivos de texto\n",
        "import chardet\n",
        "\n",
        "txt_file = data_path / \"AnnyLetter.txt\"\n",
        "if txt_file.exists():\n",
        "    with open(txt_file, 'rb') as f:\n",
        "        raw_data = f.read()\n",
        "        result = chardet.detect(raw_data)\n",
        "        print(f\"üìÑ Archivo: {txt_file.name}\")\n",
        "        print(f\"   Encoding detectado: {result['encoding']}\")\n",
        "        print(f\"   Confianza: {result['confidence']:.2%}\")\n",
        "        print(f\"   Tama√±o: {len(raw_data)} bytes\")\n",
        "        \n",
        "        # Intentar leer con diferentes encodings\n",
        "        print(f\"\\n   Pruebas de lectura:\")\n",
        "        for encoding in ['utf-8', 'windows-1252', 'latin-1']:\n",
        "            try:\n",
        "                content = raw_data.decode(encoding)\n",
        "                print(f\"     ‚úÖ {encoding}: {len(content)} caracteres\")\n",
        "            except:\n",
        "                print(f\"     ‚ùå {encoding}: Error\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resumen y Comparaci√≥n\n",
        "\n",
        "### Comparaci√≥n de Formatos\n",
        "\n",
        "| Formato | Ventajas | Limitaciones | Mejor Para |\n",
        "|---------|----------|--------------|------------|\n",
        "| **Markdown** | Estructura preservada, f√°cil de procesar | Requiere formato espec√≠fico | Documentaci√≥n, art√≠culos estructurados |\n",
        "| **Texto** | Universal, simple | Sin estructura | Texto plano, logs |\n",
        "| **Word** | Formato com√∫n, estructura preservada | Puede tener formato complejo | Documentos de oficina |\n",
        "| **PDF** | Formato est√°ndar, estructura avanzada | Requiere librer√≠as especiales | Documentos acad√©micos, informes |\n",
        "\n",
        "### Metadatos Comunes\n",
        "\n",
        "Todos los formatos incluyen:\n",
        "- `filename`: Nombre del archivo\n",
        "- `file_type`: Tipo de archivo (md, txt, docx, pdf)\n",
        "- `file_path`: Ruta completa al archivo\n",
        "\n",
        "### Metadatos Espec√≠ficos\n",
        "\n",
        "- **PDF**: `page_number`, `document_structure`, `tables`, `images` (si Docling est√° disponible)\n",
        "- **Markdown**: Informaci√≥n de estructura de headers\n",
        "- **Word**: Informaci√≥n de formato y estilo\n",
        "\n",
        "### Mejores Pr√°cticas\n",
        "\n",
        "1. **Elegir formato seg√∫n necesidad**: PDF para documentos complejos, Markdown para estructura simple\n",
        "2. **Verificar metadatos**: Revisa qu√© metadatos se extraen para tu caso de uso\n",
        "3. **Encoding**: Los archivos de texto se detectan autom√°ticamente, pero verifica si hay problemas\n",
        "4. **PDF avanzado**: Usa Docling para mejor extracci√≥n de estructura y tablas\n",
        "\n",
        "## Referencias\n",
        "\n",
        "- [Gu√≠a de Ingesta](../../docs/guides/ingestion.md)\n",
        "- [Document Loader Service](../../src/infrastructure/services/langchain_document_loader_service.py)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
