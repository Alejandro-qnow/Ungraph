{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using Ungraph Library\n",
        "\n",
        "Este notebook muestra c√≥mo usar la librer√≠a Ungraph para convertir datos no estructurados en grafos de conocimiento.\n",
        "\n",
        "## Pipeline Completo:\n",
        "1. Configurar conexi√≥n a Neo4j\n",
        "2. Obtener recomendaci√≥n de chunking\n",
        "3. Ingerir documentos al grafo\n",
        "4. Buscar informaci√≥n en el grafo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path Folder parent added: D:\\projects\\Ungraph\n",
            "Path Folder src added: D:\\projects\\Ungraph\\src\n",
            "Path Folder src/utils added: D:\\projects\\Ungraph\\src\\utils\n",
            "Path Folder src/data added: D:\\projects\\Ungraph\\src\\data\n"
          ]
        }
      ],
      "source": [
        "def add_src_to_path(path_folder: str):\n",
        "    ''' \n",
        "    Helper function for adding the \"path_folder\" directory to the path.\n",
        "    in order to work on notebooks and scripts\n",
        "    '''\n",
        "    import sys\n",
        "    from pathlib import Path\n",
        "\n",
        "    base_path = Path().resolve()\n",
        "    for parent in [base_path] + list(base_path.parents):\n",
        "        candidate = parent / path_folder\n",
        "        if candidate.exists():\n",
        "            # Agregar el directorio padre para que sea un paquete Python\n",
        "            parent_dir = candidate.parent\n",
        "            if str(parent_dir) not in sys.path:\n",
        "                sys.path.insert(0, str(parent_dir))\n",
        "                print(f\"Path Folder parent added: {parent_dir}\")\n",
        "            if str(candidate) not in sys.path:\n",
        "                sys.path.append(str(candidate))\n",
        "                print(f\"Path Folder {path_folder} added: {candidate}\")\n",
        "            return\n",
        "    print(f\"Not found '{path_folder}' folder on the hierarchy of directories\")\n",
        "\n",
        "# Agregar carpetas necesarias al path\n",
        "# Esto permite importar src como un paquete Python\n",
        "add_src_to_path(path_folder=\"src\")\n",
        "add_src_to_path(path_folder=\"src/utils\")\n",
        "add_src_to_path(path_folder=\"src/data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Ungraph importado desde src/ (modo desarrollo)\n",
            "üì¶ Ungraph version: 0.1.0\n",
            "üîß Funciones disponibles: ['Any', 'Chunk', 'ChunkingMaster', 'ChunkingRecommendation', 'ChunkingResult', 'ChunkingStrategy', 'Dict', 'HuggingFaceEmbeddingService', 'IngestDocumentUseCase', 'LangChainDocument']\n"
          ]
        }
      ],
      "source": [
        "# Importar la librer√≠a Ungraph y utilidades\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Importar handlers desde utils (ya est√° en el path)\n",
        "from src.utils.handlers import find_in_project\n",
        "\n",
        "# Importar ungraph\n",
        "# Intentar importar como paquete instalado primero\n",
        "try:\n",
        "    import ungraph\n",
        "    print(\"‚úÖ Ungraph importado como paquete instalado\")\n",
        "except ImportError:\n",
        "    # Si no est√° instalado, importar desde src\n",
        "    # src/__init__.py contiene toda la API p√∫blica\n",
        "    import src\n",
        "    ungraph = src\n",
        "    print(\"‚úÖ Ungraph importado desde src/ (modo desarrollo)\")\n",
        "\n",
        "print(f\"üì¶ Ungraph version: {ungraph.__version__}\")\n",
        "print(f\"üîß Funciones disponibles: {[f for f in dir(ungraph) if not f.startswith('_')][:10]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuraci√≥n\n",
        "\n",
        "Primero configuramos la conexi√≥n a Neo4j. Puedes usar variables de entorno o configuraci√≥n program√°tica.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Configuraci√≥n completada\n",
            "üí° Si obtienes AuthError, verifica que:\n",
            "   1. Neo4j est√© corriendo\n",
            "   2. Las credenciales sean correctas\n",
            "   3. La URI sea accesible\n"
          ]
        }
      ],
      "source": [
        "# ‚ö†Ô∏è IMPORTANTE: Configura tus credenciales de Neo4j antes de continuar\n",
        "\n",
        "# Opci√≥n 1: Usar variables de entorno (recomendado)\n",
        "# En terminal:\n",
        "# export NEO4J_URI=\"bolt://localhost:7687\"\n",
        "# export NEO4J_PASSWORD=\"tu_contrase√±a_real\"\n",
        "\n",
        "# Opci√≥n 2: Configurar program√°ticamente\n",
        "# ‚ö†Ô∏è REEMPLAZA \"tu_contrase√±a\" con tu contrase√±a real de Neo4j\n",
        "ungraph.configure(\n",
        "    neo4j_uri=\"bolt://localhost:7687\",\n",
        "    neo4j_user=\"neo4j\",\n",
        "    neo4j_password=\"Ungraph22\",  # ‚ö†Ô∏è CAMBIAR: Usa tu contrase√±a real de Neo4j\n",
        "    neo4j_database=\"neo4j\",\n",
        "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Configuraci√≥n completada\")\n",
        "print(\"üí° Si obtienes AuthError, verifica que:\")\n",
        "print(\"   1. Neo4j est√© corriendo\")\n",
        "print(\"   2. Las credenciales sean correctas\")\n",
        "print(\"   3. La URI sea accesible\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Encontrar Archivos de Datos\n",
        "\n",
        "Localizamos los archivos de prueba en la carpeta `data`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÅ Carpeta de datos encontrada: D:\\projects\\Ungraph\\src\\data\n",
            "\n",
            "üìÑ Archivos disponibles:\n",
            "  - 110225.md (6.6 KB)\n",
            "  - AnnyLetter.txt (17.8 KB)\n",
            "  - Usar s√≠mboles de silencio de corchea.docx (25.2 KB)\n"
          ]
        }
      ],
      "source": [
        "# Encontrar la carpeta data de manera agn√≥stica\n",
        "data_path = find_in_project(\n",
        "    target=\"data\",\n",
        "    search_type=\"folder\",\n",
        "    project_root=None\n",
        ")\n",
        "\n",
        "print(f\"üìÅ Carpeta de datos encontrada: {data_path}\")\n",
        "\n",
        "# Listar archivos disponibles\n",
        "files = list(data_path.glob(\"*\"))\n",
        "print(f\"\\nüìÑ Archivos disponibles:\")\n",
        "for file in files:\n",
        "    if file.is_file():\n",
        "        print(f\"  - {file.name} ({file.stat().st_size / 1024:.1f} KB)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Obtener Recomendaci√≥n de Chunking\n",
        "\n",
        "Antes de ingerir, podemos obtener una recomendaci√≥n sobre la mejor estrategia de chunking para cada documento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Analizando: 110225.md\n",
            "\n",
            "‚úÖ Recomendaci√≥n obtenida:\n",
            "   Estrategia: recursive\n",
            "   Chunk size: 2000\n",
            "   Chunk overlap: 300\n",
            "   Quality score: 70.80\n",
            "\n",
            "üìù Explicaci√≥n:\n",
            "Se recomienda la estrategia 'recursive' porque:\n",
            "- El documento es Markdown con estructura de headers\n",
            "- El documento tiene 29 p√°rrafos\n",
            "- Generar√° aproximadamente 4 chunks\n",
            "- Tama√±o promedio de chunk: 1792 caracteres\n",
            "- Score de calidad: 70.80/1.0\n"
          ]
        }
      ],
      "source": [
        "# Obtener recomendaci√≥n para un archivo Markdown\n",
        "markdown_file = data_path / \"110225.md\"\n",
        "\n",
        "if markdown_file.exists():\n",
        "    print(f\"üìä Analizando: {markdown_file.name}\")\n",
        "    recommendation = ungraph.suggest_chunking_strategy(markdown_file)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Recomendaci√≥n obtenida:\")\n",
        "    print(f\"   Estrategia: {recommendation.strategy}\")\n",
        "    print(f\"   Chunk size: {recommendation.chunk_size}\")\n",
        "    print(f\"   Chunk overlap: {recommendation.chunk_overlap}\")\n",
        "    print(f\"   Quality score: {recommendation.quality_score:.2f}\")\n",
        "    print(f\"\\nüìù Explicaci√≥n:\")\n",
        "    print(recommendation.explanation)\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Archivo no encontrado: {markdown_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Ingerir Documentos al Grafo\n",
        "\n",
        "Ahora ingerimos los documentos usando la recomendaci√≥n o par√°metros personalizados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Ingiriendo: 110225.md\n",
            "Chunk relationships created successfully\n",
            "‚úÖ Documento ingerido exitosamente!\n",
            "   Total de chunks: 9\n",
            "   Chunks con embeddings: 9\n",
            "\n",
            "üìÑ Primeros 3 chunks:\n",
            "\n",
            "   Chunk 1:\n",
            "   - ID: 110225.md_e6fd4c94-6635-4704-b161-c9dbdae794d4...\n",
            "   - Contenido: Incluso es lo primero que se abrio, quiza lo que me falta en este momento es la palabra, y necesite ...\n",
            "   - Consecutivo: 1\n",
            "   - Embeddings: 384 dimensiones\n",
            "\n",
            "   Chunk 2:\n",
            "   - ID: 110225.md_5ff5f5a6-4331-4084-bd21-cb111f92a40c...\n",
            "   - Contenido: quiero narrar como estoy desarrollando la vision de mi mismo, la que quiero para mi, el humano que q...\n",
            "   - Consecutivo: 2\n",
            "   - Embeddings: 384 dimensiones\n",
            "\n",
            "   Chunk 3:\n",
            "   - ID: 110225.md_9bf3da65-3618-48fd-94c6-369b7223ba08...\n",
            "   - Contenido: cobran sentido en la contradiccion de su naturaleza. Quiza lo que me falta es el ejercicio diario de...\n",
            "   - Consecutivo: 3\n",
            "   - Embeddings: 384 dimensiones\n"
          ]
        }
      ],
      "source": [
        "# Ingerir archivo Markdown\n",
        "markdown_file = data_path / \"110225.md\"\n",
        "\n",
        "if markdown_file.exists():\n",
        "    print(f\"üì• Ingiriendo: {markdown_file.name}\")\n",
        "    chunks = ungraph.ingest_document(\n",
        "        markdown_file,\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        clean_text=True\n",
        "    )\n",
        "    print(f\"‚úÖ Documento ingerido exitosamente!\")\n",
        "    print(f\"   Total de chunks: {len(chunks)}\")\n",
        "    print(f\"   Chunks con embeddings: {sum(1 for c in chunks if c.embeddings)}\")\n",
        "    \n",
        "    # Mostrar informaci√≥n de algunos chunks\n",
        "    print(f\"\\nüìÑ Primeros 3 chunks:\")\n",
        "    for i, chunk in enumerate(chunks[:3], 1):\n",
        "        print(f\"\\n   Chunk {i}:\")\n",
        "        print(f\"   - ID: {chunk.id[:50]}...\")\n",
        "        print(f\"   - Contenido: {chunk.page_content[:100]}...\")\n",
        "        print(f\"   - Consecutivo: {chunk.chunk_id_consecutive}\")\n",
        "        if chunk.embeddings:\n",
        "            print(f\"   - Embeddings: {len(chunk.embeddings)} dimensiones\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Archivo no encontrado: {markdown_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Ingiriendo: AnnyLetter.txt\n",
            "Chunk relationships created successfully\n",
            "‚úÖ Documento ingerido exitosamente!\n",
            "   Total de chunks: 45\n"
          ]
        }
      ],
      "source": [
        "# Ingerir archivo de texto\n",
        "txt_file = data_path / \"AnnyLetter.txt\"\n",
        "\n",
        "if txt_file.exists():\n",
        "    print(f\"üì• Ingiriendo: {txt_file.name}\")\n",
        "    chunks_txt = ungraph.ingest_document(\n",
        "        txt_file,\n",
        "        chunk_size=500,\n",
        "        chunk_overlap=100\n",
        "    )\n",
        "    print(f\"‚úÖ Documento ingerido exitosamente!\")\n",
        "    print(f\"   Total de chunks: {len(chunks_txt)}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Archivo no encontrado: {txt_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Ingiriendo: Usar s√≠mboles de silencio de corchea.docx\n",
            "Chunk relationships created successfully\n",
            "‚úÖ Documento ingerido exitosamente!\n",
            "   Total de chunks: 27\n"
          ]
        }
      ],
      "source": [
        "# Ingerir archivo Word\n",
        "docx_file = data_path / \"Usar s√≠mboles de silencio de corchea.docx\"\n",
        "\n",
        "if docx_file.exists():\n",
        "    print(f\"üì• Ingiriendo: {docx_file.name}\")\n",
        "    chunks_docx = ungraph.ingest_document(\n",
        "        docx_file,\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200\n",
        "    )\n",
        "    print(f\"‚úÖ Documento ingerido exitosamente!\")\n",
        "    print(f\"   Total de chunks: {len(chunks_docx)}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Archivo no encontrado: {docx_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Buscar en el Grafo\n",
        "\n",
        "Ahora podemos buscar informaci√≥n en el grafo usando diferentes m√©todos de b√∫squeda.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Buscando: 'computaci√≥n cu√°ntica'\n",
            "\n",
            "‚úÖ Encontrados 4 resultados:\n",
            "\n",
            "Resultado 1:\n",
            "  Score: 3.1815\n",
            "  Chunk ID: 110225.md_2386e056-324b-470a-97f6-0b5a18a31d34\n",
            "  Contenido: quiero narrar como estoy desarrollando la vision de mi mismo, la que quiero para mi, el humano que quiero moldear en este presente, y suena ambicioso, pero no me he rendido con la busqueda, no he tira...\n",
            "\n",
            "Resultado 2:\n",
            "  Score: 3.1815\n",
            "  Chunk ID: 110225.md_5ff5f5a6-4331-4084-bd21-cb111f92a40c\n",
            "  Contenido: quiero narrar como estoy desarrollando la vision de mi mismo, la que quiero para mi, el humano que quiero moldear en este presente, y suena ambicioso, pero no me he rendido con la busqueda, no he tira...\n",
            "\n",
            "Resultado 3:\n",
            "  Score: 3.1815\n",
            "  Chunk ID: 110225.md_9748a95f-b378-4b91-85de-6cdd11c55ad1\n",
            "  Contenido: quiero narrar como estoy desarrollando la vision de mi mismo, la que quiero para mi, el humano que quiero moldear en este presente, y suena ambicioso, pero no me he rendido con la busqueda, no he tira...\n",
            "\n",
            "Resultado 4:\n",
            "  Score: 3.1815\n",
            "  Chunk ID: 110225.md_e95b8428-b8a9-453c-a497-65f670e64f74\n",
            "  Contenido: quiero narrar como estoy desarrollando la vision de mi mismo, la que quiero para mi, el humano que quiero moldear en este presente, y suena ambicioso, pero no me he rendido con la busqueda, no he tira...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# B√∫squeda simple por texto\n",
        "query = \"computaci√≥n cu√°ntica\"\n",
        "print(f\"üîç Buscando: '{query}'\")\n",
        "\n",
        "results = ungraph.search(query, limit=5)\n",
        "\n",
        "if results:\n",
        "    print(f\"\\n‚úÖ Encontrados {len(results)} resultados:\\n\")\n",
        "    for i, result in enumerate(results, 1):\n",
        "        print(f\"Resultado {i}:\")\n",
        "        print(f\"  Score: {result.score:.4f}\")\n",
        "        print(f\"  Chunk ID: {result.chunk_id}\")\n",
        "        print(f\"  Contenido: {result.content[:200]}...\")\n",
        "        print()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No se encontraron resultados\")\n",
        "    print(\"üí° Aseg√∫rate de haber ingerido documentos primero\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç B√∫squeda h√≠brida: 'inteligencia artificial'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (text_node) { ... }} {position: line: 7, column: 9, offset: 215} for query: '\\n        // B√∫squeda fulltext\\n        CALL db.index.fulltext.queryNodes(\"chunk_content\", $query_text)\\n        YIELD node as text_node, score as text_score\\n        \\n        // Combinar con b√∫squeda vectorial\\n        CALL {\\n            WITH text_node\\n            CALL db.index.vector.queryNodes(\\'chunk_embeddings\\', toInteger($top_k), $query_vector)\\n            YIELD node as vec_node, score as vec_score\\n            WHERE text_node = vec_node\\n            RETURN vec_node, vec_score\\n        }\\n        \\n        // Calcular score combinado\\n        WITH text_node as node, text_score, vec_score,\\n             (text_score * $text_weight + vec_score * $vector_weight) as combined_score\\n        \\n        // Obtener contexto\\n        OPTIONAL MATCH (node)<-[:NEXT_CHUNK]-(prev)\\n        OPTIONAL MATCH (node)-[:NEXT_CHUNK]->(next)\\n        \\n        RETURN {\\n            score: combined_score,\\n            central_node_content: node.page_content,\\n            central_node_chunk_id: node.chunk_id,\\n            central_node_chunk_id_consecutive: node.chunk_id_consecutive,\\n            previous_chunk_content: prev.page_content,\\n            next_chunk_content: next.page_content\\n        } as result\\n        ORDER BY combined_score DESC\\n        LIMIT $top_k\\n        '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Encontrados 3 resultados:\n",
            "\n",
            "================================================================================\n",
            "Resultado 1:\n",
            "  Score combinado: 1.2027\n",
            "  Chunk ID: 110225.md_2386e056-324b-470a-97f6-0b5a18a31d34\n",
            "\n",
            "  [Contexto Anterior]\n",
            "  Incluso es lo primero que se abrio, quiza lo que me falta en este momento es la palabra, y necesite encontrarme en ella para retomar mi camino, hoy mi...\n",
            "\n",
            "  [Contenido Principal]\n",
            "  quiero narrar como estoy desarrollando la vision de mi mismo, la que quiero para mi, el humano que quiero moldear en este presente, y suena ambicioso, pero no me he rendido con la busqueda, no he tirado la toalla a falta de mejores opciones, no me resigno. Si tuviera algo hecho en 5 anos, seria nues...\n",
            "\n",
            "  [Contexto Siguiente]\n",
            "  cobran sentido en la contradiccion de su naturaleza. Quiza lo que me falta es el ejercicio diario de escribir, de narrarme el hombre que deje de ser, ...\n",
            "\n",
            "================================================================================\n",
            "Resultado 2:\n",
            "  Score combinado: 1.2027\n",
            "  Chunk ID: 110225.md_2386e056-324b-470a-97f6-0b5a18a31d34\n",
            "\n",
            "  [Contexto Anterior]\n",
            "  Incluso es lo primero que se abrio, quiza lo que me falta en este momento es la palabra, y necesite encontrarme en ella para retomar mi camino, hoy mi...\n",
            "\n",
            "  [Contenido Principal]\n",
            "  quiero narrar como estoy desarrollando la vision de mi mismo, la que quiero para mi, el humano que quiero moldear en este presente, y suena ambicioso, pero no me he rendido con la busqueda, no he tirado la toalla a falta de mejores opciones, no me resigno. Si tuviera algo hecho en 5 anos, seria nues...\n",
            "\n",
            "  [Contexto Siguiente]\n",
            "  cobran sentido en la contradiccion de su naturaleza. Quiza lo que me falta es el ejercicio diario de escribir, de narrarme el hombre que deje de ser, ...\n",
            "\n",
            "================================================================================\n",
            "Resultado 3:\n",
            "  Score combinado: 1.2027\n",
            "  Chunk ID: 110225.md_2386e056-324b-470a-97f6-0b5a18a31d34\n",
            "\n",
            "  [Contexto Anterior]\n",
            "  Incluso es lo primero que se abrio, quiza lo que me falta en este momento es la palabra, y necesite encontrarme en ella para retomar mi camino, hoy mi...\n",
            "\n",
            "  [Contenido Principal]\n",
            "  quiero narrar como estoy desarrollando la vision de mi mismo, la que quiero para mi, el humano que quiero moldear en este presente, y suena ambicioso, pero no me he rendido con la busqueda, no he tirado la toalla a falta de mejores opciones, no me resigno. Si tuviera algo hecho en 5 anos, seria nues...\n",
            "\n",
            "  [Contexto Siguiente]\n",
            "  cobran sentido en la contradiccion de su naturaleza. Quiza lo que me falta es el ejercicio diario de escribir, de narrarme el hombre que deje de ser, ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# B√∫squeda h√≠brida (texto + vectorial)\n",
        "query = \"inteligencia artificial\"\n",
        "print(f\"üîç B√∫squeda h√≠brida: '{query}'\")\n",
        "\n",
        "results_hybrid = ungraph.hybrid_search(\n",
        "    query,\n",
        "    limit=3,\n",
        "    weights=(0.3, 0.7)  # 30% texto, 70% vectorial\n",
        ")\n",
        "\n",
        "if results_hybrid:\n",
        "    print(f\"\\n‚úÖ Encontrados {len(results_hybrid)} resultados:\\n\")\n",
        "    for i, result in enumerate(results_hybrid, 1):\n",
        "        print(f\"{'=' * 80}\")\n",
        "        print(f\"Resultado {i}:\")\n",
        "        print(f\"  Score combinado: {result.score:.4f}\")\n",
        "        print(f\"  Chunk ID: {result.chunk_id}\")\n",
        "        \n",
        "        if result.previous_chunk_content:\n",
        "            print(f\"\\n  [Contexto Anterior]\")\n",
        "            print(f\"  {result.previous_chunk_content[:150]}...\")\n",
        "        \n",
        "        print(f\"\\n  [Contenido Principal]\")\n",
        "        print(f\"  {result.content[:300]}...\")\n",
        "        \n",
        "        if result.next_chunk_content:\n",
        "            print(f\"\\n  [Contexto Siguiente]\")\n",
        "            print(f\"  {result.next_chunk_content[:150]}...\")\n",
        "        \n",
        "        print()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No se encontraron resultados\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Pipeline Completo con M√∫ltiples Archivos\n",
        "\n",
        "Ejemplo de c√≥mo procesar m√∫ltiples archivos en un pipeline completo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Iniciando pipeline completo...\n",
            "\n",
            "üìÑ Procesando: 110225.md\n",
            "   üí° Estrategia recomendada: recursive\n",
            "Chunk relationships created successfully\n",
            "   ‚úÖ 4 chunks creados\n",
            "\n",
            "üìÑ Procesando: AnnyLetter.txt\n",
            "   üí° Estrategia recomendada: character\n",
            "Chunk relationships created successfully\n",
            "   ‚úÖ 11 chunks creados\n",
            "\n",
            "üìÑ Procesando: Usar s√≠mboles de silencio de corchea.docx\n",
            "   üí° Estrategia recomendada: recursive\n",
            "Chunk relationships created successfully\n",
            "   ‚úÖ 13 chunks creados\n",
            "\n",
            "üéâ Pipeline completado!\n",
            "   Total de chunks procesados: 28\n"
          ]
        }
      ],
      "source": [
        "# Pipeline completo para m√∫ltiples archivos\n",
        "files_to_process = [\n",
        "    data_path / \"110225.md\",\n",
        "    data_path / \"AnnyLetter.txt\",\n",
        "    data_path / \"Usar s√≠mboles de silencio de corchea.docx\"\n",
        "]\n",
        "\n",
        "print(\"üöÄ Iniciando pipeline completo...\\n\")\n",
        "\n",
        "total_chunks = 0\n",
        "for file_path in files_to_process:\n",
        "    if file_path.exists():\n",
        "        print(f\"üìÑ Procesando: {file_path.name}\")\n",
        "        \n",
        "        # 1. Obtener recomendaci√≥n\n",
        "        try:\n",
        "            recommendation = ungraph.suggest_chunking_strategy(file_path)\n",
        "            print(f\"   üí° Estrategia recomendada: {recommendation.strategy}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è  No se pudo obtener recomendaci√≥n: {e}\")\n",
        "            recommendation = None\n",
        "        \n",
        "        # 2. Ingerir documento\n",
        "        try:\n",
        "            if recommendation:\n",
        "                chunks = ungraph.ingest_document(\n",
        "                    file_path,\n",
        "                    chunk_size=recommendation.chunk_size,\n",
        "                    chunk_overlap=recommendation.chunk_overlap\n",
        "                )\n",
        "            else:\n",
        "                chunks = ungraph.ingest_document(file_path)\n",
        "            \n",
        "            total_chunks += len(chunks)\n",
        "            print(f\"   ‚úÖ {len(chunks)} chunks creados\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error al ingerir: {e}\\n\")\n",
        "\n",
        "print(f\"üéâ Pipeline completado!\")\n",
        "print(f\"   Total de chunks procesados: {total_chunks}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Ejemplo de B√∫squeda con Contexto Completo\n",
        "\n",
        "Mostrar c√≥mo construir contexto completo usando los resultados de b√∫squeda.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Buscando: 'Alejandro'\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (text_node) { ... }} {position: line: 7, column: 9, offset: 215} for query: '\\n        // B√∫squeda fulltext\\n        CALL db.index.fulltext.queryNodes(\"chunk_content\", $query_text)\\n        YIELD node as text_node, score as text_score\\n        \\n        // Combinar con b√∫squeda vectorial\\n        CALL {\\n            WITH text_node\\n            CALL db.index.vector.queryNodes(\\'chunk_embeddings\\', toInteger($top_k), $query_vector)\\n            YIELD node as vec_node, score as vec_score\\n            WHERE text_node = vec_node\\n            RETURN vec_node, vec_score\\n        }\\n        \\n        // Calcular score combinado\\n        WITH text_node as node, text_score, vec_score,\\n             (text_score * $text_weight + vec_score * $vector_weight) as combined_score\\n        \\n        // Obtener contexto\\n        OPTIONAL MATCH (node)<-[:NEXT_CHUNK]-(prev)\\n        OPTIONAL MATCH (node)-[:NEXT_CHUNK]->(next)\\n        \\n        RETURN {\\n            score: combined_score,\\n            central_node_content: node.page_content,\\n            central_node_chunk_id: node.chunk_id,\\n            central_node_chunk_id_consecutive: node.chunk_id_consecutive,\\n            previous_chunk_content: prev.page_content,\\n            next_chunk_content: next.page_content\\n        } as result\\n        ORDER BY combined_score DESC\\n        LIMIT $top_k\\n        '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Resultado 1 (Score: 0.8439)\n",
            "================================================================================\n",
            "[CONTEXTO ANTERIOR]\n",
            "y testeo de aplicaciones desarrolladas fullstack con AI para ser mas competente a nivel profesional. Ser mejor en quantum computing para poder sembrar buenas bases y raices de lo que quiero construir para mi futuro profesional. Aprender sobre Quimica Computacional para desempenarme realmente con lo qu estamos construyendo. Aprovechar sobre la complejidad, los grafos de conocimiento y la ingeniera de datos con Neo4j y analitica de grafos para poder mejorar mis skills como ingeniero de la complejidad. Dado que requiero desempeno mental muy alto y nitido necesito tener claro que las acciones para lograr esto derivan en: Mente no intoxicada, Bien alimentada, con un cuerpo que se mueve y le genera lso estimulos suficientes para crear coenxiones, descanso y reposo necesarios, sesiones de trabajo enfocado, de descanso real, remover el ocio de series, porno , fantasias del conducto de agua caliente, ([[Teoria del Conducto del Agua Caliente]]), si pudiera visualizar el siguiente Alejandro\n",
            "\n",
            "[CONTENIDO PRINCIPAL]\n",
            "enfocado, de descanso real, remover el ocio de series, porno , fantasias del conducto de agua caliente, ([[Teoria del Conducto del Agua Caliente]]), si pudiera visualizar el siguiente Alejandro todos los dias lo construiiria pedazo a pedazso. Rutina de Alejandro Giraldo Londono Version n-esima potencia: Me levanto, hago ejercicio, tomo agua, gestiono el almuerzo, comida y el desayuno, estudio, me ducho con agua fria, me visto para trabajar en una pieza limpia, reviso el plan que hice anoche/semana para trabajar en todos mis compromisos hoy, priorizo urgente/importante, dedico tiempo a los compromisos conmigo mismo (citas, vueltas), cierro el dia caminando, escribiendo, tocando guitarra en dias lluviosos, haciendo ejercicio en el ginmansio o las barras, o jugando fuchi, o yendo al taller de literatura y venideros, pasando tiempo con mi hijo, conociendo nuevas personas, viendo los atardeceres desde lo alto de algun sitio. Que seria de esa persona? Puedo visualizarlo, es una vida sonada,\n",
            "\n",
            "[CONTEXTO SIGUIENTE]\n",
            "literatura y venideros, pasando tiempo con mi hijo, conociendo nuevas personas, viendo los atardeceres desde lo alto de algun sitio. Que seria de esa persona? Puedo visualizarlo, es una vida sonada, es plena, requiere de una disciplina ferrea, de ajustar algunas cosas que hoy me distancian y me alejan de semejante futuro el que un conjunto acciones diarias asi podrian crear en mi. Para lograr esto mis metas deberian ser atomicas. Es decir, en este momento deberia enfocarme en lo que emerge de la necesidad, es decir, aprendizaje adaptativo y reactivo. Es decir, si emerge algo urgente que necesito reaccionar pronto, me debo adaptar al cambio e incluirlo masivamente. \"Ejemplo, deberia estar practicando todo lo que se en un mutante. Hoy quiero lograr, crear un MCP, FastAPI aplicacion, que permita conectarse a Neo4j local para que cualquier IDLE pueda trabajar con el grafo local, mi objetivo de 2 horas es crear una API con su debido composer, para levantar servicios de Neo4j, cypher shell\n",
            "\n",
            "================================================================================\n",
            "Resultado 2 (Score: 0.8439)\n",
            "================================================================================\n",
            "[CONTEXTO ANTERIOR]\n",
            "y testeo de aplicaciones desarrolladas fullstack con AI para ser mas competente a nivel profesional. Ser mejor en quantum computing para poder sembrar buenas bases y raices de lo que quiero construir para mi futuro profesional. Aprender sobre Quimica Computacional para desempenarme realmente con lo qu estamos construyendo. Aprovechar sobre la complejidad, los grafos de conocimiento y la ingeniera de datos con Neo4j y analitica de grafos para poder mejorar mis skills como ingeniero de la complejidad. Dado que requiero desempeno mental muy alto y nitido necesito tener claro que las acciones para lograr esto derivan en: Mente no intoxicada, Bien alimentada, con un cuerpo que se mueve y le genera lso estimulos suficientes para crear coenxiones, descanso y reposo necesarios, sesiones de trabajo enfocado, de descanso real, remover el ocio de series, porno , fantasias del conducto de agua caliente, ([[Teoria del Conducto del Agua Caliente]]), si pudiera visualizar el siguiente Alejandro\n",
            "\n",
            "[CONTENIDO PRINCIPAL]\n",
            "enfocado, de descanso real, remover el ocio de series, porno , fantasias del conducto de agua caliente, ([[Teoria del Conducto del Agua Caliente]]), si pudiera visualizar el siguiente Alejandro todos los dias lo construiiria pedazo a pedazso. Rutina de Alejandro Giraldo Londono Version n-esima potencia: Me levanto, hago ejercicio, tomo agua, gestiono el almuerzo, comida y el desayuno, estudio, me ducho con agua fria, me visto para trabajar en una pieza limpia, reviso el plan que hice anoche/semana para trabajar en todos mis compromisos hoy, priorizo urgente/importante, dedico tiempo a los compromisos conmigo mismo (citas, vueltas), cierro el dia caminando, escribiendo, tocando guitarra en dias lluviosos, haciendo ejercicio en el ginmansio o las barras, o jugando fuchi, o yendo al taller de literatura y venideros, pasando tiempo con mi hijo, conociendo nuevas personas, viendo los atardeceres desde lo alto de algun sitio. Que seria de esa persona? Puedo visualizarlo, es una vida sonada,\n",
            "\n",
            "[CONTEXTO SIGUIENTE]\n",
            "literatura y venideros, pasando tiempo con mi hijo, conociendo nuevas personas, viendo los atardeceres desde lo alto de algun sitio. Que seria de esa persona? Puedo visualizarlo, es una vida sonada, es plena, requiere de una disciplina ferrea, de ajustar algunas cosas que hoy me distancian y me alejan de semejante futuro el que un conjunto acciones diarias asi podrian crear en mi. Para lograr esto mis metas deberian ser atomicas. Es decir, en este momento deberia enfocarme en lo que emerge de la necesidad, es decir, aprendizaje adaptativo y reactivo. Es decir, si emerge algo urgente que necesito reaccionar pronto, me debo adaptar al cambio e incluirlo masivamente. \"Ejemplo, deberia estar practicando todo lo que se en un mutante. Hoy quiero lograr, crear un MCP, FastAPI aplicacion, que permita conectarse a Neo4j local para que cualquier IDLE pueda trabajar con el grafo local, mi objetivo de 2 horas es crear una API con su debido composer, para levantar servicios de Neo4j, cypher shell\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# B√∫squeda con construcci√≥n de contexto completo\n",
        "query = \"Alejandro\"\n",
        "print(f\"üîç Buscando: '{query}'\\n\")\n",
        "\n",
        "results = ungraph.hybrid_search(query, limit=2)\n",
        "\n",
        "for i, result in enumerate(results, 1):\n",
        "    # Construir contexto completo\n",
        "    contexto_completo = \"\"\n",
        "    \n",
        "    if result.previous_chunk_content:\n",
        "        contexto_completo += f\"[CONTEXTO ANTERIOR]\\n{result.previous_chunk_content}\\n\\n\"\n",
        "    \n",
        "    contexto_completo += f\"[CONTENIDO PRINCIPAL]\\n{result.content}\\n\\n\"\n",
        "    \n",
        "    if result.next_chunk_content:\n",
        "        contexto_completo += f\"[CONTEXTO SIGUIENTE]\\n{result.next_chunk_content}\"\n",
        "    \n",
        "    print(f\"{'=' * 80}\")\n",
        "    print(f\"Resultado {i} (Score: {result.score:.4f})\")\n",
        "    print(f\"{'=' * 80}\")\n",
        "    print(contexto_completo)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resumen\n",
        "\n",
        "Este notebook muestra c√≥mo usar Ungraph para:\n",
        "\n",
        "1. ‚úÖ Configurar conexi√≥n a Neo4j (variables de entorno o program√°tica)\n",
        "2. ‚úÖ Obtener recomendaciones de chunking con explicaciones\n",
        "3. ‚úÖ Ingerir documentos (Markdown, TXT, Word)\n",
        "4. ‚úÖ Buscar informaci√≥n (texto simple y b√∫squeda h√≠brida)\n",
        "5. ‚úÖ Construir contexto completo para RAG\n",
        "\n",
        "### Pr√≥ximos Pasos\n",
        "\n",
        "- Explorar m√°s opciones de configuraci√≥n\n",
        "- Probar con diferentes modelos de embeddings\n",
        "- Experimentar con diferentes estrategias de chunking\n",
        "- Crear patrones personalizados de estructura en el grafo\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
