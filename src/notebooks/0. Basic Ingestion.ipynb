{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16eba511",
   "metadata": {},
   "source": [
    "# Basic Ingestion\n",
    "El proposito de este notebook es establecers lo paso que voy a necesitar, las funciones a nivel atómico para lograr expresar los componentes del ETL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a77ef29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path Folder data added: D:\\projects\\Ungraph\\src\\data\n",
      "Path Folder utils added: D:\\projects\\Ungraph\\src\\utils\n"
     ]
    }
   ],
   "source": [
    "def add_src_to_path(path_folder: str):\n",
    "    ''' \n",
    "    Helper function for adding the \"path_folder\" directory to the path.\n",
    "    in order to work on notebooks and scripts\n",
    "    '''\n",
    "    import sys\n",
    "    import importlib.util\n",
    "    from pathlib import Path\n",
    "\n",
    "    base_path = Path().resolve()\n",
    "    for parent in [base_path] + list(base_path.parents):\n",
    "        candidate = parent / path_folder  # <-- fix: use string, not set\n",
    "        if candidate.exists():\n",
    "            if str(candidate) not in sys.path:\n",
    "                sys.path.append(str(candidate))\n",
    "                print(f\"Path Folder {path_folder} added: {candidate}\")\n",
    "            return\n",
    "    print(f\"Not found '{path_folder}' folder on the hierarchy of directories\")\n",
    "\n",
    "add_src_to_path(path_folder=\"data\")\n",
    "add_src_to_path(path_folder=\"utils\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0510a037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader, UnstructuredWordDocumentLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_docling import DoclingLoader\n",
    "import logging\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2c85fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from handlers import find_in_project\n",
    "from IPython.display import display_markdown, Markdown\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "885ff694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 19:09:18,717 - INFO - Encontrado: D:\\projects\\Ungraph\\src\\data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('D:/projects/Ungraph/src/data')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = find_in_project(\n",
    "    target = \"data\",\n",
    "    search_type = \"folder\",\n",
    "    project_root = None)\n",
    "\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7af4e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(\"Basic Structured Ingestion\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a5af2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('D:/projects/Ungraph/src/data/AnnyLetter.txt')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_path = data_path / \"AnnyLetter.txt\"\n",
    "txt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "542472ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('D:/projects/Ungraph/src/data/110225.md')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_path = data_path  /\"110225.md\"\n",
    "md_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99643053",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "\n",
    "1. Cargar la data, txt, markdown, docs, pfds\n",
    "\n",
    "- Detectar el encoding\n",
    "- Convertir a Markdown\n",
    "\n",
    "\n",
    "2. Seccionarla en sus debidos objetos Document Object que servirán para procesar el archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71b28914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, allowed_characters=None, replacement=\" \", remove_accents=True):\n",
    "    \"\"\"\n",
    "    Cleans a string of text by removing unwanted characters.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting text cleaning.\")\n",
    "    if remove_accents:\n",
    "        # Remove accents from the text\n",
    "        text = unicodedata.normalize(\"NFD\", text)\n",
    "        text = text.encode(\"ascii\", \"ignore\").decode(\"utf-8\")\n",
    "\n",
    "    if allowed_characters is None:\n",
    "        allowed_characters = \"a-zA-Z0-9áéíóúÁÉÍÓÚñÑ.,;:!?'\\\"()\\\\[\\\\]{}<>\\\\-\\\\_@#%&/\\\\s\"\n",
    "\n",
    "    # Create a pattern for disallowed characters\n",
    "    pattern = f\"[^{allowed_characters}]\"\n",
    "\n",
    "    # Replace disallowed characters\n",
    "    cleaned_text = re.sub(pattern, replacement, text)\n",
    "\n",
    "    # Remove duplicate spaces\n",
    "    cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text).strip()\n",
    "\n",
    "    logger.info(\"Text cleaning completed.\")\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d73b448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_encoding(file_path: Path, sample_size: int = 10000) -> str:\n",
    "    \"\"\"\n",
    "    Detecta automáticamente la codificación de un archivo de texto.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : Path\n",
    "        Ruta al archivo a analizar\n",
    "    sample_size : int, optional\n",
    "        Tamaño de la muestra a leer para detectar la codificación (default: 10000 bytes)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Nombre de la codificación detectada\n",
    "    \"\"\"\n",
    "    # Lista de codificaciones comunes a probar como fallback\n",
    "    fallback_encodings = ['utf-8', 'windows-1252', 'latin-1', 'iso-8859-1', 'cp1252']\n",
    "    \n",
    "    # Intentar usar chardet si está disponible\n",
    "    try:\n",
    "        import chardet\n",
    "        with open(file_path, 'rb') as f:\n",
    "            sample = f.read(sample_size)\n",
    "            result = chardet.detect(sample)\n",
    "            if result['encoding'] and result['confidence'] > 0.7:\n",
    "                detected_encoding = result['encoding'].lower()\n",
    "                logger.info(f\"Codificación detectada por chardet: {detected_encoding} (confianza: {result['confidence']:.2%})\")\n",
    "                return detected_encoding\n",
    "    except ImportError:\n",
    "        logger.debug(\"chardet no está instalado, usando método de fallback\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error al detectar codificación con chardet: {e}, usando método de fallback\")\n",
    "    \n",
    "    # Método de fallback: probar codificaciones comunes\n",
    "    logger.info(\"Intentando detectar codificación mediante prueba de lectura...\")\n",
    "    for enc in fallback_encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=enc) as f:\n",
    "                f.read(sample_size)\n",
    "            logger.info(f\"Codificación detectada por prueba: {enc}\")\n",
    "            return enc\n",
    "        except (UnicodeDecodeError, UnicodeError):\n",
    "            continue\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    # Si ninguna funciona, retornar utf-8 como default\n",
    "    logger.warning(\"No se pudo detectar la codificación, usando utf-8 como default\")\n",
    "    return 'utf-8'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "580a4d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_file_to_markdown(text, llm_extractor = None):\n",
    "    '''\n",
    "    El propósito de esta función es ser un conversor para pasar de .txt a Markdown, buscando estructura en el documento.\n",
    "    El resultado es un archivo .md con la estructura del documento original,\n",
    "    y debe preservar el texto y sus palabras de manera exacta, sin ningún cambio,\n",
    "    edición ni corrección sobre el contenido proporcionado.\n",
    "    '''\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "    if llm_extractor is None:\n",
    "        llm_extractor = ChatOllama(model=\"llama3.2:3b\", base_url=\"http://127.0.0.1:11434\", temperature=0.0)\n",
    "    else:\n",
    "        llm_extractor = llm_extractor\n",
    "\n",
    "    # Definir el prompt garantizando estricto respeto y preservación textual\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"As a markdown writer, you are given a text.\n",
    "    Your task is to write a markdown document based on the text.\n",
    "    The document should be in the following format:\n",
    "\n",
    "    - Title:\n",
    "    - Body:\n",
    "    - Footer:\n",
    "    - Entities:\n",
    "\n",
    "    You MUST STRICTLY preserve every word, spelling, accent and punctuation exactly as given in the input text. \n",
    "    DO NOT correct, modify or edit the original words under any circumstance, not even to fix grammar, accents or spelling.\n",
    "    Do not summarize nor paraphrase. Absolutely all sentences, words and their structure must remain as in the provided input.\n",
    "\n",
    "    Identify entities when possible (such as names, places, dates, etc.), but the main text content must remain unchanged and true to the input.\n",
    "    Respect the above format and do not alter the integrity of the original wording.\n",
    "    \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{text}\")\n",
    "    ])\n",
    "\n",
    "    llm_writter = prompt | llm_extractor | StrOutputParser()\n",
    "    response = llm_writter.invoke({\"text\": text})\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da5d4b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1 == 0:\n",
    "    response = convert_file_to_markdown(\"\"\"Pa, si fuera a decir que en 5 años tiene algo hecho, una sola cosa, ¿Qué sería esa única cosa?\" Y si bien pensé primero en que esperaba que mi hijo fuera independiente en 5 años, pensándolo en una segunda mano estaría bien que viviéramos un poco más de tiempo juntos. Siento que me falta vida por compartirle, y que quiero hacer más cosas con mi hijo.\"\"\")\n",
    "    Markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab311fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_markdown_file(file_path: Path, clean: bool = True) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Carga un archivo Markdown (.md) usando LangChain UnstructuredMarkdownLoader.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : Path\n",
    "        Ruta al archivo .md a cargar\n",
    "    clean : bool, optional\n",
    "        Si True, aplica limpieza de texto usando la función clean_text (default: True)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    List[Document]\n",
    "        Lista de objetos Document de LangChain con el contenido del archivo\n",
    "    \"\"\"\n",
    "    logger.info(f\"Cargando archivo Markdown: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Verificar que el archivo existe\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"El archivo no existe: {file_path}\")\n",
    "        \n",
    "        # Verificar extensión\n",
    "        if file_path.suffix.lower() not in ['.md', '.markdown']:\n",
    "            logger.warning(f\"El archivo no tiene extensión .md/.markdown: {file_path.suffix}\")\n",
    "        \n",
    "        # Cargar el archivo usando UnstructuredMarkdownLoader\n",
    "        loader = UnstructuredMarkdownLoader(str(file_path))\n",
    "        documents = loader.load()\n",
    "        \n",
    "        # Aplicar limpieza si está habilitada\n",
    "        if clean:\n",
    "            logger.info(\"Aplicando limpieza de texto a los documentos\")\n",
    "            for doc in documents:\n",
    "                doc.page_content = clean_text(doc.page_content)\n",
    "        \n",
    "        # Añadir metadatos adicionales\n",
    "        for doc in documents:\n",
    "            doc.metadata.update({\n",
    "                'file_type': 'markdown',\n",
    "                'filename': file_path.name,\n",
    "                'file_path': str(file_path)\n",
    "            })\n",
    "        \n",
    "        logger.info(f\"Archivo cargado exitosamente. Documentos generados: {len(documents)}\")\n",
    "        return documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al cargar archivo Markdown {file_path}: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c05510ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 19:09:18,824 - INFO - Cargando archivo Markdown: D:\\projects\\Ungraph\\src\\data\\110225.md\n",
      "2025-12-23 19:09:21,192 - INFO - Aplicando limpieza de texto a los documentos\n",
      "2025-12-23 19:09:21,193 - INFO - Starting text cleaning.\n",
      "2025-12-23 19:09:21,196 - INFO - Text cleaning completed.\n",
      "2025-12-23 19:09:21,198 - INFO - Archivo cargado exitosamente. Documentos generados: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md', 'file_type': 'markdown', 'filename': '110225.md', 'file_path': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md'}, page_content='Incluso es lo primero que se abrio, quiza lo que me falta en este momento es la palabra, y necesite encontrarme en ella para retomar mi camino, hoy mi hijo me hace pregunta ingenua pero solemne, como si supiera que debia de poner en mi lobulo frontal. \"Pa, si fuera a decir que en 5 anos tiene algo hecho, una sola cosa, Que seria esa unica cosa?\" Y si bien pense primero en que esperaba que mi hijo fuera independiente en 5 anos, pensandolo en una segunda mano estaria bien que vivieramos un poco mas de tiempo juntos. Siento que me falta vida por compartirle, y que quiero hacer mas cosas con mi hijo. Hoy estoy escribiendo en el silencio de mi habitacion mirando el cielo, y confirmo que a veces es el mismo cielo en todas partes, pero atardece diferente, anoche distinto. Me relato estas cosas porque quiero narrar como estoy desarrollando la vision de mi mismo, la que quiero para mi, el humano que quiero moldear en este presente, y suena ambicioso, pero no me he rendido con la busqueda, no he tirado la toalla a falta de mejores opciones, no me resigno. Si tuviera algo hecho en 5 anos, seria nuestra empresa de computacion cuantica rindiendo frutos en ese contexto, viviendo de la investigacion, y el conocimiento. Es decir puedo ver en el futuro cercano que varias cosas van a converger de manera natural, en el estudio de la complejidad, la emergencia, el computo cuantico y la inteligencia artificial hay cosas muy profundas que estan atadas conceptualmente pro muchos conceptos que ameritan verse como partes de un todo, como mezclas, conceptos que van en contra de la logica, y otros que solo cobran sentido en la contradiccion de su naturaleza. Quiza lo que me falta es el ejercicio diario de escribir, de narrarme el hombre que deje de ser, el que me estoy convirtiendo. Esto puede ser el querido Diario que tanto he necesitado hacer. \"La vida que sonamos esta del otro lado de las acciones que nos reusamos a hacer\". Cual es esa vida que me imagino?, Me imagino que logramos encontrar una forma de minar el conocimiento que de verdad pueda usarse para crear y hacer cosas con un contexto mas rico, como investigacion de medicamentos e investigacion. La vida que me imagino es la de un cientifico, artista, que se dedica a escribir sus dias y sus caminos de aprendizaje. Quiza lo que necesito es un ejercicio de escritura diaria de 30 minutos donde decanto todo en la noche, y planeo de verdad el otro dia. La mayoria de mis planes estan escritos sobre el papel, los objetivos y metas que me escribo a diario, pero que muchas veces estan sobredimensionadas porque el tiempo que tengo disponible en ciertas franjas de tiempo no es simetrico. Si una persona tiene claro el que puede atravesar casi cualquier como. Es real esto?, hasta donde lo he vivido si. He tenido momentos de mi vida en lso que no tengo esta niebla, en los que tengo claro cuales son esas acciones que me reuso a hacer y ejecuto en excelencia. He tenido momentos donde he sido mi palabra, donde he tenido compromiso conmigo mismo para lograr las cosas y lo he logrado como un crack. No es que no conozco mi fuerza, es que me falta claridad en la visualizacion de la imagen de mi yo a n tiempo. Esta decision deberia ser fundamental para fortalecer, crear, y derribar pilares que necesito y otros que no. Estaba dando una charla en la universidad de la FIUBA en argentina, al finalizar un estudiante se me acerca con mas dudas que preguntas, y conversamos por un momento. Se le iluminaron los ojos cuando le dije \"Los que te aman te van a decir que no, los que mas te importan no van a creer en ti, eso significa escucharse a uno mismo muchas veces\", es como una negacion de la negacion. Olvidarse del limitador que tenemos por defecto cultural o por una herencia castrante. El Alejandro de este momento de la vida necesita y quiere: En lo inmediato. Resolver mis deudas tecnicas en DE para poder ser competente hoy con QNOW, pipelines de datos, operaciones basadas en data. Ser mejor en conocimientos del backend y testeo de aplicaciones desarrolladas fullstack con AI para ser mas competente a nivel profesional. Ser mejor en quantum computing para poder sembrar buenas bases y raices de lo que quiero construir para mi futuro profesional. Aprender sobre Quimica Computacional para desempenarme realmente con lo qu estamos construyendo. Aprovechar sobre la complejidad, los grafos de conocimiento y la ingeniera de datos con Neo4j y analitica de grafos para poder mejorar mis skills como ingeniero de la complejidad. Dado que requiero desempeno mental muy alto y nitido necesito tener claro que las acciones para lograr esto derivan en: Mente no intoxicada, Bien alimentada, con un cuerpo que se mueve y le genera lso estimulos suficientes para crear coenxiones, descanso y reposo necesarios, sesiones de trabajo enfocado, de descanso real, remover el ocio de series, porno , fantasias del conducto de agua caliente, ([[Teoria del Conducto del Agua Caliente]]), si pudiera visualizar el siguiente Alejandro todos los dias lo construiiria pedazo a pedazso. Rutina de Alejandro Giraldo Londono Version n-esima potencia: Me levanto, hago ejercicio, tomo agua, gestiono el almuerzo, comida y el desayuno, estudio, me ducho con agua fria, me visto para trabajar en una pieza limpia, reviso el plan que hice anoche/semana para trabajar en todos mis compromisos hoy, priorizo urgente/importante, dedico tiempo a los compromisos conmigo mismo (citas, vueltas), cierro el dia caminando, escribiendo, tocando guitarra en dias lluviosos, haciendo ejercicio en el ginmansio o las barras, o jugando fuchi, o yendo al taller de literatura y venideros, pasando tiempo con mi hijo, conociendo nuevas personas, viendo los atardeceres desde lo alto de algun sitio. Que seria de esa persona? Puedo visualizarlo, es una vida sonada, es plena, requiere de una disciplina ferrea, de ajustar algunas cosas que hoy me distancian y me alejan de semejante futuro el que un conjunto acciones diarias asi podrian crear en mi. Para lograr esto mis metas deberian ser atomicas. Es decir, en este momento deberia enfocarme en lo que emerge de la necesidad, es decir, aprendizaje adaptativo y reactivo. Es decir, si emerge algo urgente que necesito reaccionar pronto, me debo adaptar al cambio e incluirlo masivamente. \"Ejemplo, deberia estar practicando todo lo que se en un mutante. Hoy quiero lograr, crear un MCP, FastAPI aplicacion, que permita conectarse a Neo4j local para que cualquier IDLE pueda trabajar con el grafo local, mi objetivo de 2 horas es crear una API con su debido composer, para levantar servicios de Neo4j, cypher shell y poder recrearlo todo. Almenos un endpoitn.\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = load_markdown_file(md_path)\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c80212f",
   "metadata": {},
   "source": [
    "### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f59d93d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ec8dbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para documentos narrativos/personales (como tu diario):\n",
    "chunk_size = 1000 # caracteres\n",
    "chunk_overlap = 200 # ~20% del chunk_size\n",
    "\n",
    "# Para documentos técnicos/estructurados:\n",
    "#chunk_size = 1500-2000\n",
    "#chunk_overlap = 300-400\n",
    "\n",
    "# Para documentos con estructura markdown (con headers):\n",
    "#chunk_size = 2000-3000\n",
    "#chunk_overlap = 400-600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6fbcfb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md', 'file_type': 'markdown', 'filename': '110225.md', 'file_path': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md'}, page_content='Incluso es lo primero que se abrio, quiza lo que me falta en este momento es la palabra, y necesite encontrarme en ella para retomar mi camino, hoy mi hijo me hace pregunta ingenua pero solemne, como si supiera que debia de poner en mi lobulo frontal. \"Pa, si fuera a decir que en 5 anos tiene algo hecho, una sola cosa, Que seria esa unica cosa?\" Y si bien pense primero en que esperaba que mi hijo fuera independiente en 5 anos, pensandolo en una segunda mano estaria bien que vivieramos un poco mas de tiempo juntos. Siento que me falta vida por compartirle, y que quiero hacer mas cosas con mi hijo. Hoy estoy escribiendo en el silencio de mi habitacion mirando el cielo, y confirmo que a veces es el mismo cielo en todas partes, pero atardece diferente, anoche distinto. Me relato estas cosas porque quiero narrar como estoy desarrollando la vision de mi mismo, la que quiero para mi, el humano que quiero moldear en este presente, y suena ambicioso, pero no me he rendido con la busqueda, no he'),\n",
       "  Document(metadata={'source': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md', 'file_type': 'markdown', 'filename': '110225.md', 'file_path': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md'}, page_content='quiero narrar como estoy desarrollando la vision de mi mismo, la que quiero para mi, el humano que quiero moldear en este presente, y suena ambicioso, pero no me he rendido con la busqueda, no he tirado la toalla a falta de mejores opciones, no me resigno. Si tuviera algo hecho en 5 anos, seria nuestra empresa de computacion cuantica rindiendo frutos en ese contexto, viviendo de la investigacion, y el conocimiento. Es decir puedo ver en el futuro cercano que varias cosas van a converger de manera natural, en el estudio de la complejidad, la emergencia, el computo cuantico y la inteligencia artificial hay cosas muy profundas que estan atadas conceptualmente pro muchos conceptos que ameritan verse como partes de un todo, como mezclas, conceptos que van en contra de la logica, y otros que solo cobran sentido en la contradiccion de su naturaleza. Quiza lo que me falta es el ejercicio diario de escribir, de narrarme el hombre que deje de ser, el que me estoy convirtiendo. Esto puede ser el'),\n",
       "  Document(metadata={'source': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md', 'file_type': 'markdown', 'filename': '110225.md', 'file_path': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md'}, page_content='cobran sentido en la contradiccion de su naturaleza. Quiza lo que me falta es el ejercicio diario de escribir, de narrarme el hombre que deje de ser, el que me estoy convirtiendo. Esto puede ser el querido Diario que tanto he necesitado hacer. \"La vida que sonamos esta del otro lado de las acciones que nos reusamos a hacer\". Cual es esa vida que me imagino?, Me imagino que logramos encontrar una forma de minar el conocimiento que de verdad pueda usarse para crear y hacer cosas con un contexto mas rico, como investigacion de medicamentos e investigacion. La vida que me imagino es la de un cientifico, artista, que se dedica a escribir sus dias y sus caminos de aprendizaje. Quiza lo que necesito es un ejercicio de escritura diaria de 30 minutos donde decanto todo en la noche, y planeo de verdad el otro dia. La mayoria de mis planes estan escritos sobre el papel, los objetivos y metas que me escribo a diario, pero que muchas veces estan sobredimensionadas porque el tiempo que tengo'),\n",
       "  Document(metadata={'source': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md', 'file_type': 'markdown', 'filename': '110225.md', 'file_path': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md'}, page_content='de verdad el otro dia. La mayoria de mis planes estan escritos sobre el papel, los objetivos y metas que me escribo a diario, pero que muchas veces estan sobredimensionadas porque el tiempo que tengo disponible en ciertas franjas de tiempo no es simetrico. Si una persona tiene claro el que puede atravesar casi cualquier como. Es real esto?, hasta donde lo he vivido si. He tenido momentos de mi vida en lso que no tengo esta niebla, en los que tengo claro cuales son esas acciones que me reuso a hacer y ejecuto en excelencia. He tenido momentos donde he sido mi palabra, donde he tenido compromiso conmigo mismo para lograr las cosas y lo he logrado como un crack. No es que no conozco mi fuerza, es que me falta claridad en la visualizacion de la imagen de mi yo a n tiempo. Esta decision deberia ser fundamental para fortalecer, crear, y derribar pilares que necesito y otros que no. Estaba dando una charla en la universidad de la FIUBA en argentina, al finalizar un estudiante se me acerca'),\n",
       "  Document(metadata={'source': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md', 'file_type': 'markdown', 'filename': '110225.md', 'file_path': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md'}, page_content='ser fundamental para fortalecer, crear, y derribar pilares que necesito y otros que no. Estaba dando una charla en la universidad de la FIUBA en argentina, al finalizar un estudiante se me acerca con mas dudas que preguntas, y conversamos por un momento. Se le iluminaron los ojos cuando le dije \"Los que te aman te van a decir que no, los que mas te importan no van a creer en ti, eso significa escucharse a uno mismo muchas veces\", es como una negacion de la negacion. Olvidarse del limitador que tenemos por defecto cultural o por una herencia castrante. El Alejandro de este momento de la vida necesita y quiere: En lo inmediato. Resolver mis deudas tecnicas en DE para poder ser competente hoy con QNOW, pipelines de datos, operaciones basadas en data. Ser mejor en conocimientos del backend y testeo de aplicaciones desarrolladas fullstack con AI para ser mas competente a nivel profesional. Ser mejor en quantum computing para poder sembrar buenas bases y raices de lo que quiero construir'),\n",
       "  Document(metadata={'source': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md', 'file_type': 'markdown', 'filename': '110225.md', 'file_path': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md'}, page_content='y testeo de aplicaciones desarrolladas fullstack con AI para ser mas competente a nivel profesional. Ser mejor en quantum computing para poder sembrar buenas bases y raices de lo que quiero construir para mi futuro profesional. Aprender sobre Quimica Computacional para desempenarme realmente con lo qu estamos construyendo. Aprovechar sobre la complejidad, los grafos de conocimiento y la ingeniera de datos con Neo4j y analitica de grafos para poder mejorar mis skills como ingeniero de la complejidad. Dado que requiero desempeno mental muy alto y nitido necesito tener claro que las acciones para lograr esto derivan en: Mente no intoxicada, Bien alimentada, con un cuerpo que se mueve y le genera lso estimulos suficientes para crear coenxiones, descanso y reposo necesarios, sesiones de trabajo enfocado, de descanso real, remover el ocio de series, porno , fantasias del conducto de agua caliente, ([[Teoria del Conducto del Agua Caliente]]), si pudiera visualizar el siguiente Alejandro'),\n",
       "  Document(metadata={'source': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md', 'file_type': 'markdown', 'filename': '110225.md', 'file_path': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md'}, page_content='enfocado, de descanso real, remover el ocio de series, porno , fantasias del conducto de agua caliente, ([[Teoria del Conducto del Agua Caliente]]), si pudiera visualizar el siguiente Alejandro todos los dias lo construiiria pedazo a pedazso. Rutina de Alejandro Giraldo Londono Version n-esima potencia: Me levanto, hago ejercicio, tomo agua, gestiono el almuerzo, comida y el desayuno, estudio, me ducho con agua fria, me visto para trabajar en una pieza limpia, reviso el plan que hice anoche/semana para trabajar en todos mis compromisos hoy, priorizo urgente/importante, dedico tiempo a los compromisos conmigo mismo (citas, vueltas), cierro el dia caminando, escribiendo, tocando guitarra en dias lluviosos, haciendo ejercicio en el ginmansio o las barras, o jugando fuchi, o yendo al taller de literatura y venideros, pasando tiempo con mi hijo, conociendo nuevas personas, viendo los atardeceres desde lo alto de algun sitio. Que seria de esa persona? Puedo visualizarlo, es una vida sonada,'),\n",
       "  Document(metadata={'source': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md', 'file_type': 'markdown', 'filename': '110225.md', 'file_path': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md'}, page_content='literatura y venideros, pasando tiempo con mi hijo, conociendo nuevas personas, viendo los atardeceres desde lo alto de algun sitio. Que seria de esa persona? Puedo visualizarlo, es una vida sonada, es plena, requiere de una disciplina ferrea, de ajustar algunas cosas que hoy me distancian y me alejan de semejante futuro el que un conjunto acciones diarias asi podrian crear en mi. Para lograr esto mis metas deberian ser atomicas. Es decir, en este momento deberia enfocarme en lo que emerge de la necesidad, es decir, aprendizaje adaptativo y reactivo. Es decir, si emerge algo urgente que necesito reaccionar pronto, me debo adaptar al cambio e incluirlo masivamente. \"Ejemplo, deberia estar practicando todo lo que se en un mutante. Hoy quiero lograr, crear un MCP, FastAPI aplicacion, que permita conectarse a Neo4j local para que cualquier IDLE pueda trabajar con el grafo local, mi objetivo de 2 horas es crear una API con su debido composer, para levantar servicios de Neo4j, cypher shell'),\n",
       "  Document(metadata={'source': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md', 'file_type': 'markdown', 'filename': '110225.md', 'file_path': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md'}, page_content='conectarse a Neo4j local para que cualquier IDLE pueda trabajar con el grafo local, mi objetivo de 2 horas es crear una API con su debido composer, para levantar servicios de Neo4j, cypher shell y poder recrearlo todo. Almenos un endpoitn.\"')]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, \n",
    "chunk_overlap=chunk_overlap)\n",
    "\n",
    "chunks = []\n",
    "for doc in documents:\n",
    "    texts = text_splitter.split_documents([doc])\n",
    "    chunks.append(texts)\n",
    "\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md', 'file_type': 'markdown', 'filename': '110225.md', 'file_path': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md'}, page_content='Incluso es lo primero que se abrio, quiza lo que me falta en este momento es la palabra, y necesite encontrarme en ella para retomar mi camino, hoy mi hijo me hace pregunta ingenua pero solemne, como si supiera que debia de poner en mi lobulo frontal. \"Pa, si fuera a decir que en 5 anos tiene algo hecho, una sola cosa, Que seria esa unica cosa?\" Y si bien pense primero en que esperaba que mi hijo fuera independiente en 5 anos, pensandolo en una segunda mano estaria bien que vivieramos un poco mas de tiempo juntos. Siento que me falta vida por compartirle, y que quiero hacer mas cosas con mi hijo. Hoy estoy escribiendo en el silencio de mi habitacion mirando el cielo, y confirmo que a veces es el mismo cielo en todas partes, pero atardece diferente, anoche distinto. Me relato estas cosas porque quiero narrar como estoy desarrollando la vision de mi mismo, la que quiero para mi, el humano que quiero moldear en este presente, y suena ambicioso, pero no me he rendido con la busqueda, no he'),\n",
       " Document(metadata={'source': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md', 'file_type': 'markdown', 'filename': '110225.md', 'file_path': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md'}, page_content='quiero narrar como estoy desarrollando la vision de mi mismo, la que quiero para mi, el humano que quiero moldear en este presente, y suena ambicioso, pero no me he rendido con la busqueda, no he tirado la toalla a falta de mejores opciones, no me resigno. Si tuviera algo hecho en 5 anos, seria nuestra empresa de computacion cuantica rindiendo frutos en ese contexto, viviendo de la investigacion, y el conocimiento. Es decir puedo ver en el futuro cercano que varias cosas van a converger de manera natural, en el estudio de la complejidad, la emergencia, el computo cuantico y la inteligencia artificial hay cosas muy profundas que estan atadas conceptualmente pro muchos conceptos que ameritan verse como partes de un todo, como mezclas, conceptos que van en contra de la logica, y otros que solo cobran sentido en la contradiccion de su naturaleza. Quiza lo que me falta es el ejercicio diario de escribir, de narrarme el hombre que deje de ser, el que me estoy convirtiendo. Esto puede ser el'),\n",
       " Document(metadata={'source': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md', 'file_type': 'markdown', 'filename': '110225.md', 'file_path': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md'}, page_content='cobran sentido en la contradiccion de su naturaleza. Quiza lo que me falta es el ejercicio diario de escribir, de narrarme el hombre que deje de ser, el que me estoy convirtiendo. Esto puede ser el querido Diario que tanto he necesitado hacer. \"La vida que sonamos esta del otro lado de las acciones que nos reusamos a hacer\". Cual es esa vida que me imagino?, Me imagino que logramos encontrar una forma de minar el conocimiento que de verdad pueda usarse para crear y hacer cosas con un contexto mas rico, como investigacion de medicamentos e investigacion. La vida que me imagino es la de un cientifico, artista, que se dedica a escribir sus dias y sus caminos de aprendizaje. Quiza lo que necesito es un ejercicio de escritura diaria de 30 minutos donde decanto todo en la noche, y planeo de verdad el otro dia. La mayoria de mis planes estan escritos sobre el papel, los objetivos y metas que me escribo a diario, pero que muchas veces estan sobredimensionadas porque el tiempo que tengo'),\n",
       " Document(metadata={'source': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md', 'file_type': 'markdown', 'filename': '110225.md', 'file_path': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md'}, page_content='de verdad el otro dia. La mayoria de mis planes estan escritos sobre el papel, los objetivos y metas que me escribo a diario, pero que muchas veces estan sobredimensionadas porque el tiempo que tengo disponible en ciertas franjas de tiempo no es simetrico. Si una persona tiene claro el que puede atravesar casi cualquier como. Es real esto?, hasta donde lo he vivido si. He tenido momentos de mi vida en lso que no tengo esta niebla, en los que tengo claro cuales son esas acciones que me reuso a hacer y ejecuto en excelencia. He tenido momentos donde he sido mi palabra, donde he tenido compromiso conmigo mismo para lograr las cosas y lo he logrado como un crack. No es que no conozco mi fuerza, es que me falta claridad en la visualizacion de la imagen de mi yo a n tiempo. Esta decision deberia ser fundamental para fortalecer, crear, y derribar pilares que necesito y otros que no. Estaba dando una charla en la universidad de la FIUBA en argentina, al finalizar un estudiante se me acerca'),\n",
       " Document(metadata={'source': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md', 'file_type': 'markdown', 'filename': '110225.md', 'file_path': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md'}, page_content='ser fundamental para fortalecer, crear, y derribar pilares que necesito y otros que no. Estaba dando una charla en la universidad de la FIUBA en argentina, al finalizar un estudiante se me acerca con mas dudas que preguntas, y conversamos por un momento. Se le iluminaron los ojos cuando le dije \"Los que te aman te van a decir que no, los que mas te importan no van a creer en ti, eso significa escucharse a uno mismo muchas veces\", es como una negacion de la negacion. Olvidarse del limitador que tenemos por defecto cultural o por una herencia castrante. El Alejandro de este momento de la vida necesita y quiere: En lo inmediato. Resolver mis deudas tecnicas en DE para poder ser competente hoy con QNOW, pipelines de datos, operaciones basadas en data. Ser mejor en conocimientos del backend y testeo de aplicaciones desarrolladas fullstack con AI para ser mas competente a nivel profesional. Ser mejor en quantum computing para poder sembrar buenas bases y raices de lo que quiero construir'),\n",
       " Document(metadata={'source': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md', 'file_type': 'markdown', 'filename': '110225.md', 'file_path': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md'}, page_content='y testeo de aplicaciones desarrolladas fullstack con AI para ser mas competente a nivel profesional. Ser mejor en quantum computing para poder sembrar buenas bases y raices de lo que quiero construir para mi futuro profesional. Aprender sobre Quimica Computacional para desempenarme realmente con lo qu estamos construyendo. Aprovechar sobre la complejidad, los grafos de conocimiento y la ingeniera de datos con Neo4j y analitica de grafos para poder mejorar mis skills como ingeniero de la complejidad. Dado que requiero desempeno mental muy alto y nitido necesito tener claro que las acciones para lograr esto derivan en: Mente no intoxicada, Bien alimentada, con un cuerpo que se mueve y le genera lso estimulos suficientes para crear coenxiones, descanso y reposo necesarios, sesiones de trabajo enfocado, de descanso real, remover el ocio de series, porno , fantasias del conducto de agua caliente, ([[Teoria del Conducto del Agua Caliente]]), si pudiera visualizar el siguiente Alejandro'),\n",
       " Document(metadata={'source': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md', 'file_type': 'markdown', 'filename': '110225.md', 'file_path': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md'}, page_content='enfocado, de descanso real, remover el ocio de series, porno , fantasias del conducto de agua caliente, ([[Teoria del Conducto del Agua Caliente]]), si pudiera visualizar el siguiente Alejandro todos los dias lo construiiria pedazo a pedazso. Rutina de Alejandro Giraldo Londono Version n-esima potencia: Me levanto, hago ejercicio, tomo agua, gestiono el almuerzo, comida y el desayuno, estudio, me ducho con agua fria, me visto para trabajar en una pieza limpia, reviso el plan que hice anoche/semana para trabajar en todos mis compromisos hoy, priorizo urgente/importante, dedico tiempo a los compromisos conmigo mismo (citas, vueltas), cierro el dia caminando, escribiendo, tocando guitarra en dias lluviosos, haciendo ejercicio en el ginmansio o las barras, o jugando fuchi, o yendo al taller de literatura y venideros, pasando tiempo con mi hijo, conociendo nuevas personas, viendo los atardeceres desde lo alto de algun sitio. Que seria de esa persona? Puedo visualizarlo, es una vida sonada,'),\n",
       " Document(metadata={'source': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md', 'file_type': 'markdown', 'filename': '110225.md', 'file_path': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md'}, page_content='literatura y venideros, pasando tiempo con mi hijo, conociendo nuevas personas, viendo los atardeceres desde lo alto de algun sitio. Que seria de esa persona? Puedo visualizarlo, es una vida sonada, es plena, requiere de una disciplina ferrea, de ajustar algunas cosas que hoy me distancian y me alejan de semejante futuro el que un conjunto acciones diarias asi podrian crear en mi. Para lograr esto mis metas deberian ser atomicas. Es decir, en este momento deberia enfocarme en lo que emerge de la necesidad, es decir, aprendizaje adaptativo y reactivo. Es decir, si emerge algo urgente que necesito reaccionar pronto, me debo adaptar al cambio e incluirlo masivamente. \"Ejemplo, deberia estar practicando todo lo que se en un mutante. Hoy quiero lograr, crear un MCP, FastAPI aplicacion, que permita conectarse a Neo4j local para que cualquier IDLE pueda trabajar con el grafo local, mi objetivo de 2 horas es crear una API con su debido composer, para levantar servicios de Neo4j, cypher shell'),\n",
       " Document(metadata={'source': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md', 'file_type': 'markdown', 'filename': '110225.md', 'file_path': 'D:\\\\projects\\\\Ungraph\\\\src\\\\data\\\\110225.md'}, page_content='conectarse a Neo4j local para que cualquier IDLE pueda trabajar con el grafo local, mi objetivo de 2 horas es crear una API con su debido composer, para levantar servicios de Neo4j, cypher shell y poder recrearlo todo. Almenos un endpoitn.\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69a95152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bd837ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc_file(file_path: Path, clean: bool = True) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Carga un archivo Word (.doc, .docx) usando LangChain UnstructuredWordDocumentLoader.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : Path\n",
    "        Ruta al archivo .doc o .docx a cargar\n",
    "    clean : bool, optional\n",
    "        Si True, aplica limpieza de texto usando la función clean_text (default: True)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    List[Document]\n",
    "        Lista de objetos Document de LangChain con el contenido del archivo\n",
    "    \"\"\"\n",
    "    logger.info(f\"Cargando archivo Word: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Verificar que el archivo existe\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"El archivo no existe: {file_path}\")\n",
    "        \n",
    "        # Verificar extensión\n",
    "        if file_path.suffix.lower() not in ['.doc', '.docx']:\n",
    "            logger.warning(f\"El archivo no tiene extensión .doc/.docx: {file_path.suffix}\")\n",
    "        \n",
    "        # Cargar el archivo usando UnstructuredWordDocumentLoader\n",
    "        loader = UnstructuredWordDocumentLoader(str(file_path))\n",
    "        documents = loader.load()\n",
    "        \n",
    "        # Aplicar limpieza si está habilitada\n",
    "        if clean:\n",
    "            logger.info(\"Aplicando limpieza de texto a los documentos\")\n",
    "            for doc in documents:\n",
    "                doc.page_content = clean_text(doc.page_content)\n",
    "        \n",
    "        # Añadir metadatos adicionales\n",
    "        for doc in documents:\n",
    "            doc.metadata.update({\n",
    "                'file_type': 'word',\n",
    "                'filename': file_path.name,\n",
    "                'file_path': str(file_path)\n",
    "            })\n",
    "        \n",
    "        logger.info(f\"Archivo cargado exitosamente. Documentos generados: {len(documents)}\")\n",
    "        return documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al cargar archivo Word {file_path}: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dee8bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función load_txt_file actualizada con detección automática de codificación\n",
    "def load_txt_file(file_path: Path, encoding: str = None, clean: bool = True, auto_detect: bool = True) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Carga un archivo de texto (.txt) usando LangChain TextLoader.\n",
    "    Incluye detección automática de codificación y fallback a codificaciones comunes.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : Path\n",
    "        Ruta al archivo .txt a cargar\n",
    "    encoding : str, optional\n",
    "        Codificación del archivo. Si es None y auto_detect=True, se detecta automáticamente.\n",
    "        Si se especifica, se usa esa codificación directamente (default: None)\n",
    "    clean : bool, optional\n",
    "        Si True, aplica limpieza de texto usando la función clean_text (default: True)\n",
    "    auto_detect : bool, optional\n",
    "        Si True, detecta automáticamente la codificación si no se especifica (default: True)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    List[Document]\n",
    "        Lista de objetos Document de LangChain con el contenido del archivo\n",
    "    \"\"\"\n",
    "    logger.info(f\"Cargando archivo de texto: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Verificar que el archivo existe\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"El archivo no existe: {file_path}\")\n",
    "        \n",
    "        # Verificar extensión\n",
    "        if file_path.suffix.lower() != '.txt':\n",
    "            logger.warning(f\"El archivo no tiene extensión .txt: {file_path.suffix}\")\n",
    "        \n",
    "        # Determinar la codificación a usar\n",
    "        if encoding is None and auto_detect:\n",
    "            encoding = detect_encoding(file_path)\n",
    "            logger.info(f\"Usando codificación detectada: {encoding}\")\n",
    "        elif encoding is None:\n",
    "            encoding = \"utf-8\"\n",
    "            logger.info(f\"Usando codificación por defecto: {encoding}\")\n",
    "        \n",
    "        # Lista de codificaciones de fallback si la principal falla\n",
    "        fallback_encodings = ['windows-1252', 'latin-1', 'iso-8859-1', 'cp1252']\n",
    "        \n",
    "        # Intentar cargar con la codificación especificada\n",
    "        last_error = None\n",
    "        try:\n",
    "            loader = TextLoader(str(file_path), encoding=encoding)\n",
    "            documents = loader.load()\n",
    "        except (UnicodeDecodeError, RuntimeError) as e:\n",
    "            last_error = e\n",
    "            logger.warning(f\"Error al cargar con codificación {encoding}: {e}\")\n",
    "            \n",
    "            # Si auto_detect está activado y falló, intentar con fallback\n",
    "            if auto_detect:\n",
    "                logger.info(\"Intentando con codificaciones de fallback...\")\n",
    "                for fallback_enc in fallback_encodings:\n",
    "                    if fallback_enc == encoding:\n",
    "                        continue\n",
    "                    try:\n",
    "                        logger.info(f\"Intentando con codificación: {fallback_enc}\")\n",
    "                        loader = TextLoader(str(file_path), encoding=fallback_enc)\n",
    "                        documents = loader.load()\n",
    "                        encoding = fallback_enc  # Actualizar la codificación usada\n",
    "                        logger.info(f\"Archivo cargado exitosamente con codificación: {encoding}\")\n",
    "                        break\n",
    "                    except (UnicodeDecodeError, RuntimeError):\n",
    "                        continue\n",
    "                else:\n",
    "                    # Si todas las codificaciones fallan, lanzar el último error\n",
    "                    raise RuntimeError(f\"No se pudo cargar el archivo con ninguna codificación. Último error: {last_error}\") from last_error\n",
    "            else:\n",
    "                raise\n",
    "        \n",
    "        # Aplicar limpieza si está habilitada\n",
    "        if clean:\n",
    "            logger.info(\"Aplicando limpieza de texto a los documentos\")\n",
    "            for doc in documents:\n",
    "                doc.page_content = clean_text(doc.page_content)\n",
    "        \n",
    "        # Añadir metadatos adicionales\n",
    "        for doc in documents:\n",
    "            doc.metadata.update({\n",
    "                'file_type': 'txt',\n",
    "                'filename': file_path.name,\n",
    "                'file_path': str(file_path),\n",
    "                'encoding': encoding\n",
    "            })\n",
    "        \n",
    "        logger.info(f\"Archivo cargado exitosamente. Documentos generados: {len(documents)}\")\n",
    "        return documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al cargar archivo de texto {file_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "#documents =load_txt_file(data_path / \"AnnyLetter.txt\", encoding=\"utf-8\", clean=True)\n",
    "\n",
    "#documents \n",
    "#txt_docs = load_txt_file(data_path / \"AnnyLetter.txt\", clean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "543963a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_to_dataframe(documents: List[Document], path_to_persist: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    This function converts a list of Document objects into a pandas DataFrame \n",
    "    and saves the DataFrame to the specified path with a name that includes the filename\n",
    "    of the first document.\n",
    "\n",
    "    Parameters:\n",
    "    documents (List[Document]): List of Document objects to be converted.\n",
    "    path_to_persist (str): Path where the resulting DataFrame will be saved.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The resulting DataFrame containing the documents' data.\n",
    "    '''\n",
    "    # Prepare the data for the DataFrame\n",
    "    data = []\n",
    "    for doc in documents:\n",
    "        data.append({\n",
    "            'page_content': doc.page_content,\n",
    "            'chunk_id': doc.metadata.get('chunk_id', None),\n",
    "            'filename': doc.metadata.get('filename', None),\n",
    "            'page_number': doc.metadata.get('page_number', None),\n",
    "            'embeddings': doc.metadata.get('embeddings', None),\n",
    "            'is_unitary': doc.metadata.get('is_unitary', None),\n",
    "            'embeddings_dimensions': doc.metadata.get('embeddings_dimensions', None),\n",
    "            'embedding_encoder_info': doc.metadata.get('embedding_encoder_info', None)\n",
    "        })\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Extract the filename of the first document\n",
    "    if documents:\n",
    "        filename = documents[0].metadata.get('filename', 'default')\n",
    "    else:\n",
    "        filename = 'default'\n",
    "    \n",
    "    # Construct the full path with the filename included\n",
    "    full_path = f\"{path_to_persist}/document_data_{filename}.csv\"\n",
    "    \n",
    "    # Save the DataFrame to the specified path\n",
    "    df.to_csv(full_path, index=False, escapechar='\\\\')\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28a33c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_content</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>page_number</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>is_unitary</th>\n",
       "      <th>embeddings_dimensions</th>\n",
       "      <th>embedding_encoder_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Incluso es lo primero que se abrio, quiza lo q...</td>\n",
       "      <td>None</td>\n",
       "      <td>110225.md</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>quiero narrar como estoy desarrollando la visi...</td>\n",
       "      <td>None</td>\n",
       "      <td>110225.md</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cobran sentido en la contradiccion de su natur...</td>\n",
       "      <td>None</td>\n",
       "      <td>110225.md</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>de verdad el otro dia. La mayoria de mis plane...</td>\n",
       "      <td>None</td>\n",
       "      <td>110225.md</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ser fundamental para fortalecer, crear, y derr...</td>\n",
       "      <td>None</td>\n",
       "      <td>110225.md</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>y testeo de aplicaciones desarrolladas fullsta...</td>\n",
       "      <td>None</td>\n",
       "      <td>110225.md</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>enfocado, de descanso real, remover el ocio de...</td>\n",
       "      <td>None</td>\n",
       "      <td>110225.md</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>literatura y venideros, pasando tiempo con mi ...</td>\n",
       "      <td>None</td>\n",
       "      <td>110225.md</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>conectarse a Neo4j local para que cualquier ID...</td>\n",
       "      <td>None</td>\n",
       "      <td>110225.md</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        page_content chunk_id   filename  \\\n",
       "0  Incluso es lo primero que se abrio, quiza lo q...     None  110225.md   \n",
       "1  quiero narrar como estoy desarrollando la visi...     None  110225.md   \n",
       "2  cobran sentido en la contradiccion de su natur...     None  110225.md   \n",
       "3  de verdad el otro dia. La mayoria de mis plane...     None  110225.md   \n",
       "4  ser fundamental para fortalecer, crear, y derr...     None  110225.md   \n",
       "5  y testeo de aplicaciones desarrolladas fullsta...     None  110225.md   \n",
       "6  enfocado, de descanso real, remover el ocio de...     None  110225.md   \n",
       "7  literatura y venideros, pasando tiempo con mi ...     None  110225.md   \n",
       "8  conectarse a Neo4j local para que cualquier ID...     None  110225.md   \n",
       "\n",
       "  page_number embeddings is_unitary embeddings_dimensions  \\\n",
       "0        None       None       None                  None   \n",
       "1        None       None       None                  None   \n",
       "2        None       None       None                  None   \n",
       "3        None       None       None                  None   \n",
       "4        None       None       None                  None   \n",
       "5        None       None       None                  None   \n",
       "6        None       None       None                  None   \n",
       "7        None       None       None                  None   \n",
       "8        None       None       None                  None   \n",
       "\n",
       "  embedding_encoder_info  \n",
       "0                   None  \n",
       "1                   None  \n",
       "2                   None  \n",
       "3                   None  \n",
       "4                   None  \n",
       "5                   None  \n",
       "6                   None  \n",
       "7                   None  \n",
       "8                   None  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_to_dataframe(chunks[0], path_to_persist=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17a782cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dataframe_to_documents(df: pd.DataFrame) -> List[Document]:\n",
    "    ''' \n",
    "    La idea de esta función es añadir metadatos al objeto de Langchain que estamos creando.\n",
    "    Para que sea compatible, el atributo 'source' va necesitar existir. La otra es heradarlo de la clase,\n",
    "    sobreescribirlo y añadirle lo que queremos persistir del objeto.\n",
    "\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    documents = []\n",
    "    for index, row in df.iterrows():\n",
    "        metadata = {\n",
    "            'chunk_id': row['chunk_id'],\n",
    "            'filename': row['filename'],\n",
    "            'page_number': row['page_number'],\n",
    "            'embeddings': row['embeddings'],\n",
    "            'is_unitary': row['is_unitary'],\n",
    "            'embeddings_dimensions': row['embeddings_dimensions'],\n",
    "            'embedding_encoder_info': row['embedding_encoder_info']\n",
    "        }\n",
    "        document = Document(page_content=row['page_content'], metadata=metadata)\n",
    "        documents.append(document)\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525e0286",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb5246f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_encoder(model: str):\n",
    "    \"\"\"\n",
    "    Retrieves the embedding encoder based on the model specified.\n",
    "    \"\"\"\n",
    "    logger.info(\"Getting embedding encoder.\")\n",
    "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "    import torch\n",
    "    \n",
    "    if model == \"llama\":\n",
    "        logger.info(f\"model selected for embedding -> {model}.\")\n",
    "        return OllamaEmbeddings(\n",
    "            model=\"llama3.2:3b\",\n",
    "            base_url=\"http://0.0.0.0:11434\",\n",
    "            embed_instruction=\"passage: \",\n",
    "            temperature=0.0,\n",
    "            num_gpu=1,\n",
    "            num_thread=8,\n",
    "        )\n",
    "    if model == \"hf\":\n",
    "        logger.info(f\"model selected for embedding -> {model}.\")\n",
    "\n",
    "        model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        \n",
    "        # Detectar automáticamente si CUDA está disponible\n",
    "        if torch.cuda.is_available():\n",
    "            device = 'cuda'\n",
    "            logger.info(\"CUDA disponible, usando GPU para embeddings.\")\n",
    "        else:\n",
    "            device = 'cpu'\n",
    "            logger.info(\"CUDA no disponible, usando CPU para embeddings.\")\n",
    "        \n",
    "        model_kwargs = {'device': device}\n",
    "        encode_kwargs = {'normalize_embeddings': False}\n",
    "        return HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs=model_kwargs,\n",
    "            encode_kwargs=encode_kwargs\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"The function needs a valid embedding model ('openai' or 'hf') to encode data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "596a9aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 19:09:21,352 - INFO - Getting embedding encoder.\n",
      "2025-12-23 19:09:21,359 - INFO - model selected for embedding -> hf.\n",
      "2025-12-23 19:09:21,360 - INFO - CUDA no disponible, usando CPU para embeddings.\n",
      "C:\\Users\\jefai\\AppData\\Local\\Temp\\ipykernel_29032\\1759712085.py:34: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  return HuggingFaceEmbeddings(\n",
      "2025-12-23 19:09:21,362 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={'device': 'cpu'}, encode_kwargs={'normalize_embeddings': False}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_encoder =  get_embedding_encoder(\"hf\")\n",
    "embedding_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d77d9b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_object(documents: List[Document], embedding_encoder=None):\n",
    "    \"\"\"\n",
    "    Crea objetos Document con embeddings a partir de una lista de documentos (chunks).\n",
    "    Añade embeddings y metadatos necesarios para el procesamiento posterior.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    documents : List[Document]\n",
    "        Una lista de objetos Document (chunks) que ya han sido divididos del documento original.\n",
    "    embedding_encoder : object, optional\n",
    "        Un objeto encoder usado para generar embeddings de texto. Si es None, no se generan embeddings.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    List[Document]\n",
    "        Una lista de objetos Document con embeddings y metadatos añadidos.\n",
    "    \"\"\"\n",
    "    logger.info(\"Creating document objects with embeddings.\")\n",
    "    document_objects = []\n",
    "\n",
    "    if embedding_encoder is None:\n",
    "        logger.warning(\"Embedding encoder not provided. Documents will be created without embeddings.\")\n",
    "\n",
    "    for doc in documents:\n",
    "        try:\n",
    "            # Generar embeddings si se proporciona un encoder\n",
    "            text_embeddings = None\n",
    "            if embedding_encoder:\n",
    "                text_embeddings = embedding_encoder.embed_query(doc.page_content)\n",
    "            \n",
    "            # Crear o actualizar metadatos\n",
    "            document_metadata = doc.metadata.copy() if doc.metadata else {}\n",
    "            \n",
    "            # Añadir o actualizar metadatos específicos\n",
    "            document_metadata.update({\n",
    "                \"chunk_id\": document_metadata.get(\"chunk_id\", f\"{document_metadata.get('filename', 'unknown')}_{uuid.uuid4()}\"),\n",
    "                \"embeddings\": str(text_embeddings) if text_embeddings else \"\",\n",
    "                \"embeddings_dimensions\": len(text_embeddings) if text_embeddings else None,\n",
    "                \"embedding_encoder_info\": str(embedding_encoder) if embedding_encoder else \"\",\n",
    "            })\n",
    "            \n",
    "            # Crear el documento con el contenido y metadatos actualizados\n",
    "            document = Document(page_content=doc.page_content, metadata=document_metadata)\n",
    "            document_objects.append(document)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating document object: {e}\")\n",
    "            raise e\n",
    "\n",
    "    logger.info(f\"Document objects creation completed. Total documents: {len(document_objects)}\")\n",
    "    return document_objects\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73b8f302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 19:09:24,110 - INFO - Creating document objects with embeddings.\n",
      "2025-12-23 19:09:24,599 - INFO - Document objects creation completed. Total documents: 9\n"
     ]
    }
   ],
   "source": [
    "documents_with_embeddings = create_document_object(\n",
    "    documents=chunks[0],  # Lista de chunks del documento\n",
    "    embedding_encoder=embedding_encoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43c1d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir documentos con embeddings a DataFrame y guardar\n",
    "df = document_to_dataframe(\n",
    "    documents=documents_with_embeddings,  # Documentos CON embeddings\n",
    "    path_to_persist=\".\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fed56c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_content</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>page_number</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>is_unitary</th>\n",
       "      <th>embeddings_dimensions</th>\n",
       "      <th>embedding_encoder_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Incluso es lo primero que se abrio, quiza lo q...</td>\n",
       "      <td>110225.md_3da12fed-7e00-4ec6-bd0f-e8b889f37981</td>\n",
       "      <td>110225.md</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.03818148747086525, 0.04162796959280968, 0.0...</td>\n",
       "      <td>None</td>\n",
       "      <td>384</td>\n",
       "      <td>client=SentenceTransformer(\\n  (0): Transforme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>quiero narrar como estoy desarrollando la visi...</td>\n",
       "      <td>110225.md_2386e056-324b-470a-97f6-0b5a18a31d34</td>\n",
       "      <td>110225.md</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.04164035990834236, 0.01807655394077301, -0....</td>\n",
       "      <td>None</td>\n",
       "      <td>384</td>\n",
       "      <td>client=SentenceTransformer(\\n  (0): Transforme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cobran sentido en la contradiccion de su natur...</td>\n",
       "      <td>110225.md_4436e309-c4d1-4229-b46a-4e7abe088efa</td>\n",
       "      <td>110225.md</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.023161746561527252, 0.090914785861969, -0.0...</td>\n",
       "      <td>None</td>\n",
       "      <td>384</td>\n",
       "      <td>client=SentenceTransformer(\\n  (0): Transforme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>de verdad el otro dia. La mayoria de mis plane...</td>\n",
       "      <td>110225.md_448a5c94-3ec0-45ac-a94a-ebd92037a163</td>\n",
       "      <td>110225.md</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.02458001859486103, 0.0350651890039444, 0.00...</td>\n",
       "      <td>None</td>\n",
       "      <td>384</td>\n",
       "      <td>client=SentenceTransformer(\\n  (0): Transforme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ser fundamental para fortalecer, crear, y derr...</td>\n",
       "      <td>110225.md_0b500597-bef4-43d7-9d6e-850c56a716b4</td>\n",
       "      <td>110225.md</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.0082578519359231, 0.023625241592526436, 0.0...</td>\n",
       "      <td>None</td>\n",
       "      <td>384</td>\n",
       "      <td>client=SentenceTransformer(\\n  (0): Transforme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>y testeo de aplicaciones desarrolladas fullsta...</td>\n",
       "      <td>110225.md_bef32d9b-a179-413b-800e-3bc4ca48b353</td>\n",
       "      <td>110225.md</td>\n",
       "      <td>None</td>\n",
       "      <td>[-0.02488844469189644, -0.02288469299674034, -...</td>\n",
       "      <td>None</td>\n",
       "      <td>384</td>\n",
       "      <td>client=SentenceTransformer(\\n  (0): Transforme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>enfocado, de descanso real, remover el ocio de...</td>\n",
       "      <td>110225.md_b0cd5a3b-4741-42eb-bdd9-d992678250a0</td>\n",
       "      <td>110225.md</td>\n",
       "      <td>None</td>\n",
       "      <td>[-0.009872610680758953, 0.016194215044379234, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>384</td>\n",
       "      <td>client=SentenceTransformer(\\n  (0): Transforme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>literatura y venideros, pasando tiempo con mi ...</td>\n",
       "      <td>110225.md_9fa6b460-24aa-420c-a091-6e118908d590</td>\n",
       "      <td>110225.md</td>\n",
       "      <td>None</td>\n",
       "      <td>[-0.0019361716695129871, 0.008947310037910938,...</td>\n",
       "      <td>None</td>\n",
       "      <td>384</td>\n",
       "      <td>client=SentenceTransformer(\\n  (0): Transforme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>conectarse a Neo4j local para que cualquier ID...</td>\n",
       "      <td>110225.md_b6b0a0b3-3122-43b5-b68b-d3e8a66de499</td>\n",
       "      <td>110225.md</td>\n",
       "      <td>None</td>\n",
       "      <td>[0.06550847738981247, 0.0676582008600235, -0.0...</td>\n",
       "      <td>None</td>\n",
       "      <td>384</td>\n",
       "      <td>client=SentenceTransformer(\\n  (0): Transforme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        page_content  \\\n",
       "0  Incluso es lo primero que se abrio, quiza lo q...   \n",
       "1  quiero narrar como estoy desarrollando la visi...   \n",
       "2  cobran sentido en la contradiccion de su natur...   \n",
       "3  de verdad el otro dia. La mayoria de mis plane...   \n",
       "4  ser fundamental para fortalecer, crear, y derr...   \n",
       "5  y testeo de aplicaciones desarrolladas fullsta...   \n",
       "6  enfocado, de descanso real, remover el ocio de...   \n",
       "7  literatura y venideros, pasando tiempo con mi ...   \n",
       "8  conectarse a Neo4j local para que cualquier ID...   \n",
       "\n",
       "                                         chunk_id   filename page_number  \\\n",
       "0  110225.md_3da12fed-7e00-4ec6-bd0f-e8b889f37981  110225.md        None   \n",
       "1  110225.md_2386e056-324b-470a-97f6-0b5a18a31d34  110225.md        None   \n",
       "2  110225.md_4436e309-c4d1-4229-b46a-4e7abe088efa  110225.md        None   \n",
       "3  110225.md_448a5c94-3ec0-45ac-a94a-ebd92037a163  110225.md        None   \n",
       "4  110225.md_0b500597-bef4-43d7-9d6e-850c56a716b4  110225.md        None   \n",
       "5  110225.md_bef32d9b-a179-413b-800e-3bc4ca48b353  110225.md        None   \n",
       "6  110225.md_b0cd5a3b-4741-42eb-bdd9-d992678250a0  110225.md        None   \n",
       "7  110225.md_9fa6b460-24aa-420c-a091-6e118908d590  110225.md        None   \n",
       "8  110225.md_b6b0a0b3-3122-43b5-b68b-d3e8a66de499  110225.md        None   \n",
       "\n",
       "                                          embeddings is_unitary  \\\n",
       "0  [0.03818148747086525, 0.04162796959280968, 0.0...       None   \n",
       "1  [0.04164035990834236, 0.01807655394077301, -0....       None   \n",
       "2  [0.023161746561527252, 0.090914785861969, -0.0...       None   \n",
       "3  [0.02458001859486103, 0.0350651890039444, 0.00...       None   \n",
       "4  [0.0082578519359231, 0.023625241592526436, 0.0...       None   \n",
       "5  [-0.02488844469189644, -0.02288469299674034, -...       None   \n",
       "6  [-0.009872610680758953, 0.016194215044379234, ...       None   \n",
       "7  [-0.0019361716695129871, 0.008947310037910938,...       None   \n",
       "8  [0.06550847738981247, 0.0676582008600235, -0.0...       None   \n",
       "\n",
       "   embeddings_dimensions                             embedding_encoder_info  \n",
       "0                    384  client=SentenceTransformer(\\n  (0): Transforme...  \n",
       "1                    384  client=SentenceTransformer(\\n  (0): Transforme...  \n",
       "2                    384  client=SentenceTransformer(\\n  (0): Transforme...  \n",
       "3                    384  client=SentenceTransformer(\\n  (0): Transforme...  \n",
       "4                    384  client=SentenceTransformer(\\n  (0): Transforme...  \n",
       "5                    384  client=SentenceTransformer(\\n  (0): Transforme...  \n",
       "6                    384  client=SentenceTransformer(\\n  (0): Transforme...  \n",
       "7                    384  client=SentenceTransformer(\\n  (0): Transforme...  \n",
       "8                    384  client=SentenceTransformer(\\n  (0): Transforme...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70c7740",
   "metadata": {},
   "source": [
    "## Load To Grap\n",
    "\n",
    "Aqui es donde creamos el proceso de carga de datos al grafo y los diferentes patrones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17609952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "from neo4j import GraphDatabase\n",
    "from neo4j.exceptions import ClientError\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7baaf332",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DB Connection\n",
    "def graph_session() -> GraphDatabase:\n",
    "    '''\n",
    "    Creates and returns a connection session to the Neo4j database.\n",
    "\n",
    "    This function uses the environment variables NEO4J_URI and NEO4J_PASSWORD to authenticate\n",
    "    the connection. If either of these variables is not set, a ValueError is raised.\n",
    "\n",
    "    Returns:\n",
    "        GraphDatabase: A Neo4j database driver that allows performing operations\n",
    "        on the database.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If NEO4J_URI or NEO4J_PASSWORD are not set in the environment variables.\n",
    "        Exception: If an error occurs while trying to create the database session.\n",
    "    '''\n",
    "    URI = os.environ.get(\"NEO4J_URI\")\n",
    "    PASSWORD = os.environ.get(\"NEO4J_PASSWORD\")\n",
    "\n",
    "    if not URI or not PASSWORD:\n",
    "        raise ValueError(\"NEO4J_URI and NEO4J_PASSWORD must be set in environment variables.\")\n",
    "    AUTH = (\"neo4j\", PASSWORD)\n",
    "\n",
    "    try:\n",
    "        # Retornar el driver sin usar 'with' para que el llamador controle el ciclo de vida\n",
    "        driver = GraphDatabase.driver(URI, auth=AUTH)\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create a graph session: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58809158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<neo4j._sync.driver.Neo4jDriver at 0x16b51814f20>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver = graph_session()\n",
    "driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70280195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de índices avanzados, para la búsqueda por contenido y por vector.\n",
    "def setup_advanced_indexes(session):\n",
    "    \"\"\"Configuración de índices avanzados\"\"\"\n",
    "    try:\n",
    "        # Índice vectorial mejorado\n",
    "        vector_index_query = \"\"\"\n",
    "        CALL db.index.vector.createNodeIndex(\n",
    "            'chunk_embeddings',           // nombre del índice\n",
    "            'Chunk',                      // label del nodo\n",
    "            'embeddings',                 // propiedad que contiene el vector\n",
    "            384,                          // dimensiones del vector\n",
    "            'cosine'                      // similitud por coseno\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        # Índice de texto completo mejorado\n",
    "        fulltext_index_query = \"\"\"\n",
    "        CREATE FULLTEXT INDEX chunk_content IF NOT EXISTS\n",
    "        FOR (c:Chunk)\n",
    "        ON EACH [c.page_content]\n",
    "        OPTIONS {\n",
    "            indexConfig: {\n",
    "                `fulltext.analyzer`: 'spanish',\n",
    "                `fulltext.eventually_consistent`: false\n",
    "            }\n",
    "        }\n",
    "        \"\"\"\n",
    "        # Índice regular para búsquedas por chunk_id_consecutive\n",
    "        regular_index_query = \"\"\"\n",
    "        CREATE INDEX chunk_consecutive_idx IF NOT EXISTS\n",
    "        FOR (c:Chunk)\n",
    "        ON (c.chunk_id_consecutive)\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        try:\n",
    "            session.execute_write(lambda tx: tx.run(regular_index_query))\n",
    "            print(\"Regular index created successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Regular index creation message: {e}\")\n",
    "            \n",
    "        try:\n",
    "            session.execute_write(lambda tx: tx.run(vector_index_query))\n",
    "            print(\"Vector index created successfully\")\n",
    "        except Exception as e:\n",
    "            if \"An equivalent index already exists\" not in str(e):\n",
    "                raise e\n",
    "            print(\"Vector index already exists\")\n",
    "\n",
    "        try:\n",
    "            session.execute_write(lambda tx: tx.run(fulltext_index_query))\n",
    "            print(\"Full-text index created successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Full-text index creation message: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in index setup: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2b51dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 19:09:24,878 - INFO - Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE RANGE INDEX chunk_consecutive_idx IF NOT EXISTS FOR (e:Chunk) ON (e.chunk_id_consecutive)` has no effect.} {description: `RANGE INDEX chunk_consecutive_idx FOR (e:Chunk) ON (e.chunk_id_consecutive)` already exists.} {position: None} for query: '\\n        CREATE INDEX chunk_consecutive_idx IF NOT EXISTS\\n        FOR (c:Chunk)\\n        ON (c.chunk_id_consecutive)\\n        '\n",
      "2025-12-23 19:09:24,902 - INFO - Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE FULLTEXT INDEX chunk_content IF NOT EXISTS FOR (e:Chunk) ON EACH [e.page_content] OPTIONS {indexConfig: {`fulltext.analyzer`: \"spanish\", `fulltext.eventually_consistent`: false}}` has no effect.} {description: `FULLTEXT INDEX chunk_content FOR (e:Chunk) ON EACH [e.page_content]` already exists.} {position: None} for query: \"\\n        CREATE FULLTEXT INDEX chunk_content IF NOT EXISTS\\n        FOR (c:Chunk)\\n        ON EACH [c.page_content]\\n        OPTIONS {\\n            indexConfig: {\\n                `fulltext.analyzer`: 'spanish',\\n                `fulltext.eventually_consistent`: false\\n            }\\n        }\\n        \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular index created successfully\n",
      "Vector index already exists\n",
      "Full-text index created successfully\n"
     ]
    }
   ],
   "source": [
    "with driver.session() as session:\n",
    "    setup_advanced_indexes(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d9f3ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PROCESAMIENTO DE DOCUMENT DATA OBJECT A GRAFO.\n",
    "# Función para extraer estructura de documento\n",
    "def extract_document_structure(tx, \n",
    "                               filename, \n",
    "                               page_number, \n",
    "                               chunk_id, \n",
    "                               page_content, \n",
    "                               is_unitary, \n",
    "                               embeddings, \n",
    "                               embeddings_dimensions, \n",
    "                               embedding_encoder_info,\n",
    "                               chunk_id_consecutive):\n",
    "    \n",
    "    ''' \n",
    "    TODO:\n",
    "\n",
    "    CREAR EL FUNCIONAMIENTO DE DOD PARA QUE SIRVA CON LO QUE SE LEE EN EL DCUMENTO DE DOCLING.\n",
    "    \n",
    "    '''\n",
    "    try:\n",
    "        query = \"\"\"\n",
    "                MERGE (f:File {filename: $filename})\n",
    "                ON CREATE SET f.createdAt = timestamp()\n",
    "\n",
    "                MERGE (p:Page {filename: $filename, page_number: toInteger($page_number)})\n",
    "\n",
    "                MERGE (c:Chunk {chunk_id: $chunk_id})\n",
    "                ON CREATE SET c.page_content = $page_content,\n",
    "                              c.is_unitary = $is_unitary,\n",
    "                              c.embeddings = $embeddings, \n",
    "                              c.embeddings_dimensions = toInteger($embeddings_dimensions),\n",
    "                              c.embedding_encoder_info = $embedding_encoder_info,\n",
    "                              c.chunk_id_consecutive = toInteger($chunk_id_consecutive)\n",
    "\n",
    "                MERGE (f)-[:CONTAINS]->(p)\n",
    "                MERGE (p)-[:HAS_CHUNK]->(c)\n",
    "\n",
    "            \"\"\"\n",
    "        result = tx.run(query, \n",
    "                        filename=filename, \n",
    "                        page_number=page_number,\n",
    "                        chunk_id=chunk_id,\n",
    "                        page_content=page_content,\n",
    "                        is_unitary=is_unitary,\n",
    "                        embeddings=embeddings,\n",
    "                        embeddings_dimensions=embeddings_dimensions,\n",
    "                        embedding_encoder_info=embedding_encoder_info,\n",
    "                        chunk_id_consecutive=chunk_id_consecutive)\n",
    "        return result\n",
    "    except ClientError as e:\n",
    "        logger.error(\"Database error\", exc_info=True)\n",
    "        tx.rollback()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75213494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo las relaciones entre chunks consecutivos.\n",
    "def create_chunk_relationships(session):\n",
    "    \"\"\"Crear relaciones NEXT_CHUNK entre chunks consecutivos\"\"\"\n",
    "    join_chunks_query = \"\"\"\n",
    "    MATCH (c1:Chunk),(c2:Chunk)\n",
    "    WHERE c1.chunk_id_consecutive + 1 = c2.chunk_id_consecutive\n",
    "    MERGE (c1)-[:NEXT_CHUNK]->(c2)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        session.execute_write(lambda tx: tx.run(join_chunks_query))\n",
    "        print(\"Chunk relationships created successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating chunk relationships: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52e9d70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Valido que el DataFrame tenga la estructura correcta.\n",
    "def validate_dataframe(df, expected_dim=384):\n",
    "    \"\"\"Validar que el DataFrame tenga la estructura correcta\"\"\"\n",
    "    import ast\n",
    "    \n",
    "    required_columns = [\n",
    "        'filename', 'page_number', 'chunk_id', 'page_content',\n",
    "        'is_unitary', 'embeddings', 'embeddings_dimensions',\n",
    "        'embedding_encoder_info', 'chunk_id_consecutive'\n",
    "    ]\n",
    "    \n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "    \n",
    "    # Parsear embeddings si son strings\n",
    "    def parse_embedding(emb):\n",
    "        if isinstance(emb, str):\n",
    "            try:\n",
    "                return ast.literal_eval(emb)\n",
    "            except (ValueError, SyntaxError):\n",
    "                return []\n",
    "        return emb if emb is not None else []\n",
    "    \n",
    "    # Validar dimensiones de embeddings\n",
    "    # Aqui podemos poner las demas dimensiones de los modelo, y ligarlos a la columna de encoder_info.\n",
    "    parsed_embeddings = df['embeddings'].apply(parse_embedding)\n",
    "    if not all(len(emb) == expected_dim for emb in parsed_embeddings if emb):\n",
    "        # Mostrar qué embeddings tienen dimensiones incorrectas\n",
    "        wrong_dims = [(i, len(emb)) for i, emb in enumerate(parsed_embeddings) if emb and len(emb) != expected_dim]\n",
    "        raise ValueError(f\"All embeddings must have {expected_dim} dimensions. Found: {wrong_dims[:5]}\")\n",
    "    \n",
    "    # Validar que chunk_id_consecutive sea secuencial\n",
    "    expected_range = range(1, len(df) + 1)\n",
    "    if not all(df['chunk_id_consecutive'].values == list(expected_range)):\n",
    "        raise ValueError(\"chunk_id_consecutive must be sequential starting from 1\")\n",
    "    \n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2ee005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "031c9c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tratamiento de columnas para añadir secuencialidad y tipo de dato\n",
    "def colummn_pretreatment(df):\n",
    "    # Se les da el formato necesario, esto lo peudo llevar al momento en que se escribe la data en el modulo de ingestión\n",
    "    df[\"embeddings\"] = df[\"embeddings\"].apply(ast.literal_eval)\n",
    "    df['chunk_id_consecutive'] = range(1, len(df) + 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5130b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para centralizar  el proceso de ingestión de datos al grafo.\n",
    "def process_with_neo4j(df, batch_size=100, target_database=\"neo4j\"):\n",
    "    ''' \n",
    "    Función que busca:\n",
    "    1. Configurar los índices en la base de datos.\n",
    "    2. Validar la idoneidad del dataframe\n",
    "    3. Procesar en lotes los chunks del texto.\n",
    "    3.1 Cada lote procesarlo con el query que facilita extraer la extructura del texto.\n",
    "    4. Una vez creado, populamos con relaciones consecutivas.\n",
    "    \n",
    "    '''\n",
    "    import pandas as pd\n",
    "    \n",
    "    driver = graph_session()  # No usar 'with' aquí, solo obtener el driver\n",
    "    \n",
    "    try:\n",
    "        with driver.session(database=target_database) as session:\n",
    "            # Configurar índices\n",
    "            setup_advanced_indexes(session)\n",
    "            \n",
    "            # Validar datos\n",
    "            if validate_dataframe(df):\n",
    "                # Procesar chunks en lotes\n",
    "                for i in range(0, len(df), batch_size):\n",
    "                    batch = df.iloc[i:i + batch_size]\n",
    "                    print(f\"Processing batch {i//batch_size + 1} of {(len(df)-1)//batch_size + 1}\")\n",
    "                    \n",
    "                    batch.apply(\n",
    "                        lambda row: session.execute_write(\n",
    "                            extract_document_structure,\n",
    "                            row['filename'],\n",
    "                            row['page_number'] if pd.notna(row['page_number']) else 1,\n",
    "                            row['chunk_id'],\n",
    "                            row['page_content'],\n",
    "                            row['is_unitary'] if pd.notna(row['is_unitary']) else False,\n",
    "                            row['embeddings'],\n",
    "                            row['embeddings_dimensions'] if pd.notna(row['embeddings_dimensions']) else 384,\n",
    "                            row['embedding_encoder_info'] if pd.notna(row['embedding_encoder_info']) else 'unknown',\n",
    "                            row['chunk_id_consecutive']\n",
    "                        ),\n",
    "                        axis=1\n",
    "                    )\n",
    "                \n",
    "                # Crear relaciones entre chunks consecutivos\n",
    "                create_chunk_relationships(session)\n",
    "            else:\n",
    "                print(\"Data validation failed. Exiting.\")\n",
    "    finally:\n",
    "        driver.close()  # Cerrar el driver al final\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62f0eaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame preparado: 9 filas\n",
      "Embeddings parseados: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jefai\\AppData\\Local\\Temp\\ipykernel_29032\\1534677213.py:16: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['page_number'] = df['page_number'].fillna(1)\n",
      "C:\\Users\\jefai\\AppData\\Local\\Temp\\ipykernel_29032\\1534677213.py:17: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['is_unitary'] = df['is_unitary'].fillna(False)\n"
     ]
    }
   ],
   "source": [
    "# 3. Preparar datos faltantes antes de procesar\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "# Parsear embeddings de string a lista si es necesario\n",
    "if df['embeddings'].dtype == 'object':\n",
    "    df['embeddings'] = df['embeddings'].apply(\n",
    "        lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith('[') else x\n",
    "    )\n",
    "\n",
    "# Asegurar que chunk_id_consecutive esté correctamente numerado\n",
    "if 'chunk_id_consecutive' not in df.columns or df['chunk_id_consecutive'].isna().any():\n",
    "    df['chunk_id_consecutive'] = range(1, len(df) + 1)\n",
    "\n",
    "# Llenar valores faltantes\n",
    "df['page_number'] = df['page_number'].fillna(1)\n",
    "df['is_unitary'] = df['is_unitary'].fillna(False)\n",
    "df['embeddings_dimensions'] = df['embeddings_dimensions'].fillna(384)\n",
    "df['embedding_encoder_info'] = df['embedding_encoder_info'].fillna('unknown')\n",
    "\n",
    "print(f\"DataFrame preparado: {len(df)} filas\")\n",
    "print(f\"Embeddings parseados: {isinstance(df['embeddings'].iloc[0], list) if len(df) > 0 else 'N/A'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "29b7fa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 19:09:25,017 - INFO - Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE RANGE INDEX chunk_consecutive_idx IF NOT EXISTS FOR (e:Chunk) ON (e.chunk_id_consecutive)` has no effect.} {description: `RANGE INDEX chunk_consecutive_idx FOR (e:Chunk) ON (e.chunk_id_consecutive)` already exists.} {position: None} for query: '\\n        CREATE INDEX chunk_consecutive_idx IF NOT EXISTS\\n        FOR (c:Chunk)\\n        ON (c.chunk_id_consecutive)\\n        '\n",
      "2025-12-23 19:09:25,046 - INFO - Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE FULLTEXT INDEX chunk_content IF NOT EXISTS FOR (e:Chunk) ON EACH [e.page_content] OPTIONS {indexConfig: {`fulltext.analyzer`: \"spanish\", `fulltext.eventually_consistent`: false}}` has no effect.} {description: `FULLTEXT INDEX chunk_content FOR (e:Chunk) ON EACH [e.page_content]` already exists.} {position: None} for query: \"\\n        CREATE FULLTEXT INDEX chunk_content IF NOT EXISTS\\n        FOR (c:Chunk)\\n        ON EACH [c.page_content]\\n        OPTIONS {\\n            indexConfig: {\\n                `fulltext.analyzer`: 'spanish',\\n                `fulltext.eventually_consistent`: false\\n            }\\n        }\\n        \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular index created successfully\n",
      "Vector index already exists\n",
      "Full-text index created successfully\n",
      "Processing batch 1 of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 19:09:29,706 - INFO - Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Statement.CartesianProduct} {category: PERFORMANCE} {title: This query builds a cartesian product between disconnected patterns.} {description: If a part of a query contains multiple disconnected patterns, this will build a cartesian product between all those parts. This may produce a large amount of data and slow down query processing. While occasionally intended, it may often be possible to reformulate the query that avoids the use of this cross product, perhaps by adding a relationship between the different parts or by using OPTIONAL MATCH (identifier is: (c2))} {position: line: 2, column: 5, offset: 5} for query: '\\n    MATCH (c1:Chunk),(c2:Chunk)\\n    WHERE c1.chunk_id_consecutive + 1 = c2.chunk_id_consecutive\\n    MERGE (c1)-[:NEXT_CHUNK]->(c2)\\n    '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk relationships created successfully\n"
     ]
    }
   ],
   "source": [
    "# Procesar el DataFrame en Neo4j\n",
    "# Nota: process_with_neo4j() maneja la conexión internamente\n",
    "process_with_neo4j(\n",
    "    df=df,\n",
    "    batch_size=100,  # Procesar en lotes de 100 chunks\n",
    "    target_database=\"neo4j\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d5774a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
