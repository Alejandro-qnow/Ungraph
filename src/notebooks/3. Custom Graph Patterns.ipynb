{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Custom Graph Patterns - Ungraph\n",
        "\n",
        "Este notebook demuestra c√≥mo crear y usar patrones personalizados de grafo desde la ingesta de datos hasta la exploraci√≥n del grafo.\n",
        "\n",
        "## Objetivos\n",
        "\n",
        "1. **Crear patrones personalizados** de estructura de grafo ‚úÖ\n",
        "2. **Ingerir datos** usando patrones personalizados ‚úÖ **IMPLEMENTADO**\n",
        "3. **Explorar el grafo** creado con diferentes estructuras\n",
        "4. **Comparar patrones** y sus ventajas/desventajas\n",
        "\n",
        "## Flujo\n",
        "\n",
        "1. Definir patrones personalizados ‚úÖ\n",
        "2. Validar patrones ‚úÖ\n",
        "3. Generar queries Cypher ‚úÖ\n",
        "4. Ingerir datos con patrones personalizados ‚úÖ **DISPONIBLE AHORA**\n",
        "5. Explorar el grafo creado\n",
        "\n",
        "**Nota:** La funcionalidad de ingesta con patrones personalizados est√° **implementada y funcional** en `ungraph.ingest_document(pattern=...)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path Folder parent added: D:\\projects\\Ungraph\n",
            "Path Folder src added: D:\\projects\\Ungraph\\src\n",
            "Path Folder src/utils added: D:\\projects\\Ungraph\\src\\utils\n",
            "Path Folder src/data added: D:\\projects\\Ungraph\\src\\data\n"
          ]
        }
      ],
      "source": [
        "def add_src_to_path(path_folder: str):\n",
        "    ''' \n",
        "    Helper function for adding the \"path_folder\" directory to the path.\n",
        "    in order to work on notebooks and scripts\n",
        "    '''\n",
        "    import sys\n",
        "    from pathlib import Path\n",
        "\n",
        "    base_path = Path().resolve()\n",
        "    for parent in [base_path] + list(base_path.parents):\n",
        "        candidate = parent / path_folder\n",
        "        if candidate.exists():\n",
        "            # Agregar el directorio padre para que sea un paquete Python\n",
        "            parent_dir = candidate.parent\n",
        "            if str(parent_dir) not in sys.path:\n",
        "                sys.path.insert(0, str(parent_dir))\n",
        "                print(f\"Path Folder parent added: {parent_dir}\")\n",
        "            if str(candidate) not in sys.path:\n",
        "                sys.path.append(str(candidate))\n",
        "                print(f\"Path Folder {path_folder} added: {candidate}\")\n",
        "            return\n",
        "    print(f\"Not found '{path_folder}' folder on the hierarchy of directories\")\n",
        "\n",
        "# Agregar carpetas necesarias al path\n",
        "add_src_to_path(path_folder=\"src\")\n",
        "add_src_to_path(path_folder=\"src/utils\")\n",
        "add_src_to_path(path_folder=\"src/data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\projects\\Ungraph\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Ungraph importado desde src/ (modo desarrollo)\n",
            "üì¶ Ungraph version: 0.1.0\n"
          ]
        }
      ],
      "source": [
        "# Importar librer√≠as necesarias\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "# Importar handlers desde utils\n",
        "from src.utils.handlers import find_in_project\n",
        "\n",
        "# Importar ungraph\n",
        "try:\n",
        "    import ungraph\n",
        "    print(\"‚úÖ Ungraph importado como paquete instalado\")\n",
        "except ImportError:\n",
        "    import src\n",
        "    ungraph = src\n",
        "    print(\"‚úÖ Ungraph importado desde src/ (modo desarrollo)\")\n",
        "\n",
        "# Importar Value Objects y servicios\n",
        "from domain.value_objects.graph_pattern import (\n",
        "    GraphPattern,\n",
        "    NodeDefinition,\n",
        "    RelationshipDefinition\n",
        ")\n",
        "from domain.value_objects.predefined_patterns import FILE_PAGE_CHUNK_PATTERN\n",
        "from infrastructure.services.neo4j_pattern_service import Neo4jPatternService\n",
        "\n",
        "print(f\"üì¶ Ungraph version: {ungraph.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 1: Patr√≥n Simple - Solo Chunks\n",
        "\n",
        "Creamos un patr√≥n minimalista que solo almacena chunks sin estructura File-Page.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìù Creando patr√≥n: SIMPLE_CHUNK\n",
            "============================================================\n",
            "‚úÖ Patr√≥n creado:\n",
            "   Nombre: SIMPLE_CHUNK\n",
            "   Descripci√≥n: Solo chunks, sin estructura File-Page. √ötil para documentos simples.\n",
            "   Nodos: ['Chunk']\n",
            "   Relaciones: 0\n"
          ]
        }
      ],
      "source": [
        "# Patr√≥n 1: Solo Chunks (sin File/Page)\n",
        "print(\"üìù Creando patr√≥n: SIMPLE_CHUNK\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "simple_chunk_node = NodeDefinition(\n",
        "    label=\"Chunk\",\n",
        "    required_properties={\n",
        "        \"chunk_id\": str,\n",
        "        \"content\": str,\n",
        "        \"embeddings\": list,\n",
        "        \"embeddings_dimensions\": int\n",
        "    },\n",
        "    optional_properties={\n",
        "        \"chunk_id_consecutive\": int,\n",
        "        \"source_file\": str\n",
        "    },\n",
        "    indexes=[\"chunk_id\", \"chunk_id_consecutive\"]\n",
        ")\n",
        "\n",
        "SIMPLE_CHUNK_PATTERN = GraphPattern(\n",
        "    name=\"SIMPLE_CHUNK\",\n",
        "    description=\"Solo chunks, sin estructura File-Page. √ötil para documentos simples.\",\n",
        "    node_definitions=[simple_chunk_node],\n",
        "    relationship_definitions=[],\n",
        "    search_patterns=[\"basic\", \"hybrid\"]\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Patr√≥n creado:\")\n",
        "print(f\"   Nombre: {SIMPLE_CHUNK_PATTERN.name}\")\n",
        "print(f\"   Descripci√≥n: {SIMPLE_CHUNK_PATTERN.description}\")\n",
        "print(f\"   Nodos: {[n.label for n in SIMPLE_CHUNK_PATTERN.node_definitions]}\")\n",
        "print(f\"   Relaciones: {len(SIMPLE_CHUNK_PATTERN.relationship_definitions)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Validaci√≥n: V√ÅLIDO\n",
            "\n",
            "üìù Query Cypher generado:\n",
            "MERGE (n0:Chunk {chunk_id: $chunk_id, content: $content, embeddings: $embeddings, embeddings_dimensions: $embeddings_dimensions})\n",
            "ON CREATE SET n0.chunk_id_consecutive = $chunk_id_consecutive, n0.source_file = $source_file\n"
          ]
        }
      ],
      "source": [
        "# Validar y generar query Cypher para SIMPLE_CHUNK\n",
        "service = Neo4jPatternService()\n",
        "\n",
        "# Validar\n",
        "is_valid = service.validate_pattern(SIMPLE_CHUNK_PATTERN)\n",
        "print(f\"‚úÖ Validaci√≥n: {'V√ÅLIDO' if is_valid else 'INV√ÅLIDO'}\")\n",
        "\n",
        "# Generar query Cypher\n",
        "cypher_query = service.generate_cypher(SIMPLE_CHUNK_PATTERN, \"create\")\n",
        "print(f\"\\nüìù Query Cypher generado:\")\n",
        "print(cypher_query)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 2: Patr√≥n con Relaciones Secuenciales\n",
        "\n",
        "Creamos un patr√≥n que conecta chunks consecutivos directamente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìù Creando patr√≥n: SEQUENTIAL_CHUNKS\n",
            "============================================================\n",
            "‚úÖ Patr√≥n creado:\n",
            "   Nombre: SEQUENTIAL_CHUNKS\n",
            "   Relaciones: ['NEXT_CHUNK']\n",
            "\n",
            "‚úÖ Validaci√≥n: V√ÅLIDO\n",
            "\n",
            "üìù Query Cypher generado:\n",
            "MERGE (n0:Chunk {chunk_id: $chunk_id, content: $content, embeddings: $embeddings, embeddings_dimensions: $embeddings_dimensions})\n",
            "ON CREATE SET n0.chunk_id_consecutive = $chunk_id_consecutive, n0.source_file = $source_file\n",
            "MERGE (n0)-[:NEXT_CHUNK]->(n0)\n"
          ]
        }
      ],
      "source": [
        "# Patr√≥n 2: Chunks con relaciones secuenciales\n",
        "print(\"üìù Creando patr√≥n: SEQUENTIAL_CHUNKS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "chunk_node = NodeDefinition(\n",
        "    label=\"Chunk\",\n",
        "    required_properties={\n",
        "        \"chunk_id\": str,\n",
        "        \"content\": str,\n",
        "        \"embeddings\": list,\n",
        "        \"embeddings_dimensions\": int\n",
        "    },\n",
        "    optional_properties={\n",
        "        \"chunk_id_consecutive\": int,\n",
        "        \"source_file\": str\n",
        "    },\n",
        "    indexes=[\"chunk_id\", \"chunk_id_consecutive\"]\n",
        ")\n",
        "\n",
        "# Relaci√≥n: Chunk siguiente\n",
        "next_chunk_rel = RelationshipDefinition(\n",
        "    from_node=\"Chunk\",\n",
        "    to_node=\"Chunk\",\n",
        "    relationship_type=\"NEXT_CHUNK\",\n",
        "    direction=\"OUTGOING\"\n",
        ")\n",
        "\n",
        "SEQUENTIAL_CHUNKS_PATTERN = GraphPattern(\n",
        "    name=\"SEQUENTIAL_CHUNKS\",\n",
        "    description=\"Chunks con relaciones NEXT_CHUNK entre consecutivos. √ötil para mantener orden secuencial.\",\n",
        "    node_definitions=[chunk_node],\n",
        "    relationship_definitions=[next_chunk_rel],\n",
        "    search_patterns=[\"basic\", \"hybrid\"]\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Patr√≥n creado:\")\n",
        "print(f\"   Nombre: {SEQUENTIAL_CHUNKS_PATTERN.name}\")\n",
        "print(f\"   Relaciones: {[r.relationship_type for r in SEQUENTIAL_CHUNKS_PATTERN.relationship_definitions]}\")\n",
        "\n",
        "# Validar y generar query\n",
        "is_valid = service.validate_pattern(SEQUENTIAL_CHUNKS_PATTERN)\n",
        "print(f\"\\n‚úÖ Validaci√≥n: {'V√ÅLIDO' if is_valid else 'INV√ÅLIDO'}\")\n",
        "\n",
        "cypher_query = service.generate_cypher(SEQUENTIAL_CHUNKS_PATTERN, \"create\")\n",
        "print(f\"\\nüìù Query Cypher generado:\")\n",
        "print(cypher_query)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 3: Patr√≥n L√©xico - Entidades y Chunks\n",
        "\n",
        "Creamos un patr√≥n para grafo l√©xico con entidades extra√≠das y sus relaciones con chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìù Creando patr√≥n: LEXICAL_GRAPH\n",
            "============================================================\n",
            "‚úÖ Patr√≥n creado:\n",
            "   Nombre: LEXICAL_GRAPH\n",
            "   Nodos: ['Entity', 'Chunk']\n",
            "   Relaciones: ['MENTIONS', 'RELATED_TO']\n",
            "\n",
            "‚úÖ Validaci√≥n: V√ÅLIDO\n",
            "\n",
            "üìù Query Cypher generado:\n",
            "MERGE (n0:Entity {name: $name, type: $type})\n",
            "ON CREATE SET n0.description = $description, n0.frequency = $frequency\n",
            "MERGE (n1:Chunk {chunk_id: $chunk_id, content: $content, embeddings: $embeddings, embeddings_dimensions: $embeddings_dimensions})\n",
            "MERGE (n1)-[:MENTIONS {count: $count}]->(n0)\n",
            "MERGE (n0)-[:RELATED_TO {strength: $strength}]->(n0)\n"
          ]
        }
      ],
      "source": [
        "# Patr√≥n 3: Grafo L√©xico con Entidades\n",
        "print(\"üìù Creando patr√≥n: LEXICAL_GRAPH\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Nodo Entity\n",
        "entity_node = NodeDefinition(\n",
        "    label=\"Entity\",\n",
        "    required_properties={\n",
        "        \"name\": str,\n",
        "        \"type\": str  # \"PERSON\", \"ORGANIZATION\", \"CONCEPT\", etc.\n",
        "    },\n",
        "    optional_properties={\n",
        "        \"description\": str,\n",
        "        \"frequency\": int\n",
        "    },\n",
        "    indexes=[\"name\", \"type\"]\n",
        ")\n",
        "\n",
        "# Nodo Chunk\n",
        "chunk_node = NodeDefinition(\n",
        "    label=\"Chunk\",\n",
        "    required_properties={\n",
        "        \"chunk_id\": str,\n",
        "        \"content\": str,\n",
        "        \"embeddings\": list,\n",
        "        \"embeddings_dimensions\": int\n",
        "    },\n",
        "    indexes=[\"chunk_id\"]\n",
        ")\n",
        "\n",
        "# Relaci√≥n: Chunk menciona Entity\n",
        "mentions_rel = RelationshipDefinition(\n",
        "    from_node=\"Chunk\",\n",
        "    to_node=\"Entity\",\n",
        "    relationship_type=\"MENTIONS\",\n",
        "    properties={\"count\": int},  # N√∫mero de veces que se menciona\n",
        "    direction=\"OUTGOING\"\n",
        ")\n",
        "\n",
        "# Relaci√≥n: Entity relacionada con otra Entity\n",
        "related_rel = RelationshipDefinition(\n",
        "    from_node=\"Entity\",\n",
        "    to_node=\"Entity\",\n",
        "    relationship_type=\"RELATED_TO\",\n",
        "    properties={\"strength\": float},  # Fuerza de la relaci√≥n\n",
        "    direction=\"OUTGOING\"\n",
        ")\n",
        "\n",
        "LEXICAL_GRAPH_PATTERN = GraphPattern(\n",
        "    name=\"LEXICAL_GRAPH\",\n",
        "    description=\"Grafo l√©xico con entidades extra√≠das y sus relaciones. √ötil para an√°lisis sem√°ntico.\",\n",
        "    node_definitions=[entity_node, chunk_node],\n",
        "    relationship_definitions=[mentions_rel, related_rel],\n",
        "    search_patterns=[\"basic\", \"hybrid\", \"pattern_matching\"]\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Patr√≥n creado:\")\n",
        "print(f\"   Nombre: {LEXICAL_GRAPH_PATTERN.name}\")\n",
        "print(f\"   Nodos: {[n.label for n in LEXICAL_GRAPH_PATTERN.node_definitions]}\")\n",
        "print(f\"   Relaciones: {[r.relationship_type for r in LEXICAL_GRAPH_PATTERN.relationship_definitions]}\")\n",
        "\n",
        "# Validar\n",
        "is_valid = service.validate_pattern(LEXICAL_GRAPH_PATTERN)\n",
        "print(f\"\\n‚úÖ Validaci√≥n: {'V√ÅLIDO' if is_valid else 'INV√ÅLIDO'}\")\n",
        "\n",
        "# Generar query\n",
        "cypher_query = service.generate_cypher(LEXICAL_GRAPH_PATTERN, \"create\")\n",
        "print(f\"\\nüìù Query Cypher generado:\")\n",
        "print(cypher_query)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìù Creando patr√≥n: DOCUMENT_SECTION_PARAGRAPH\n",
            "============================================================\n",
            "‚úÖ Patr√≥n creado:\n",
            "   Nombre: DOCUMENT_SECTION_PARAGRAPH\n",
            "   Nodos: ['Document', 'Section', 'Paragraph']\n",
            "   Relaciones: ['HAS_SECTION', 'HAS_PARAGRAPH', 'NEXT_PARAGRAPH']\n",
            "\n",
            "‚úÖ Validaci√≥n: V√ÅLIDO\n",
            "\n",
            "üìù Query Cypher generado:\n",
            "MERGE (n0:Document {doc_id: $doc_id, title: $title})\n",
            "ON CREATE SET n0.created_at = $created_at, n0.author = $author\n",
            "MERGE (n1:Section {section_id: $section_id, title: $title, order: $order})\n",
            "ON CREATE SET n1.summary = $summary\n",
            "MERGE (n2:Paragraph {para_id: $para_id, content: $content, embeddings: $embeddings, embeddings_dimensions: $embeddings_dimensions})\n",
            "ON CREATE SET n2.order = $order\n",
            "MERGE (n0)-[:HAS_SECTION]->(n1)\n",
            "MERGE (n1)-[:HAS_PARAGRAPH]->(n2)\n",
            "MERGE (n2)-[:NEXT_PARAGRAPH]->(n2)\n"
          ]
        }
      ],
      "source": [
        "# Patr√≥n 4: Documento ‚Üí Secci√≥n ‚Üí P√°rrafo\n",
        "print(\"üìù Creando patr√≥n: DOCUMENT_SECTION_PARAGRAPH\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "document_node = NodeDefinition(\n",
        "    label=\"Document\",\n",
        "    required_properties={\"doc_id\": str, \"title\": str},\n",
        "    optional_properties={\"created_at\": int, \"author\": str},\n",
        "    indexes=[\"doc_id\"]\n",
        ")\n",
        "\n",
        "section_node = NodeDefinition(\n",
        "    label=\"Section\",\n",
        "    required_properties={\"section_id\": str, \"title\": str, \"order\": int},\n",
        "    optional_properties={\"summary\": str},\n",
        "    indexes=[\"section_id\"]\n",
        ")\n",
        "\n",
        "paragraph_node = NodeDefinition(\n",
        "    label=\"Paragraph\",\n",
        "    required_properties={\n",
        "        \"para_id\": str,\n",
        "        \"content\": str,\n",
        "        \"embeddings\": list,\n",
        "        \"embeddings_dimensions\": int\n",
        "    },\n",
        "    optional_properties={\"order\": int},\n",
        "    indexes=[\"para_id\"]\n",
        ")\n",
        "\n",
        "# Relaciones\n",
        "has_section = RelationshipDefinition(\n",
        "    from_node=\"Document\",\n",
        "    to_node=\"Section\",\n",
        "    relationship_type=\"HAS_SECTION\",\n",
        "    direction=\"OUTGOING\"\n",
        ")\n",
        "\n",
        "has_paragraph = RelationshipDefinition(\n",
        "    from_node=\"Section\",\n",
        "    to_node=\"Paragraph\",\n",
        "    relationship_type=\"HAS_PARAGRAPH\",\n",
        "    direction=\"OUTGOING\"\n",
        ")\n",
        "\n",
        "next_paragraph = RelationshipDefinition(\n",
        "    from_node=\"Paragraph\",\n",
        "    to_node=\"Paragraph\",\n",
        "    relationship_type=\"NEXT_PARAGRAPH\",\n",
        "    direction=\"OUTGOING\"\n",
        ")\n",
        "\n",
        "DOCUMENT_SECTION_PARAGRAPH_PATTERN = GraphPattern(\n",
        "    name=\"DOCUMENT_SECTION_PARAGRAPH\",\n",
        "    description=\"Documento contiene secciones, secciones contienen p√°rrafos. Alternativa jer√°rquica a FILE_PAGE_CHUNK.\",\n",
        "    node_definitions=[document_node, section_node, paragraph_node],\n",
        "    relationship_definitions=[has_section, has_paragraph, next_paragraph],\n",
        "    search_patterns=[\"basic\", \"hybrid\", \"parent_child\"]\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Patr√≥n creado:\")\n",
        "print(f\"   Nombre: {DOCUMENT_SECTION_PARAGRAPH_PATTERN.name}\")\n",
        "print(f\"   Nodos: {[n.label for n in DOCUMENT_SECTION_PARAGRAPH_PATTERN.node_definitions]}\")\n",
        "print(f\"   Relaciones: {[r.relationship_type for r in DOCUMENT_SECTION_PARAGRAPH_PATTERN.relationship_definitions]}\")\n",
        "\n",
        "# Validar\n",
        "is_valid = service.validate_pattern(DOCUMENT_SECTION_PARAGRAPH_PATTERN)\n",
        "print(f\"\\n‚úÖ Validaci√≥n: {'V√ÅLIDO' if is_valid else 'INV√ÅLIDO'}\")\n",
        "\n",
        "# Generar query\n",
        "cypher_query = service.generate_cypher(DOCUMENT_SECTION_PARAGRAPH_PATTERN, \"create\")\n",
        "print(f\"\\nüìù Query Cypher generado:\")\n",
        "print(cypher_query[:500] + \"...\" if len(cypher_query) > 500 else cypher_query)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 5: Comparar Patrones\n",
        "\n",
        "Comparamos los diferentes patrones creados y sus caracter√≠sticas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä COMPARACI√ìN DE PATRONES\n",
            "================================================================================\n",
            "\n",
            "FILE_PAGE_CHUNK:\n",
            "  Nodos: 3\n",
            "  Relaciones: 3\n",
            "  Complejidad: Alta\n",
            "  Uso: Patr√≥n b√°sico: File contiene Pages, Pages contiene...\n",
            "\n",
            "SIMPLE_CHUNK:\n",
            "  Nodos: 1\n",
            "  Relaciones: 0\n",
            "  Complejidad: Baja\n",
            "  Uso: Solo chunks, sin estructura File-Page. √ötil para d...\n",
            "\n",
            "SEQUENTIAL_CHUNKS:\n",
            "  Nodos: 1\n",
            "  Relaciones: 1\n",
            "  Complejidad: Baja\n",
            "  Uso: Chunks con relaciones NEXT_CHUNK entre consecutivo...\n",
            "\n",
            "LEXICAL_GRAPH:\n",
            "  Nodos: 2\n",
            "  Relaciones: 2\n",
            "  Complejidad: Media\n",
            "  Uso: Grafo l√©xico con entidades extra√≠das y sus relacio...\n",
            "\n",
            "DOCUMENT_SECTION_PARAGRAPH:\n",
            "  Nodos: 3\n",
            "  Relaciones: 3\n",
            "  Complejidad: Alta\n",
            "  Uso: Documento contiene secciones, secciones contienen ...\n"
          ]
        }
      ],
      "source": [
        "# Comparar todos los patrones\n",
        "print(\"üìä COMPARACI√ìN DE PATRONES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "patterns = {\n",
        "    \"FILE_PAGE_CHUNK\": FILE_PAGE_CHUNK_PATTERN,\n",
        "    \"SIMPLE_CHUNK\": SIMPLE_CHUNK_PATTERN,\n",
        "    \"SEQUENTIAL_CHUNKS\": SEQUENTIAL_CHUNKS_PATTERN,\n",
        "    \"LEXICAL_GRAPH\": LEXICAL_GRAPH_PATTERN,\n",
        "    \"DOCUMENT_SECTION_PARAGRAPH\": DOCUMENT_SECTION_PARAGRAPH_PATTERN\n",
        "}\n",
        "\n",
        "comparison_data = []\n",
        "for name, pattern in patterns.items():\n",
        "    comparison_data.append({\n",
        "        \"Nombre\": name,\n",
        "        \"Nodos\": len(pattern.node_definitions),\n",
        "        \"Relaciones\": len(pattern.relationship_definitions),\n",
        "        \"Complejidad\": \"Alta\" if len(pattern.node_definitions) > 2 else \"Media\" if len(pattern.node_definitions) == 2 else \"Baja\",\n",
        "        \"Uso\": pattern.description[:50] + \"...\"\n",
        "    })\n",
        "\n",
        "# Mostrar comparaci√≥n\n",
        "for data in comparison_data:\n",
        "    print(f\"\\n{data['Nombre']}:\")\n",
        "    print(f\"  Nodos: {data['Nodos']}\")\n",
        "    print(f\"  Relaciones: {data['Relaciones']}\")\n",
        "    print(f\"  Complejidad: {data['Complejidad']}\")\n",
        "    print(f\"  Uso: {data['Uso']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 6: Explorar Queries Cypher Generados\n",
        "\n",
        "Examinamos los queries Cypher generados para cada patr√≥n.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìù QUERIES CYPHER GENERADOS\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "Patr√≥n: FILE_PAGE_CHUNK\n",
            "================================================================================\n",
            "MERGE (n0:File {filename: $filename})\n",
            "ON CREATE SET n0.createdAt = $createdAt\n",
            "MERGE (n1:Page {filename: $filename, page_number: $page_number})\n",
            "MERGE (n2:Chunk {chunk_id: $chunk_id, page_content: $page_content, embeddings: $embeddings, embeddings_dimensions: $embeddings_dimensions})\n",
            "ON CREATE SET n2.is_unitary = $is_unitary, n2.chunk_id_consecutive = $chunk_id_consecutive, n2.embedding_encoder_info = $embedding_encoder_info\n",
            "MERGE (n0)-[:CONTAINS]->(n1)\n",
            "MERGE (n1)-[:HAS_CHUNK]->(n2)\n",
            "MERGE (n2)-[:NEXT_CHUNK]->(n2)\n",
            "\n",
            "‚úÖ Query generado exitosamente (516 caracteres)\n",
            "\n",
            "================================================================================\n",
            "Patr√≥n: SIMPLE_CHUNK\n",
            "================================================================================\n",
            "MERGE (n0:Chunk {chunk_id: $chunk_id, content: $content, embeddings: $embeddings, embeddings_dimensions: $embeddings_dimensions})\n",
            "ON CREATE SET n0.chunk_id_consecutive = $chunk_id_consecutive, n0.source_file = $source_file\n",
            "\n",
            "‚úÖ Query generado exitosamente (222 caracteres)\n",
            "\n",
            "================================================================================\n",
            "Patr√≥n: SEQUENTIAL_CHUNKS\n",
            "================================================================================\n",
            "MERGE (n0:Chunk {chunk_id: $chunk_id, content: $content, embeddings: $embeddings, embeddings_dimensions: $embeddings_dimensions})\n",
            "ON CREATE SET n0.chunk_id_consecutive = $chunk_id_consecutive, n0.source_file = $source_file\n",
            "MERGE (n0)-[:NEXT_CHUNK]->(n0)\n",
            "\n",
            "‚úÖ Query generado exitosamente (253 caracteres)\n",
            "\n",
            "================================================================================\n",
            "Patr√≥n: LEXICAL_GRAPH\n",
            "================================================================================\n",
            "MERGE (n0:Entity {name: $name, type: $type})\n",
            "ON CREATE SET n0.description = $description, n0.frequency = $frequency\n",
            "MERGE (n1:Chunk {chunk_id: $chunk_id, content: $content, embeddings: $embeddings, embeddings_dimensions: $embeddings_dimensions})\n",
            "MERGE (n1)-[:MENTIONS {count: $count}]->(n0)\n",
            "MERGE (n0)-[:RELATED_TO {strength: $strength}]->(n0)\n",
            "\n",
            "‚úÖ Query generado exitosamente (343 caracteres)\n",
            "\n",
            "================================================================================\n",
            "Patr√≥n: DOCUMENT_SECTION_PARAGRAPH\n",
            "================================================================================\n",
            "MERGE (n0:Document {doc_id: $doc_id, title: $title})\n",
            "ON CREATE SET n0.created_at = $created_at, n0.author = $author\n",
            "MERGE (n1:Section {section_id: $section_id, title: $title, order: $order})\n",
            "ON CREATE SET n1.summary = $summary\n",
            "MERGE (n2:Paragraph {para_id: $para_id, content: $content, embeddings: $embeddings, embeddings_dimensions: $embeddings_dimensions})\n",
            "ON CREATE SET n2.order = $order\n",
            "MERGE (n0)-[:HAS_SECTION]->(n1)\n",
            "MERGE (n1)-[:HAS_PARAGRAPH]->(n2)\n",
            "MERGE (n2)-[:NEXT_PARAGRAPH]->(n2)\n",
            "\n",
            "‚úÖ Query generado exitosamente (491 caracteres)\n"
          ]
        }
      ],
      "source": [
        "# Generar y comparar queries Cypher para cada patr√≥n\n",
        "print(\"üìù QUERIES CYPHER GENERADOS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for name, pattern in patterns.items():\n",
        "    print(f\"\\n{'=' * 80}\")\n",
        "    print(f\"Patr√≥n: {name}\")\n",
        "    print(f\"{'=' * 80}\")\n",
        "    \n",
        "    try:\n",
        "        cypher = service.generate_cypher(pattern, \"create\")\n",
        "        print(cypher)\n",
        "        print(f\"\\n‚úÖ Query generado exitosamente ({len(cypher)} caracteres)\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error al generar query: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 7: Uso con Datos Reales ‚úÖ IMPLEMENTADO\n",
        "\n",
        "**Nota:** La integraci√≥n de patrones personalizados con `ingest_document()` est√° **implementada y funcional** (Fase 2 completa).\n",
        "\n",
        "Ahora puedes:\n",
        "1. Crear y validar patrones ‚úÖ\n",
        "2. Generar queries Cypher ‚úÖ\n",
        "3. **Ingerir datos con patrones personalizados** ‚úÖ **DISPONIBLE AHORA**\n",
        "\n",
        "**Ejemplo de uso:**\n",
        "\n",
        "```python\n",
        "# Ingerir documento con patr√≥n personalizado\n",
        "chunks = ungraph.ingest_document(\n",
        "    \"documento.md\",\n",
        "    pattern=SIMPLE_CHUNK_PATTERN  # Usar patr√≥n personalizado\n",
        ")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìù PROBAR INGESTA CON PATRONES PERSONALIZADOS\n",
            "============================================================\n",
            "‚úÖ Carpeta de datos encontrada: D:\\projects\\Ungraph\\src\\data\n",
            "\n",
            "üìÑ Archivos disponibles (3):\n",
            "   - 110225.md\n",
            "   - AnnyLetter.txt\n",
            "   - Usar s√≠mboles de silencio de corchea.docx\n",
            "\n",
            "üí° Ejemplo de uso con patr√≥n personalizado:\n",
            "   chunks = ungraph.ingest_document(file_path, pattern=SIMPLE_CHUNK_PATTERN)\n",
            "\n",
            "‚ö†Ô∏è  Descomenta el c√≥digo siguiente para probar con datos reales:\n"
          ]
        }
      ],
      "source": [
        "# Probar ingesta con patrones personalizados\n",
        "print(\"üìù PROBAR INGESTA CON PATRONES PERSONALIZADOS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Encontrar archivos de datos\n",
        "data_path = find_in_project(\n",
        "    target=\"data\",\n",
        "    search_type=\"folder\",\n",
        "    project_root=None\n",
        ")\n",
        "\n",
        "if data_path:\n",
        "    print(f\"‚úÖ Carpeta de datos encontrada: {data_path}\")\n",
        "    files = list(data_path.glob(\"*\"))\n",
        "    print(f\"\\nüìÑ Archivos disponibles ({len([f for f in files if f.is_file()])}):\")\n",
        "    for file in files:\n",
        "        if file.is_file():\n",
        "            print(f\"   - {file.name}\")\n",
        "    \n",
        "    print(\"\\nüí° Ejemplo de uso con patr√≥n personalizado:\")\n",
        "    print(\"   chunks = ungraph.ingest_document(file_path, pattern=SIMPLE_CHUNK_PATTERN)\")\n",
        "    print(\"\\n‚ö†Ô∏è  Descomenta el c√≥digo siguiente para probar con datos reales:\")\n",
        "    \n",
        "    # Ejemplo comentado para probar\n",
        "    \"\"\"\n",
        "    # Seleccionar un archivo de prueba\n",
        "    test_file = list(data_path.glob(\"*.md\"))[0] if list(data_path.glob(\"*.md\")) else None\n",
        "    \n",
        "    if test_file:\n",
        "        print(f\"\\nüîç Probando ingesta con patr√≥n SIMPLE_CHUNK...\")\n",
        "        try:\n",
        "            chunks = ungraph.ingest_document(\n",
        "                str(test_file),\n",
        "                pattern=SIMPLE_CHUNK_PATTERN,\n",
        "                chunk_size=500,\n",
        "                chunk_overlap=100\n",
        "            )\n",
        "            print(f\"‚úÖ Ingesta exitosa: {len(chunks)} chunks creados\")\n",
        "            print(f\"   Archivo: {test_file.name}\")\n",
        "            print(f\"   Patr√≥n usado: {SIMPLE_CHUNK_PATTERN.name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {e}\")\n",
        "            print(\"üí° Aseg√∫rate de tener Neo4j configurado y corriendo\")\n",
        "    \"\"\"\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Carpeta de datos no encontrada\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 8: Resumen\n",
        "\n",
        "Resumen de patrones creados y pr√≥ximos pasos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üìä RESUMEN DE PATRONES CREADOS\n",
            "================================================================================\n",
            "Patrones Creados: 5\n",
            "Patrones Validados: 5\n",
            "Queries Generados: 5\n",
            "\n",
            "üìã Patrones disponibles:\n",
            "  ‚úÖ FILE_PAGE_CHUNK\n",
            "  ‚úÖ SIMPLE_CHUNK\n",
            "  ‚úÖ SEQUENTIAL_CHUNKS\n",
            "  ‚úÖ LEXICAL_GRAPH\n",
            "  ‚úÖ DOCUMENT_SECTION_PARAGRAPH\n",
            "\n",
            "üéØ Estado de Implementaci√≥n:\n",
            "  ‚úÖ Fase 1: Fundaci√≥n - COMPLETA\n",
            "  ‚úÖ Fase 2: Integraci√≥n con Ingesta - COMPLETA\n",
            "  ‚úÖ Fase 3: B√∫squeda GraphRAG B√°sicos - COMPLETA\n",
            "\n",
            "üéØ Pr√≥ximos Pasos:\n",
            "  1. ‚è≥ Probar patrones con datos reales\n",
            "  2. ‚è∏Ô∏è  Fase 3 Avanzada: Implementar patrones avanzados (si hay demanda)\n",
            "  3. ‚è∏Ô∏è  Optimizaci√≥n y mejoras\n",
            "================================================================================\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìä RESUMEN DE PATRONES CREADOS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "summary = {\n",
        "    \"Patrones Creados\": len(patterns),\n",
        "    \"Patrones Validados\": sum(1 for p in patterns.values() if service.validate_pattern(p)),\n",
        "    \"Queries Generados\": len([p for p in patterns.values() if service.generate_cypher(p, \"create\")])\n",
        "}\n",
        "\n",
        "for key, value in summary.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "print(\"\\nüìã Patrones disponibles:\")\n",
        "for name in patterns.keys():\n",
        "    print(f\"  ‚úÖ {name}\")\n",
        "\n",
        "print(\"\\nüéØ Estado de Implementaci√≥n:\")\n",
        "print(\"  ‚úÖ Fase 1: Fundaci√≥n - COMPLETA\")\n",
        "print(\"  ‚úÖ Fase 2: Integraci√≥n con Ingesta - COMPLETA\")\n",
        "print(\"  ‚úÖ Fase 3: B√∫squeda GraphRAG B√°sicos - COMPLETA\")\n",
        "print(\"\\nüéØ Pr√≥ximos Pasos:\")\n",
        "print(\"  1. ‚è≥ Probar patrones con datos reales\")\n",
        "print(\"  2. ‚è∏Ô∏è  Fase 3 Avanzada: Implementar patrones avanzados (si hay demanda)\")\n",
        "print(\"  3. ‚è∏Ô∏è  Optimizaci√≥n y mejoras\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Referencias\n",
        "\n",
        "- [Documentaci√≥n de Patrones](../../docs/concepts/graph-patterns.md)\n",
        "- [Gu√≠a de Patrones Personalizados](../../docs/guides/custom-patterns.md)\n",
        "- [Plan de Patrones de Grafo](../../_PLAN_PATRONES_GRAFO.md)\n",
        "- [GraphRAG Pattern Catalog](https://graphrag.com/reference/)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
